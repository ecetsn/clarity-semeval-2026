{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Evaluation: Individual Models\n",
    "\n",
    "================================================================================\n",
    "PURPOSE: Train and evaluate classifiers on features from each model separately\n",
    "================================================================================\n",
    "\n",
    "This notebook trains multiple classifiers on Context Tree features extracted\n",
    "from individual transformer models. Each model (BERT, RoBERTa, DeBERTa, XLNet)\n",
    "is evaluated separately to assess their individual performance on the clarity\n",
    "and evasion classification tasks.\n",
    "\n",
    "**Workflow:**\n",
    "1. Load features from Google Drive (saved by 02_feature_extraction_separate.ipynb)\n",
    "2. Train multiple classifiers on each model's features\n",
    "3. Evaluate on Dev set (model selection and hyperparameter tuning)\n",
    "4. Save predictions and probabilities for further analysis\n",
    "5. Generate comprehensive results tables and evaluation plots\n",
    "\n",
    "**Classifiers:**\n",
    "- Logistic Regression\n",
    "- Linear Support Vector Classifier (LinearSVC)\n",
    "- Random Forest\n",
    "- XGBoost\n",
    "- LightGBM\n",
    "\n",
    "**Output:** \n",
    "- Predictions (hard labels) and probabilities saved to Google Drive\n",
    "- Results tables comparing classifiers for each model/task combination\n",
    "- Evaluation plots (confusion matrices, PR curves, ROC curves)\n",
    "- Results metadata saved for final summary generation\n",
    "\n",
    "================================================================================\n",
    "INPUTS (What this notebook loads)\n",
    "================================================================================\n",
    "\n",
    "**From GitHub:**\n",
    "- Repository code (cloned automatically if not present)\n",
    "- Source modules from `src/` directory:\n",
    "  - `src.storage.manager` (StorageManager)\n",
    "  - `src.models.trainer` (training and evaluation functions)\n",
    "  - `src.models.classifiers` (classifier definitions)\n",
    "  - `src.evaluation.tables` (final summary table functions)\n",
    "\n",
    "**From Google Drive:**\n",
    "- Dataset splits: `splits/dataset_splits.pkl`\n",
    "  - Train split (for label extraction)\n",
    "  - Dev split (for label extraction)\n",
    "- Feature matrices: `features/raw/X_{split}_{model}_{task}.npy`\n",
    "  - For each model (bert, roberta, deberta, xlnet)\n",
    "  - For each task (clarity, evasion)\n",
    "  - For Train and Dev splits\n",
    "  - Loaded via `storage.load_features(model, task, split)`\n",
    "\n",
    "**From HuggingFace Hub:**\n",
    "- Nothing (all features already extracted)\n",
    "\n",
    "================================================================================\n",
    "OUTPUTS (What this notebook saves)\n",
    "================================================================================\n",
    "\n",
    "**To Google Drive:**\n",
    "- Predictions: `predictions/pred_{split}_{model}_{classifier}_{task}.npy`\n",
    "  - Hard label predictions for Dev set\n",
    "  - For each model/classifier/task combination\n",
    "- Probabilities: `features/probabilities/probs_{split}_{model}_{classifier}_{task}.npy`\n",
    "  - Probability distributions for Dev set\n",
    "  - For each model/classifier/task combination\n",
    "- Evaluation plots: `plots/{model}_{task}_{classifier}/`\n",
    "  - Confusion matrices\n",
    "  - Precision-Recall curves\n",
    "  - ROC curves\n",
    "- Final summary tables: `results/tables/`\n",
    "  - `final_summary_all_models_classifiers_tasks.{csv,html,png}`\n",
    "  - `final_summary_model_wise.{csv,html,png}`\n",
    "  - `final_summary_classifier_wise.{csv,html,png}`\n",
    "- Complete results dictionary: `results/all_results_dev.pkl`\n",
    "\n",
    "**To GitHub:**\n",
    "- Results metadata: `results/{model}_{task}_separate.json`\n",
    "  - Metrics for each classifier\n",
    "  - Train/Dev sample counts\n",
    "  - Timestamp information\n",
    "- Results dictionary (JSON): `results/all_results_dev.json`\n",
    "\n",
    "**Evaluation Metrics Computed and Printed:**\n",
    "- Accuracy\n",
    "- Macro Precision, Recall, F1\n",
    "- Weighted Precision, Recall, F1\n",
    "- Per-class metrics (precision, recall, F1, support)\n",
    "- Cohen's Kappa\n",
    "- Matthews Correlation Coefficient\n",
    "- Hamming Loss\n",
    "- Jaccard Score (IoU)\n",
    "- Confusion Matrix\n",
    "\n",
    "**What gets passed to next notebook:**\n",
    "- All predictions and probabilities saved to persistent storage\n",
    "- Final summary tables (CSV, HTML, PNG formats)\n",
    "- Results metadata for comparison with fusion approaches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SETUP: Repository Clone, Drive Mount, and Path Configuration\n",
    "# ============================================================================\n",
    "# This cell performs minimal setup required for the notebook to run:\n",
    "# 1. Clones repository from GitHub (if not already present)\n",
    "# 2. Mounts Google Drive for persistent data storage\n",
    "# 3. Configures Python paths and initializes StorageManager\n",
    "# 4. Loads data splits and features created in previous notebooks\n",
    "\n",
    "import shutil\n",
    "import os\n",
    "import subprocess\n",
    "import time\n",
    "import requests\n",
    "import zipfile\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from google.colab import drive\n",
    "import numpy as np\n",
    "\n",
    "# Repository configuration\n",
    "repo_dir = '/content/semeval-context-tree-modular'\n",
    "repo_url = 'https://github.com/EonTechie/semeval-context-tree-modular.git'\n",
    "zip_url = 'https://github.com/EonTechie/semeval-context-tree-modular/archive/refs/heads/main.zip'\n",
    "\n",
    "# Clone repository (if not already present)\n",
    "if not os.path.exists(repo_dir):\n",
    "    print(\"Cloning repository from GitHub...\")\n",
    "    max_retries = 2\n",
    "    clone_success = False\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            result = subprocess.run(\n",
    "                ['git', 'clone', repo_url],\n",
    "                cwd='/content',\n",
    "                capture_output=True,\n",
    "                text=True,\n",
    "                timeout=60\n",
    "            )\n",
    "            if result.returncode == 0:\n",
    "                print(\"Repository cloned successfully via git\")\n",
    "                clone_success = True\n",
    "                break\n",
    "            else:\n",
    "                if attempt < max_retries - 1:\n",
    "                    time.sleep(3)\n",
    "        except Exception as e:\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(3)\n",
    "    \n",
    "    # Fallback: Download as ZIP if git clone fails\n",
    "    if not clone_success:\n",
    "        print(\"Git clone failed. Downloading repository as ZIP archive...\")\n",
    "        zip_path = '/tmp/repo.zip'\n",
    "        try:\n",
    "            response = requests.get(zip_url, stream=True, timeout=60)\n",
    "            response.raise_for_status()\n",
    "            with open(zip_path, 'wb') as f:\n",
    "                for chunk in response.iter_content(chunk_size=8192):\n",
    "                    f.write(chunk)\n",
    "            with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "                zip_ref.extractall('/content')\n",
    "            extracted_dir = '/content/semeval-context-tree-modular-main'\n",
    "            if os.path.exists(extracted_dir):\n",
    "                os.rename(extracted_dir, repo_dir)\n",
    "            os.remove(zip_path)\n",
    "            print(\"Repository downloaded and extracted successfully\")\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to obtain repository: {e}\")\n",
    "\n",
    "# Mount Google Drive (if not already mounted)\n",
    "try:\n",
    "    drive.mount('/content/drive', force_remount=False)\n",
    "except Exception:\n",
    "    pass  # Already mounted\n",
    "\n",
    "# Configure paths\n",
    "BASE_PATH = Path('/content/semeval-context-tree-modular')\n",
    "DATA_PATH = Path('/content/drive/MyDrive/semeval_data')\n",
    "\n",
    "# Verify repository structure exists\n",
    "if not BASE_PATH.exists():\n",
    "    raise RuntimeError(f\"Repository directory not found: {BASE_PATH}\")\n",
    "if not (BASE_PATH / 'src').exists():\n",
    "    raise RuntimeError(f\"src directory not found in repository: {BASE_PATH / 'src'}\")\n",
    "if not (BASE_PATH / 'src' / 'storage' / 'manager.py').exists():\n",
    "    raise RuntimeError(f\"Required file not found: {BASE_PATH / 'src' / 'storage' / 'manager.py'}\")\n",
    "\n",
    "# Add repository to Python path\n",
    "sys.path.insert(0, str(BASE_PATH))\n",
    "\n",
    "# Verify imports work\n",
    "try:\n",
    "    from src.storage.manager import StorageManager\n",
    "    from src.models.trainer import train_and_evaluate\n",
    "    from src.models.classifiers import get_classifier_dict\n",
    "except ImportError as e:\n",
    "    raise ImportError(\n",
    "        f\"Failed to import required modules. \"\n",
    "        f\"Repository path: {BASE_PATH}, \"\n",
    "        f\"Python path: {sys.path[:3]}, \"\n",
    "        f\"Error: {e}\"\n",
    "    )\n",
    "\n",
    "# Initialize StorageManager\n",
    "storage = StorageManager(\n",
    "    base_path=str(BASE_PATH),\n",
    "    data_path=str(DATA_PATH),\n",
    "    github_path=str(BASE_PATH)\n",
    ")\n",
    "\n",
    "# Data splits will be loaded per-task in the training loop\n",
    "# Clarity and Evasion have different splits (Evasion uses majority voting)\n",
    "\n",
    "print(\"Setup complete\")\n",
    "print(f\"  Repository: {BASE_PATH}\")\n",
    "print(f\"  Data storage: {DATA_PATH}\")\n",
    "print(f\"\\nNOTE: Data splits will be loaded per-task (task-specific splits)\")\n",
    "print(f\"      Clarity and Evasion have different splits due to majority voting\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CONFIGURE MODELS, TASKS, AND CLASSIFIERS\n",
    "# ============================================================================\n",
    "# Defines the models to evaluate, tasks to perform, and classifiers to train\n",
    "# Label mappings are defined for clarity (3-class) and evasion (9-class) tasks\n",
    "\n",
    "MODELS = ['bert', 'bert_political', 'bert_ambiguity', 'roberta', 'deberta', 'xlnet']\n",
    "TASKS = ['clarity', 'evasion']\n",
    "\n",
    "# Label mappings for each task\n",
    "CLARITY_LABELS = ['Ambivalent', 'Clear Non-Reply', 'Clear Reply']\n",
    "EVASION_LABELS = ['Claims ignorance', 'Clarification', 'Declining to answer', \n",
    "                  'Deflection', 'Dodging', 'Explicit', \n",
    "                  'General', 'Implicit', 'Partial/half-answer']\n",
    "\n",
    "# Initialize classifiers with fixed random seed for reproducibility\n",
    "classifiers = get_classifier_dict(random_state=42)\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"  Models: {MODELS}\")\n",
    "print(f\"  Tasks: {TASKS}\")\n",
    "print(f\"  Classifiers: {list(classifiers.keys())}\")\n",
    "print(f\"  Clarity labels: {len(CLARITY_LABELS)} classes\")\n",
    "print(f\"  Evasion labels: {len(EVASION_LABELS)} classes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# TRAIN AND EVALUATE CLASSIFIERS FOR EACH MODEL AND TASK\n",
    "# ============================================================================\n",
    "# Iterates through each model and task, trains all classifiers, and evaluates\n",
    "# on the Dev set. Results are saved for later analysis and final summary generation.\n",
    "\n",
    "all_results = {}\n",
    "\n",
    "for model in MODELS:\n",
    "    all_results[model] = {}\n",
    "    \n",
    "    for task in TASKS:\n",
    "        # Select appropriate label list and dataset key for this task\n",
    "        if task == 'clarity':\n",
    "            label_list = CLARITY_LABELS\n",
    "            label_key = 'clarity_label'\n",
    "        else:  # evasion\n",
    "            label_list = EVASION_LABELS\n",
    "            label_key = 'evasion_label'\n",
    "        \n",
    "        # Load task-specific splits (Clarity and Evasion have different splits)\n",
    "        # Evasion splits are filtered by majority voting\n",
    "        train_ds = storage.load_split('train', task=task)\n",
    "        dev_ds = storage.load_split('dev', task=task)\n",
    "        \n",
    "        # Load features from persistent storage\n",
    "        X_train = storage.load_features(model, task, 'train')\n",
    "        X_dev = storage.load_features(model, task, 'dev')\n",
    "        \n",
    "        # Extract labels from dataset splits\n",
    "        y_train = np.array([train_ds[i][label_key] for i in range(len(train_ds))])\n",
    "        y_dev = np.array([dev_ds[i][label_key] for i in range(len(dev_ds))])\n",
    "        \n",
    "        # Train all classifiers and evaluate on Dev set\n",
    "        # This function handles training, prediction, metric computation, and visualization\n",
    "        results = train_and_evaluate(\n",
    "            X_train, y_train, X_dev, y_dev,\n",
    "            label_list=label_list,\n",
    "            task_name=f\"{model}_{task}\",\n",
    "            classifiers=classifiers,\n",
    "            random_state=42,\n",
    "            print_report=False,     # Don't print classification reports\n",
    "            print_table=True,       # Print results comparison table only\n",
    "            create_plots=True,      # Generate confusion matrices and PR/ROC curves (silently)\n",
    "            save_plots_dir=str(DATA_PATH / 'plots')\n",
    "        )\n",
    "        \n",
    "        # Save predictions and probabilities to persistent storage\n",
    "        # These will be used for further analysis and final summary generation\n",
    "        for classifier_name, result in results.items():\n",
    "            # Save hard label predictions\n",
    "            storage.save_predictions(\n",
    "                result['dev_pred'],\n",
    "                model, classifier_name, task, 'dev'\n",
    "            )\n",
    "            \n",
    "            # Save probability distributions (if classifier supports it)\n",
    "            if result['dev_proba'] is not None:\n",
    "                storage.save_probabilities(\n",
    "                    result['dev_proba'],\n",
    "                    model, classifier_name, task, 'dev'\n",
    "                )\n",
    "        \n",
    "        all_results[model][task] = results\n",
    "        \n",
    "        # Save results summary to metadata for final summary generation\n",
    "        experiment_id = f\"{model}_{task}_separate\"\n",
    "        storage.save_results({\n",
    "            'model': model,\n",
    "            'task': task,\n",
    "            'results': {\n",
    "                name: {\n",
    "                    'metrics': res['metrics'],\n",
    "                    'n_train': len(y_train),\n",
    "                    'n_dev': len(y_dev)\n",
    "                }\n",
    "                for name, res in results.items()\n",
    "            }\n",
    "        }, experiment_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ============================================================================\n",
    "# FINAL SUMMARY: PIVOT TABLES AND STYLED TABLES\n",
    "# ============================================================================\n",
    "# This cell creates comprehensive final summary tables from all training results:\n",
    "# 1. Final Summary Pivot Table: ALL MODELS × CLASSIFIERS × TASKS\n",
    "# 2. Model-wise Summary: Classifier × Tasks (grouped by Model)\n",
    "# 3. Classifier-wise Summary: Model × Tasks (grouped by Classifier)\n",
    "# \n",
    "# All tables are styled with color gradients and saved in multiple formats\n",
    "# (CSV, HTML, PNG) for easy sharing and publication.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# FINAL SUMMARY GENERATION\n",
    "# ============================================================================\n",
    "# Generate comprehensive summary tables and save all results\n",
    "\n",
    "import pandas as pd\n",
    "from src.evaluation.tables import (\n",
    "    create_final_summary_pivot,\n",
    "    create_model_wise_summary_pivot,\n",
    "    create_classifier_wise_summary_pivot,\n",
    "    style_table\n",
    ")\n",
    "\n",
    "# Save all_results dictionary to persistent storage\n",
    "storage.save_all_results_dict(all_results, filename='all_results_dev.pkl')\n",
    "\n",
    "# Extract classifier names from results\n",
    "classifier_names = set()\n",
    "all_tasks = ['clarity', 'evasion', 'hierarchical_evasion_to_clarity']  # 3 tasks: Clarity, Evasion, Hierarchical\n",
    "\n",
    "for model in MODELS:\n",
    "    if model in all_results:\n",
    "        for task in all_tasks:\n",
    "            if task in all_results[model]:\n",
    "                classifier_names.update(all_results[model][task].keys())\n",
    "classifier_names = sorted(list(classifier_names))\n",
    "\n",
    "# ============================================================================\n",
    "# FINAL SUMMARY: ALL MODELS × CLASSIFIERS × TASKS (3 Tasks Side-by-Side)\n",
    "# ============================================================================\n",
    "# Shows Clarity, Evasion, and Hierarchical results side-by-side for comparison\n",
    "# Color coding helps identify which task performs better for each model/classifier\n",
    "\n",
    "df_final_pivot = create_final_summary_pivot(\n",
    "    all_results, MODELS, classifier_names, all_tasks, metric='macro_f1'\n",
    ")\n",
    "\n",
    "if not df_final_pivot.empty:\n",
    "    # Display styled pivot table with color coding\n",
    "    styled_final = style_table(df_final_pivot, precision=4)\n",
    "    display(styled_final)\n",
    "    \n",
    "    # Save table in multiple formats\n",
    "    storage.save_table(\n",
    "        styled_final,\n",
    "        table_name='final_summary_all_models_classifiers_tasks',\n",
    "        formats=['csv', 'html', 'png']\n",
    "    )\n",
    "else:\n",
    "    print(\"WARNING: No results available for final summary pivot table\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ============================================================================\n",
    "# HIERARCHICAL EVASION → CLARITY APPROACH\n",
    "# ============================================================================\n",
    "# This cell implements the hierarchical approach where evasion predictions\n",
    "# are mapped to clarity predictions using a predefined mapping function.\n",
    "# This approach leverages the hierarchical relationship between evasion\n",
    "# (fine-grained) and clarity (coarse-grained) labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# HIERARCHICAL EVALUATION: EVASION PREDICTIONS → CLARITY PREDICTIONS\n",
    "# ============================================================================\n",
    "# For each model, uses evasion predictions to generate clarity predictions\n",
    "# via hierarchical mapping, then evaluates against true clarity labels\n",
    "# This is treated as a 3rd task alongside Clarity and Evasion\n",
    "\n",
    "from src.models.hierarchical import evaluate_hierarchical_approach\n",
    "import numpy as np\n",
    "\n",
    "hierarchical_results = {}\n",
    "\n",
    "# Load dev split for clarity task (to get true labels)\n",
    "dev_ds_clarity = storage.load_split('dev', task='clarity')\n",
    "\n",
    "for model in MODELS:\n",
    "    # Check if we have evasion predictions for this model\n",
    "    if 'evasion' not in all_results.get(model, {}):\n",
    "        continue\n",
    "    \n",
    "    # Get evasion predictions and true labels\n",
    "    evasion_results = all_results[model]['evasion']\n",
    "    \n",
    "    # Use predictions from best classifier (by Macro F1)\n",
    "    best_classifier = None\n",
    "    best_f1 = -1\n",
    "    \n",
    "    for clf_name, clf_result in evasion_results.items():\n",
    "        if 'metrics' in clf_result:\n",
    "            f1 = clf_result['metrics'].get('macro_f1', 0.0)\n",
    "            if f1 > best_f1:\n",
    "                best_f1 = f1\n",
    "                best_classifier = clf_name\n",
    "    \n",
    "    if best_classifier is None:\n",
    "        continue\n",
    "    \n",
    "    # Get predictions (already string labels from train_classifiers)\n",
    "    y_evasion_pred = evasion_results[best_classifier]['dev_pred']\n",
    "    \n",
    "    # Get true clarity labels\n",
    "    y_clarity_true = np.array([dev_ds_clarity[i]['clarity_label'] for i in range(len(dev_ds_clarity))])\n",
    "    \n",
    "    # Encode clarity labels for evaluation (hierarchical function expects encoded)\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    \n",
    "    le_clarity = LabelEncoder()\n",
    "    y_clarity_true_encoded = le_clarity.fit_transform(y_clarity_true)\n",
    "    \n",
    "    # Evaluate hierarchical approach\n",
    "    # y_evasion_pred is already string labels, y_clarity_true_encoded is encoded\n",
    "    # We pass dummy encoded evasion_true (not used in mapping, only for consistency)\n",
    "    y_evasion_true_dummy = np.zeros(len(y_evasion_pred), dtype=int)  # Dummy, not used\n",
    "    \n",
    "    hierarchical_metrics = evaluate_hierarchical_approach(\n",
    "        y_evasion_true_dummy,  # Not used in mapping, only for function signature\n",
    "        y_evasion_pred,  # String labels - function will handle both string and int\n",
    "        y_clarity_true_encoded,  # Encoded integers\n",
    "        EVASION_LABELS,\n",
    "        CLARITY_LABELS\n",
    "    )\n",
    "    \n",
    "    hierarchical_results[model] = {\n",
    "        'classifier': best_classifier,\n",
    "        'metrics': hierarchical_metrics,\n",
    "        'evasion_f1': best_f1\n",
    "    }\n",
    "\n",
    "# Add hierarchical results to all_results for final summary (as 3rd task)\n",
    "for model in hierarchical_results:\n",
    "    if model not in all_results:\n",
    "        all_results[model] = {}\n",
    "    if 'hierarchical_evasion_to_clarity' not in all_results[model]:\n",
    "        all_results[model]['hierarchical_evasion_to_clarity'] = {}\n",
    "    \n",
    "    # Store hierarchical results in same format as other tasks\n",
    "    # Use best_classifier name instead of 'MLP' for consistency\n",
    "    best_clf_name = hierarchical_results[model]['classifier']\n",
    "    all_results[model]['hierarchical_evasion_to_clarity'][best_clf_name] = {\n",
    "        'metrics': hierarchical_results[model]['metrics'],\n",
    "        'dev_pred': hierarchical_results[model]['metrics']['predictions'],\n",
    "        'dev_proba': None  # Hierarchical approach doesn't produce probabilities\n",
    "    }\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
