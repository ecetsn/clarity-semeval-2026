# -*- coding: utf-8 -*-
"""05_final_evaluation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/github/EonTechie/semeval-context-tree-modular/blob/main/notebooks/05_final_evaluation.ipynb

<div style="font-size: 10px;">

# Final Evaluation: Test Set Performance

================================================================================
PURPOSE: Evaluate final models on the held-out test set
================================================================================

**CRITICAL**: This notebook evaluates on the **TEST set** which was separated
at the beginning (in 01_data_split.ipynb) and has NEVER been used for training,
model selection, feature selection, or any development decisions.

**Evaluation Protocol:**
1. Load best model/classifier combinations (based on Dev set results from
   03_train_evaluate.ipynb and 04_early_fusion.ipynb)
2. Extract features for TEST set (if not already extracted)
3. Retrain models on combined Train+Dev data (final training)
4. Evaluate on TEST set (unbiased final performance estimate)
5. Generate comprehensive final reports, plots, and summary tables

**Important Notes:**
- Test set is ONLY accessed in this notebook
- Models are retrained on Train+Dev before test evaluation
- This provides an unbiased estimate of generalization performance
- Results from this notebook represent the final competition submission metrics

================================================================================
INPUTS (What this notebook loads)
================================================================================

**From GitHub:**
- Repository code (cloned automatically if not present)
- Source modules from `src/` directory:
  - `src.storage.manager` (StorageManager)
  - `src.features.extraction` (feature extraction functions)
  - `src.models.trainer` (training and evaluation functions)
  - `src.models.classifiers` (classifier definitions)
  - `src.evaluation.metrics` (metric computation functions)
  - `src.evaluation.tables` (results table functions)
  - `src.evaluation.visualizer` (plotting functions)

**From Google Drive:**
- Dataset splits: `splits/dataset_splits.pkl`
  - Test split (ONLY accessed in this notebook, 308 samples)
  - Train and Dev splits (for combined training)
- Feature matrices (if already extracted):
  - `features/raw/X_test_{model}_{task}.npy` (may not exist yet)
  - `features/raw/X_train_{model}_{task}.npy` (from 02_feature_extraction_separate.ipynb)
  - `features/raw/X_dev_{model}_{task}.npy` (from 02_feature_extraction_separate.ipynb)

**From HuggingFace Hub:**
- Transformer models (if test features need extraction):
  - BERT, RoBERTa, DeBERTa, XLNet tokenizers and models

================================================================================
OUTPUTS (What this notebook saves)
================================================================================

**To Google Drive:**
- Test features (if extracted): `features/raw/X_test_{model}_{task}.npy`
  - For each model (bert, roberta, deberta, xlnet)
  - For each task (clarity, evasion)
  - Shape: (308_samples, 19_features)
- Final predictions: `predictions/pred_test_{model}_{classifier}_{task}.npy`
  - Hard label predictions for Test set
  - For each model/classifier/task combination
- Final probabilities: `features/probabilities/probs_test_{model}_{classifier}_{task}.npy`
  - Probability distributions for Test set
  - For each model/classifier/task combination
- Final evaluation plots: `plots/final_evaluation/{model}_{task}_{classifier}/`
  - Confusion matrices
  - Precision-Recall curves
  - ROC curves

**To GitHub:**
- Test feature metadata: `metadata/features_test_{model}_{task}.json`
  - Feature names and dimensions
  - Timestamp information
- Final results metadata: `results/FINAL_TEST_{model}_{task}.json`
  - Final test set metrics
  - Model/classifier/task information
  - Test sample counts

**Evaluation Metrics Computed and Printed:**
- Accuracy
- Macro Precision, Recall, F1
- Weighted Precision, Recall, F1
- Per-class metrics (precision, recall, F1, support)
- Cohen's Kappa
- Matthews Correlation Coefficient
- Hamming Loss
- Jaccard Score (IoU)
- Confusion Matrix

**What represents final competition submission:**
- All test set predictions and probabilities
- Final evaluation metrics (computed on 308 test samples)
- Complete evaluation results for all model/classifier/task combinations

</div>
"""

# ============================================================================
# SETUP: Repository Clone, Drive Mount, and Path Configuration
# ============================================================================
# This cell performs minimal setup required for the notebook to run:
# 1. Clones repository from GitHub (if not already present)
# 2. Mounts Google Drive for persistent data storage
# 3. Configures Python paths and initializes StorageManager
# 4. Loads test split (ONLY accessed in this notebook)

import shutil
import os
import subprocess
import time
import requests
import zipfile
import sys
from pathlib import Path
from google.colab import drive
import numpy as np
import torch
from transformers import AutoTokenizer, AutoModel
!rm -rf /content/semeval-context-tree-modular
!git clone https://github.com/EonTechie/semeval-context-tree-modular.git
!cd /content/semeval-context-tree-modular && git pull
# Repository configuration
repo_dir = '/content/semeval-context-tree-modular'
repo_url = 'https://github.com/EonTechie/semeval-context-tree-modular.git'
zip_url = 'https://github.com/EonTechie/semeval-context-tree-modular/archive/refs/heads/main.zip'

# Clone repository (if not already present)
if not os.path.exists(repo_dir):
    print("Cloning repository from GitHub...")
    max_retries = 2
    clone_success = False

    for attempt in range(max_retries):
        try:
            result = subprocess.run(
                ['git', 'clone', repo_url],
                cwd='/content',
                capture_output=True,
                text=True,
                timeout=60
            )
            if result.returncode == 0:
                print("Repository cloned successfully via git")
                clone_success = True
                break
            else:
                if attempt < max_retries - 1:
                    time.sleep(3)
        except Exception as e:
            if attempt < max_retries - 1:
                time.sleep(3)

    # Fallback: Download as ZIP if git clone fails
    if not clone_success:
        print("Git clone failed. Downloading repository as ZIP archive...")
        zip_path = '/tmp/repo.zip'
        try:
            response = requests.get(zip_url, stream=True, timeout=60)
            response.raise_for_status()
            with open(zip_path, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    f.write(chunk)
            with zipfile.ZipFile(zip_path, 'r') as zip_ref:
                zip_ref.extractall('/content')
            extracted_dir = '/content/semeval-context-tree-modular-main'
            if os.path.exists(extracted_dir):
                os.rename(extracted_dir, repo_dir)
            os.remove(zip_path)
            print("Repository downloaded and extracted successfully")
        except Exception as e:
            raise RuntimeError(f"Failed to obtain repository: {e}")

# Mount Google Drive (if not already mounted)
try:
    drive.mount('/content/drive', force_remount=False)
except Exception:
    pass  # Already mounted

# Configure paths
BASE_PATH = Path('/content/semeval-context-tree-modular')
DATA_PATH = Path('/content/drive/MyDrive/semeval_data')

# Verify repository structure exists
if not BASE_PATH.exists():
    raise RuntimeError(f"Repository directory not found: {BASE_PATH}")
if not (BASE_PATH / 'src').exists():
    raise RuntimeError(f"src directory not found in repository: {BASE_PATH / 'src'}")
if not (BASE_PATH / 'src' / 'storage' / 'manager.py').exists():
    raise RuntimeError(f"Required file not found: {BASE_PATH / 'src' / 'storage' / 'manager.py'}")

# Add repository to Python path
sys.path.insert(0, str(BASE_PATH))

# Verify imports work
try:
    from src.storage.manager import StorageManager
    from src.features.extraction import featurize_hf_dataset_in_batches_v2
    from src.models.classifiers import get_classifier_dict
    from src.evaluation.metrics import compute_all_metrics, print_classification_report
    from src.evaluation.tables import print_results_table
    from src.evaluation.visualizer import visualize_all_evaluation
except ImportError as e:
    raise ImportError(
        f"Failed to import required modules. "
        f"Repository path: {BASE_PATH}, "
        f"Python path: {sys.path[:3]}, "
        f"Error: {e}"
    )

# Initialize StorageManager
storage = StorageManager(
    base_path=str(BASE_PATH),
    data_path=str(DATA_PATH),
    github_path=str(BASE_PATH)
)

# Test splits will be loaded per-task in the evaluation loop
# Clarity and Evasion have different test splits (Evasion uses majority voting)

print("Setup complete")
print(f"  Repository: {BASE_PATH}")
print(f"  Data storage: {DATA_PATH}")
print(f"\nCRITICAL: Test sets will be loaded per-task (task-specific splits)")
print("         Clarity and Evasion have different test splits due to majority voting")
print("         These sets have NEVER been used for training or development!")

# ============================================================================
# REPRODUCIBILITY SETUP: Set Random Seeds for All Libraries
# ============================================================================
# This cell sets random seeds for Python, NumPy, PyTorch, and HuggingFace
# to ensure reproducible results across all runs.
#
# IMPORTANT: Run this cell FIRST before any other code that uses randomness.
# Seed value: 42 (same as used in all other parts of the pipeline)

from src.utils.reproducibility import set_all_seeds

# Set all random seeds to 42 for full reproducibility
# deterministic=True ensures PyTorch operations are deterministic (slower but fully reproducible)
set_all_seeds(seed=42, deterministic=True)

print("✓ Reproducibility configured: All random seeds set to 42")
print("✓ PyTorch deterministic mode enabled")
print("\nNOTE: If you encounter performance issues or non-deterministic behavior,")
print("      you can set deterministic=False in set_all_seeds() call above.")

# ============================================================================
# CONFIGURE MODELS, TASKS, AND CLASSIFIERS FOR FINAL EVALUATION
# ============================================================================
# Defines the models, tasks, and classifiers for final evaluation
# NOTE: In practice, you should select best model/classifier based on Dev set
# results. For comprehensive comparison, all combinations are evaluated here.

# NOTE: run_final_evaluation() artık kullanılmıyor - ayrı cell'lerde yapılıyor (Cell 7-10)
# from src.models.final_evaluation import run_final_evaluation

MODELS = ['bert', 'bert_political', 'bert_ambiguity', 'roberta', 'deberta', 'xlnet']
TASKS = ['clarity', 'evasion']

# Label mappings for each task
CLARITY_LABELS = ['Ambivalent', 'Clear Non-Reply', 'Clear Reply']
EVASION_LABELS = ['Claims ignorance', 'Clarification', 'Declining to answer',
                  'Deflection', 'Dodging', 'Explicit',
                  'General', 'Implicit', 'Partial/half-answer']

# Model configurations (HuggingFace model names)
MODEL_CONFIGS = {
    'bert': 'bert-base-uncased',
    'bert_political': 'bert-base-uncased',  # TODO: Replace with actual political discourse BERT model
    'bert_ambiguity': 'bert-base-uncased',  # TODO: Replace with actual ambiguity-focused BERT model
    'roberta': 'roberta-base',
    'deberta': 'microsoft/deberta-v3-base',
    'xlnet': 'xlnet-base-cased'
}

# Max sequence lengths for each model
MODEL_MAX_LENGTHS = {
    'bert': 512,
    'bert_political': 512,
    'bert_ambiguity': 512,
    'roberta': 512,
    'deberta': 512,
    'xlnet': 1024
}

# Label lists for each task
LABEL_LISTS = {
    'clarity': CLARITY_LABELS,
    'evasion': EVASION_LABELS
}

# Initialize classifiers with fixed random seed for reproducibility
classifiers = get_classifier_dict(random_state=42)

# Configure device (GPU if available, otherwise CPU)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

print("Configuration for final evaluation:")
print(f"  Models: {MODELS}")
print(f"  Tasks: {TASKS}")
print(f"  Classifiers: {list(classifiers.keys())}")
print(f"  Device: {device}")
print(f"\nNOTE: Evaluating all model/classifier combinations on TEST set")
print("      In practice, select best combination based on Dev set results")

"""# ============================================================================
# FINAL EVALUATION TYPE 2: WEIGHTED AVERAGE ENSEMBLE (METHOD 2)
# ============================================================================
# Bu fonksiyon Method 2 (greedy-based top-15 features) modellerini kullanarak
# weighted average ensemble yapar:
# 1. Method 2 trained models'ları yükler (04_model_specific_top15_fusion.ipynb)
# 2. Test feature'larını extract eder (yoksa) veya Drive'dan yükler
# 3. Her model×task×classifier için probability predict eder
# 4. Dev set macro F1'ye göre weighted average ensemble yapar
# 5. Test setinde evaluate eder (clarity, evasion, hierarchical, annotator-specific)
# 6. Sonuçları FinalResultsType2'ye kaydeder

"""

# ============================================================================
# NOTE: Type2 Evaluation Removed
# ============================================================================
# This cell previously contained Type2 evaluation which depended on
# 04_model_specific_top15_fusion.ipynb (now deleted).
#
# The FinalResultsType2 folder is now used for:
# - 03_5_ablation_study.ipynb results (ablation/ and classifier_specific/)
#
# Final evaluation uses run_final_evaluation() which saves to FinalResultsType1/

# ============================================================================
# STEP 1: TEST FEATURE EXTRACTION (Model-Independent + Model-Dependent)
# ============================================================================
# 2. notebook gibi: Model-independent feature'ları 1 kere çıkar,
# sonra her model için model-dependent feature'ları efficiency mode ile çıkar
# Sonuçlar results/FinalResultsType1/test/ altında kaydedilir

from src.features.extraction import featurize_model_independent_features, featurize_hf_dataset_in_batches_v2
from transformers import pipeline

# Create test features directory
test_features_dir = storage.data_path / 'results/FinalResultsType1/test'
test_features_dir.mkdir(parents=True, exist_ok=True)
print(f"Test features directory: {test_features_dir}")

# Load sentiment pipeline for model-independent features
print("\nLoading sentiment analysis pipeline...")
try:
    sentiment_pipeline = pipeline(
        "sentiment-analysis",
        model="cardiffnlp/twitter-roberta-base-sentiment-latest",
        device=0 if torch.cuda.is_available() else -1,
        return_all_scores=True
    )
    print("  ✓ Sentiment pipeline loaded")
except Exception as e:
    print(f"  ⚠ Could not load sentiment pipeline: {e}")
    sentiment_pipeline = None

metadata_keys = {
    'inaudible': 'inaudible',
    'multiple_questions': 'multiple_questions',
    'affirmative_questions': 'affirmative_questions'
}

# STEP 1.1: Extract model-independent test features (1 kere çıkar, tüm modeller için kullan)
print("\n" + "="*80)
print("STEP 1.1: MODEL-INDEPENDENT TEST FEATURES")
print("="*80)

test_model_independent_features = {}  # {task: X_test_indep}

for task in TASKS:
    print(f"\nTask: {task.upper()}")

    # Load test split
    try:
        test_ds = storage.load_split('test', task=task)
        print(f"  Test set: {len(test_ds)} samples")
    except FileNotFoundError as e:
        print(f"  ⚠ Test split not found: {e}")
        continue

    # CHECKPOINT: Try to load model-independent test features
    test_indep_path = test_features_dir / f'X_test_independent_{task}.npy'

    if test_indep_path.exists():
        X_test_indep = np.load(test_indep_path)
        print(f"  ✓ Loaded from Drive: {X_test_indep.shape}")
    else:
        # Extract model-independent test features
        print(f"  → Extracting model-independent test features...")
        X_test_indep, feature_names_indep = featurize_model_independent_features(
            test_ds,
            question_key='interview_question',
            answer_key='interview_answer',
            batch_size=32,
            show_progress=True,
            sentiment_pipeline=sentiment_pipeline,
            metadata_keys=metadata_keys,
        )
        # Save to Drive
        np.save(test_indep_path, X_test_indep)
        print(f"  ✓ Extracted and saved: {X_test_indep.shape}")

    test_model_independent_features[task] = X_test_indep

# STEP 1.2: Extract model-dependent test features for each model (efficiency mode)
print("\n" + "="*80)
print("STEP 1.2: MODEL-DEPENDENT TEST FEATURES (Efficiency Mode)")
print("="*80)

for model_key in MODELS:
    print(f"\n{'-'*60}")
    print(f"Model: {model_key.upper()}")
    print(f"{'-'*60}")

    # Clear GPU cache before loading new model (prevent CUDA errors)
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.synchronize()

    # Load transformer model and tokenizer
    model_name = MODEL_CONFIGS[model_key]
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    hf_model = AutoModel.from_pretrained(model_name)
    hf_model.to(device)
    hf_model.eval()

    max_seq_len = MODEL_MAX_LENGTHS.get(model_key, 512)
    print(f"  Max sequence length: {max_seq_len}")

    for task in TASKS:
        print(f"\n  Task: {task}")

        if task not in test_model_independent_features:
            print(f"    ⚠ Skipping: Model-independent features not available")
            continue

        # CHECKPOINT: Try to load test features from Drive
        try:
            X_test = storage.load_features(model_key, task, 'test')
            print(f"    ✓ Loaded from Drive: {X_test.shape}")
        except FileNotFoundError:
            # Extract test features (efficiency mode)
            print(f"    → Extracting test features (efficiency mode)...")

            # Load test split
            test_ds = storage.load_split('test', task=task)

            # EFFICIENCY MODE: Use pre-extracted model-independent features
            X_test, feature_names, _ = featurize_hf_dataset_in_batches_v2(
                test_ds, tokenizer, hf_model, device,
                question_key='interview_question',
                answer_key='interview_answer',
                batch_size=8,
                max_sequence_length=max_seq_len,
                show_progress=True,
                model_independent_features=test_model_independent_features[task]  # Reuse pre-extracted
            )

            # Save to Drive
            storage.save_features(X_test, model_key, task, 'test', feature_names)
            print(f"    ✓ Extracted and saved: {X_test.shape}")

    # Free GPU memory
    del hf_model, tokenizer
    if torch.cuda.is_available():
        torch.cuda.empty_cache()

print("\n" + "="*80)
print("TEST FEATURE EXTRACTION COMPLETE")
print("="*80)

# ============================================================================
# STEP 2: TRAIN ON TRAIN+DEV AND EVALUATE ON TEST
# ============================================================================
# Notebook 3 gibi: Her model×task×classifier kombinasyonunu eğit ve test setinde değerlendir

from src.evaluation.metrics import compute_all_metrics, print_classification_report
from src.evaluation.tables import print_results_table
from sklearn.preprocessing import LabelEncoder

# Create output directories
plots_dir = storage.data_path / 'results/FinalResultsType1/plots'
predictions_dir = storage.data_path / 'results/FinalResultsType1/predictions'
plots_dir.mkdir(parents=True, exist_ok=True)
predictions_dir.mkdir(parents=True, exist_ok=True)

# Store all results
all_results = {}  # {model: {task: {classifier: {predictions, probabilities, metrics}}}}

print("\n" + "="*80)
print("STEP 2: TRAIN ON TRAIN+DEV AND EVALUATE ON TEST")
print("="*80)

for model_key in MODELS:
    print(f"\n{'-'*80}")
    print(f"MODEL: {model_key.upper()}")
    print(f"{'-'*80}")

    all_results[model_key] = {}

    for task in TASKS:
        print(f"\n  Task: {task.upper()}")

        label_list = LABEL_LISTS[task]
        label_key = 'clarity_label' if task == 'clarity' else 'evasion_label'

        # Load splits
        try:
            test_ds = storage.load_split('test', task=task)
            train_ds = storage.load_split('train', task=task)
            dev_ds = storage.load_split('dev', task=task)
        except FileNotFoundError as e:
            print(f"  ⚠ Error loading splits: {e}")
            continue

        # Load features
        try:
            X_train = storage.load_features(model_key, task, 'train')
            X_dev = storage.load_features(model_key, task, 'dev')
            X_test = storage.load_features(model_key, task, 'test')
        except FileNotFoundError as e:
            print(f"  ⚠ Error loading features: {e}")
            continue

        # Extract labels
        y_train = np.array([train_ds[i][label_key] for i in range(len(train_ds))])
        y_dev = np.array([dev_ds[i][label_key] for i in range(len(dev_ds))])
        y_test = np.array([test_ds[i][label_key] for i in range(len(test_ds))])

        # CRITICAL: Verify test set sizes match expectations
        n_test_ds = len(test_ds)
        n_test_features = X_test.shape[0]
        n_test_labels = len(y_test)

        if n_test_ds != n_test_features or n_test_ds != n_test_labels:
            raise ValueError(
                f"❌ INDEX MISMATCH in {task} task:\n"
                f"   Test dataset: {n_test_ds} samples\n"
                f"   Test features: {n_test_features} samples\n"
                f"   Test labels: {n_test_labels} samples\n"
                f"   All must be equal!"
            )

        # Combine train+dev for final training
        X_train_full = np.vstack([X_train, X_dev])
        y_train_full = np.concatenate([y_train, y_dev])

        print(f"    Training: {X_train_full.shape[0]} samples (train+dev)")
        print(f"    Testing: {X_test.shape[0]} samples (test dataset: {n_test_ds} samples)")
        if task == 'evasion':
            print(f"    ⚠ NOTE: Evasion test set has {n_test_ds} samples (Clarity test set has 308)")
        elif task == 'clarity':
            print(f"    ⚠ NOTE: Clarity test set has {n_test_ds} samples (Evasion test set has 275)")

        # CRITICAL FIX: Notebook 3 gibi - train_classifiers yerine manuel training
        # Her classifier için ayrı LabelEncoder ve encoded labels kullan
        task_results = {}

        for clf_name, clf in classifiers.items():
            print(f"\n    Classifier: {clf_name}")

            # CRITICAL: Her classifier için ayrı LabelEncoder (Notebook 3 gibi)
            le = LabelEncoder()
            y_train_encoded = le.fit_transform(y_train_full)
            y_test_encoded = le.transform(y_test)

            # Train classifier (Pipeline içinde StandardScaler varsa otomatik kullanılır)
            clf.fit(X_train_full, y_train_encoded)

            # Predict on test
            y_test_pred_encoded = clf.predict(X_test)
            y_test_pred = le.inverse_transform(y_test_pred_encoded)  # Decoded for storage

            # Get probabilities (if available)
            if hasattr(clf, 'predict_proba'):
                y_test_proba = clf.predict_proba(X_test)
            else:
                y_test_proba = None

            # CRITICAL FIX: Notebook 3 gibi - ENCODED labels ile compute_all_metrics çağır
            metrics = compute_all_metrics(
                y_test_encoded,        # ENCODED (Notebook 3 gibi)
                y_test_pred_encoded,  # ENCODED (Notebook 3 gibi)
                label_list,           # String labels (Notebook 3 gibi)
                task_name=f"TEST_{model_key}_{task}_{clf_name}"
            )

            # CRITICAL FIX: Use DECODED labels for print_classification_report
            # sklearn's classification_report expects labels to match y_true/y_pred type
            # Since we have string labels, use decoded string predictions
            print_classification_report(
                y_test,                # DECODED string labels (matches label_list type)
                y_test_pred,          # DECODED string predictions (matches label_list type)
                label_list,           # String labels
                task_name=f"TEST - {model_key} - {task} - {clf_name}"
            )

            # Create plots (Notebook 3 gibi - visualize_all_evaluation kullan)
            from src.evaluation.visualizer import visualize_all_evaluation

            if y_test_proba is not None:
                visualize_all_evaluation(
                    y_test_encoded,        # ENCODED (Notebook 3 gibi)
                    y_test_pred_encoded,  # ENCODED (Notebook 3 gibi)
                    y_test_proba,         # Probabilities
                    label_list,
                    task_name=f"TEST - {model_key} - {task}",
                    classifier_name=clf_name,
                    save_dir=str(plots_dir)
                )

            task_results[clf_name] = {
                'predictions': y_test_pred,      # Decoded string labels (for storage)
                'probabilities': y_test_proba,
                'metrics': metrics
            }

            # Save predictions
            storage.save_predictions(
                y_test_pred, model_key, clf_name, task, 'test',
                save_dir=str(predictions_dir),
                metadata_dir='results/FinalResultsType1Results'
            )

            # Save probabilities if available
            if y_test_proba is not None:
                storage.save_probabilities(y_test_proba, model_key, clf_name, task, 'test')

        # Print comparison table (Notebook 3 gibi)
        print_results_table(
            {name: {'metrics': res['metrics'], 'dev_pred': res['predictions'], 'dev_proba': res['probabilities']}
             for name, res in task_results.items()},
            task_name=f"TEST - {model_key} - {task}",
            sort_by="Macro F1",
            show_per_class=True,
            label_list=label_list
        )

        # Create comparison plots (Notebook 3 gibi)
        from src.evaluation.visualizer import visualize_comparison
        visualize_comparison(
            {name: {'dev_pred': res['predictions'], 'dev_proba': res['probabilities'], 'metrics': res['metrics']}
             for name, res in task_results.items()},
            label_list,
            task_name=f"TEST - {model_key} - {task}",
            save_dir=str(plots_dir)
        )

        all_results[model_key][task] = task_results

        # Save results metadata
        experiment_id = f"FINAL_TEST_{model_key}_{task}"
        storage.save_results({
            'split': 'test',
            'model': model_key,
            'task': task,
            'n_test': len(y_test),
            'results': {
                name: {'metrics': res['metrics']}
                for name, res in task_results.items()
            }
        }, experiment_id, save_dir='results/FinalResultsType1Results')

        # ========================================================================
        # ADDITIONAL: For evasion task, map annotator1/2/3 labels to clarity and evaluate
        # ========================================================================
        if task == 'evasion':
            print(f"\n  Additional Evaluations: Annotator1/2/3 → Clarity Mapping")

            # Extract annotator labels from test dataset
            try:
                y_annotator1_evasion = np.array([test_ds[i]['annotator1'] for i in range(len(test_ds))])
                y_annotator2_evasion = np.array([test_ds[i]['annotator2'] for i in range(len(test_ds))])
                y_annotator3_evasion = np.array([test_ds[i]['annotator3'] for i in range(len(test_ds))])
                # Get true clarity labels for evaluation
                y_clarity_true_test = np.array([test_ds[i]['clarity_label'] for i in range(len(test_ds))])
            except KeyError as e:
                print(f"    WARNING: Could not find annotator columns in test dataset: {e}")
                print(f"    Skipping annotator evaluations...")
                continue

            # Encode clarity labels for evaluation
            le_clarity = LabelEncoder()
            y_clarity_true_encoded = le_clarity.fit_transform(y_clarity_true_test)

            # Import mapping function
            from src.models.hierarchical import evasion_to_clarity, evaluate_hierarchical_approach

            # Evaluate each annotator's labels mapped to clarity
            for annotator_name, y_annotator_evasion in [
                ('annotator1_based_clarity', y_annotator1_evasion),
                ('annotator2_based_clarity', y_annotator2_evasion),
                ('annotator3_based_clarity', y_annotator3_evasion)
            ]:
                print(f"\n    Evaluating {annotator_name} (annotator evasion → clarity mapping)...")

                # Map annotator's evasion labels to clarity for comparison
                y_annotator_clarity_mapped = np.array([
                    evasion_to_clarity(str(ev_label)) for ev_label in y_annotator_evasion
                ])
                y_annotator_clarity_encoded = le_clarity.transform(y_annotator_clarity_mapped)

                annotator_results = {}

                # For each classifier, map its evasion predictions to clarity and evaluate
                for clf_name, clf_result in task_results.items():
                    # Get evasion predictions (string labels)
                    y_evasion_pred = clf_result['predictions']

                    # Map evasion predictions to clarity using hierarchical mapping
                    hierarchical_metrics = evaluate_hierarchical_approach(
                        np.zeros(len(y_evasion_pred), dtype=int),  # Dummy evasion_true (not used)
                        y_evasion_pred,  # Evasion predictions (string labels)
                        y_annotator_clarity_encoded,  # Annotator's mapped clarity labels (encoded)
                        LABEL_LISTS['evasion'],
                        LABEL_LISTS['clarity']
                    )

                    # Evaluate mapped predictions against annotator's mapped clarity labels
                    annotator_metrics = compute_all_metrics(
                        y_annotator_clarity_encoded,
                        hierarchical_metrics['predictions'],
                        LABEL_LISTS['clarity'],
                        task_name=f"TEST_{model_key}_{annotator_name}_{clf_name}"
                    )

                    # Decode predictions for print_classification_report (needs string labels)
                    y_annotator_clarity_decoded = le_clarity.inverse_transform(y_annotator_clarity_encoded)
                    y_clarity_pred_decoded = le_clarity.inverse_transform(hierarchical_metrics['predictions'])

                    # Print classification report (use decoded string labels)
                    print_classification_report(
                        y_annotator_clarity_decoded,
                        y_clarity_pred_decoded,
                        LABEL_LISTS['clarity'],
                        task_name=f"TEST - {model_key} - {annotator_name} - {clf_name}"
                    )

                    # Create confusion matrix
                    from src.evaluation.plots import plot_confusion_matrix
                    plot_confusion_matrix(
                        y_annotator_clarity_encoded,
                        hierarchical_metrics['predictions'],
                        LABEL_LISTS['clarity'],
                        task_name=f"TEST - {model_key} - {annotator_name} - {clf_name}",
                        save_path=str(plots_dir / f'confusion_matrix_{clf_name}_TEST_{model_key}_{annotator_name}.png')
                    )

                    annotator_results[clf_name] = {
                        'predictions': hierarchical_metrics['predictions'],  # Clarity predictions
                        'probabilities': None,  # No probabilities for mapping-based approach
                        'metrics': annotator_metrics
                    }

                # Print comparison table for this annotator
                print_results_table(
                    {name: {'metrics': res['metrics']} for name, res in annotator_results.items()},
                    task_name=f"TEST - {model_key} - {annotator_name}",
                    sort_by="Macro F1"
                )

                # Store results in all_results (will appear in summary tables)
                if annotator_name not in all_results[model_key]:
                    all_results[model_key][annotator_name] = {}
                all_results[model_key][annotator_name] = annotator_results

                # Save results metadata
                experiment_id = f"FINAL_TEST_{model_key}_{annotator_name}"
                storage.save_results({
                    'split': 'test',
                    'model': model_key,
                    'task': annotator_name,
                    'n_test': len(y_annotator_clarity_encoded),
                    'results': {
                        name: {'metrics': res['metrics']}
                        for name, res in annotator_results.items()
                    }
                }, experiment_id, save_dir='results/FinalResultsType1Results')

# CRITICAL: Save all_results dictionary immediately after evaluation
# This ensures results are saved even if table generation fails later
print("\nSaving all_results dictionary...")
# Save to GitHub (not Drive) for version control
import pickle
github_results_dir = storage.github_path / 'results/FinalResultsType1'
github_results_dir.mkdir(parents=True, exist_ok=True)
pkl_path = github_results_dir / 'all_results_test_step2.pkl'
with open(pkl_path, 'wb') as f:
    pickle.dump({'final_results': all_results}, f)
print(f"  ✓ Saved all_results to GitHub: {pkl_path}")

# Also save to Drive as backup
storage.save_all_results_dict(
    {'final_results': all_results},
    filename='all_results_test_step2.pkl',
    save_dir='results/FinalResultsType1'
)
print(f"  ✓ Saved all_results to Drive: results/FinalResultsType1/all_results_test_step2.pkl")

print("\n" + "="*80)
print("TRAINING AND EVALUATION COMPLETE")
print("="*80)

# ============================================================================
# STEP 3: HIERARCHICAL EVALUATION (EVASION → CLARITY)
# ============================================================================
# Notebook 3 gibi: Evasion tahminlerini clarity'ye map et ve değerlendir

from src.models.hierarchical import evaluate_hierarchical_approach

print("\n" + "="*80)
print("STEP 3: HIERARCHICAL EVALUATION (EVASION → CLARITY)")
print("="*80)

hierarchical_results = {}

for model_key in MODELS:
    if 'evasion' not in all_results.get(model_key, {}):
        continue

    print(f"\n{'-'*80}")
    print(f"MODEL: {model_key.upper()}")
    print(f"{'-'*80}")

    evasion_results = all_results[model_key]['evasion']

    # Find best classifier by Macro F1
    best_classifier = None
    best_f1 = -1

    for clf_name, clf_result in evasion_results.items():
        f1 = clf_result['metrics'].get('macro_f1', 0.0)
        if f1 > best_f1:
            best_f1 = f1
            best_classifier = clf_name

    if best_classifier is None:
        continue

    print(f"  Using evasion predictions from: {best_classifier} (F1: {best_f1:.4f})")

    # Load test set
    test_ds_evasion = storage.load_split('test', task='evasion')

    # Get predictions and true labels
    y_evasion_pred_test = evasion_results[best_classifier]['predictions']  # String labels
    y_evasion_true_test = np.array([test_ds_evasion[i]['evasion_label'] for i in range(len(test_ds_evasion))])
    y_clarity_true_test = np.array([test_ds_evasion[i]['clarity_label'] for i in range(len(test_ds_evasion))])

    # CRITICAL: Verify index alignment - all arrays must have same length
    n_evasion_test = len(test_ds_evasion)
    n_evasion_pred = len(y_evasion_pred_test)
    n_evasion_true = len(y_evasion_true_test)
    n_clarity_true = len(y_clarity_true_test)

    print(f"    Test set sizes:")
    print(f"      Evasion test dataset: {n_evasion_test} samples")
    print(f"      Evasion predictions: {n_evasion_pred} samples")
    print(f"      Evasion true labels: {n_evasion_true} samples")
    print(f"      Clarity true labels: {n_clarity_true} samples")

    if n_evasion_test != n_evasion_pred:
        raise ValueError(
            f"❌ INDEX MISMATCH: Evasion test dataset ({n_evasion_test}) and predictions ({n_evasion_pred}) have different lengths!\n"
            f"   This indicates predictions are from wrong test set or index misalignment."
        )
    if n_evasion_test != n_evasion_true:
        raise ValueError(
            f"❌ INDEX MISMATCH: Evasion test dataset ({n_evasion_test}) and true labels ({n_evasion_true}) have different lengths!"
        )
    if n_evasion_test != n_clarity_true:
        raise ValueError(
            f"❌ INDEX MISMATCH: Evasion test dataset ({n_evasion_test}) and clarity labels ({n_clarity_true}) have different lengths!\n"
            f"   This indicates clarity labels are from wrong test set (clarity test has 308 samples, evasion has 275)."
        )

    print(f"    ✓ All arrays aligned: {n_evasion_test} samples")

    # Encode labels
    le_evasion = LabelEncoder()
    le_clarity = LabelEncoder()

    y_evasion_true_encoded = le_evasion.fit_transform(y_evasion_true_test)
    y_clarity_true_encoded = le_clarity.fit_transform(y_clarity_true_test)

    # Evaluate hierarchical approach
    hierarchical_metrics = evaluate_hierarchical_approach(
        y_evasion_true_encoded,  # Encoded (not used in mapping, only for signature)
        y_evasion_pred_test,      # String labels
        y_clarity_true_encoded,   # Encoded integers
        LABEL_LISTS['evasion'],
        LABEL_LISTS['clarity']
    )

    hierarchical_results[model_key] = {
        'classifier': best_classifier,
        'metrics': hierarchical_metrics,
        'evasion_f1': best_f1
    }

    print(f"\n  Hierarchical Clarity Performance:")
    print(f"    Accuracy: {hierarchical_metrics['accuracy']:.4f}")
    print(f"    Macro F1: {hierarchical_metrics['macro_f1']:.4f}")
    print(f"    Weighted F1: {hierarchical_metrics['weighted_f1']:.4f}")

    # Decode predictions for print_classification_report (needs string labels)
    y_clarity_pred_decoded = le_clarity.inverse_transform(hierarchical_metrics['predictions'])

    print_classification_report(
        y_clarity_true_test,  # Use original string labels (not encoded)
        y_clarity_pred_decoded,  # Decoded string predictions
        LABEL_LISTS['clarity'],
        task_name=f"TEST - {model_key} - Hierarchical Evasion→Clarity"
    )

    # Add hierarchical results to all_results
    if 'evasion_based_clarity' not in all_results[model_key]:
        all_results[model_key]['evasion_based_clarity'] = {}

    all_results[model_key]['evasion_based_clarity'][best_classifier] = {
        'metrics': hierarchical_metrics,
        'predictions': hierarchical_metrics['predictions'],
        'probabilities': None
    }

    # Save hierarchical predictions
    storage.save_predictions(
        hierarchical_metrics['predictions'],
        model_key, best_classifier, 'evasion_based_clarity', 'test',
        save_dir=str(predictions_dir),
        metadata_dir='results/FinalResultsType1Results'
    )

    # Save results metadata
    experiment_id = f"FINAL_TEST_{model_key}_evasion_based_clarity"
    storage.save_results({
        'split': 'test',
        'model': model_key,
        'task': 'evasion_based_clarity',
        'n_test': len(y_clarity_true_test),
        'evasion_classifier': best_classifier,
        'evasion_f1': best_f1,
        'results': {
            best_classifier: {'metrics': hierarchical_metrics}
        }
    }, experiment_id, save_dir='results/FinalResultsType1Results')

# CRITICAL: Save all_results dictionary after hierarchical evaluation
# This ensures all results (including hierarchical) are saved even if table generation fails
print("\nSaving all_results dictionary (including hierarchical results)...")
# Save to GitHub (not Drive) for version control
import pickle
github_results_dir = storage.github_path / 'results/FinalResultsType1'
github_results_dir.mkdir(parents=True, exist_ok=True)
pkl_path = github_results_dir / 'all_results_test_step3.pkl'
with open(pkl_path, 'wb') as f:
    pickle.dump({'final_results': all_results, 'hierarchical_results': hierarchical_results}, f)
print(f"  ✓ Saved all_results to GitHub: {pkl_path}")

# Also save to Drive as backup
storage.save_all_results_dict(
    {'final_results': all_results, 'hierarchical_results': hierarchical_results},
    filename='all_results_test_step3.pkl',
    save_dir='results/FinalResultsType1'
)
print(f"  ✓ Saved all_results to Drive: results/FinalResultsType1/all_results_test_step3.pkl")

print("\n" + "="*80)
print("HIERARCHICAL EVALUATION COMPLETE")
print("="*80)

# ============================================================================
# STEP 4: FINAL SUMMARY TABLES
# ============================================================================
# Notebook 3 gibi: Model-wise ve Classifier-wise summary tabloları oluştur

from src.evaluation.tables import style_table_paper

print("\n" + "="*80)
print("STEP 4: FINAL SUMMARY TABLES GENERATION")
print("="*80)

# CRITICAL: Try to load all_results if not in memory (notebook was restarted)
# This allows table generation even if notebook was restarted
if 'all_results' not in globals() or not all_results:
    print("\n⚠ all_results not found in memory. Attempting to load from disk...")
    try:
        import pickle
        # Try GitHub first (preferred)
        github_pkl_path = storage.github_path / 'results/FinalResultsType1/all_results_test_step3.pkl'
        if github_pkl_path.exists():
            with open(github_pkl_path, 'rb') as f:
                loaded_data = pickle.load(f)
            if 'final_results' in loaded_data:
                all_results = loaded_data['final_results']
                if 'hierarchical_results' in loaded_data:
                    hierarchical_results = loaded_data['hierarchical_results']
                else:
                    hierarchical_results = {}
                print(f"  ✓ Loaded from GitHub: {github_pkl_path}")
            else:
                raise KeyError("'final_results' not found in loaded data")
        else:
            # Fallback to Drive
            drive_pkl_path = storage.data_path / 'results/FinalResultsType1/all_results_test_step3.pkl'
            if drive_pkl_path.exists():
                with open(drive_pkl_path, 'rb') as f:
                    loaded_data = pickle.load(f)
                if 'final_results' in loaded_data:
                    all_results = loaded_data['final_results']
                    if 'hierarchical_results' in loaded_data:
                        hierarchical_results = loaded_data['hierarchical_results']
                    else:
                        hierarchical_results = {}
                    print(f"  ✓ Loaded from Drive: {drive_pkl_path}")
                else:
                    raise KeyError("'final_results' not found in loaded data")
            else:
                # Try step 2 as fallback (GitHub first)
                github_pkl_path_step2 = storage.github_path / 'results/FinalResultsType1/all_results_test_step2.pkl'
                if github_pkl_path_step2.exists():
                    with open(github_pkl_path_step2, 'rb') as f:
                        loaded_data = pickle.load(f)
                    if 'final_results' in loaded_data:
                        all_results = loaded_data['final_results']
                        hierarchical_results = {}
                        print(f"  ✓ Loaded from GitHub (step2): {github_pkl_path_step2}")
                    else:
                        raise KeyError("'final_results' not found in loaded data")
                else:
                    # Try step 2 from Drive
                    drive_pkl_path_step2 = storage.data_path / 'results/FinalResultsType1/all_results_test_step2.pkl'
                    if drive_pkl_path_step2.exists():
                        with open(drive_pkl_path_step2, 'rb') as f:
                            loaded_data = pickle.load(f)
                        if 'final_results' in loaded_data:
                            all_results = loaded_data['final_results']
                            hierarchical_results = {}
                            print(f"  ✓ Loaded from Drive (step2): {drive_pkl_path_step2}")
                        else:
                            raise KeyError("'final_results' not found in loaded data")
                    else:
                        raise FileNotFoundError(f"Could not find saved all_results. Checked:\n"
                                              f"  - GitHub: {github_pkl_path}\n"
                                              f"  - GitHub: {github_pkl_path_step2}\n"
                                              f"  - Drive: {drive_pkl_path}\n"
                                              f"  - Drive: {drive_pkl_path_step2}")
    except (FileNotFoundError, KeyError, Exception) as e:
        print(f"  ✗ Could not load all_results: {e}")
        print("  ⚠ Please run Cell 8 (STEP 2) and Cell 9 (STEP 3) first!")
        raise
else:
    print("  ✓ all_results found in memory")

# Collect all results for summary
summary_rows = []
# Include clarity task and all clarity-based tasks (evasion_based_clarity + annotator-based clarity)
all_tasks = ['clarity', 'evasion_based_clarity']
# Add annotator-based clarity tasks if they exist in results
annotator_tasks = ['annotator1_based_clarity', 'annotator2_based_clarity', 'annotator3_based_clarity']
# Check if any model has annotator results (they will be added dynamically)
for model_key in MODELS:
    if model_key in all_results:
        for annotator_task in annotator_tasks:
            if annotator_task in all_results[model_key] and annotator_task not in all_tasks:
                all_tasks.append(annotator_task)

for model_key in MODELS:
    if model_key not in all_results:
        continue
    for task in all_tasks:
        if task not in all_results[model_key]:
            continue
        task_results = all_results[model_key][task]
        for clf_name, result in task_results.items():
            if 'metrics' in result:
                metrics = result['metrics']
                summary_rows.append({
                    'model': model_key,
                    'classifier': clf_name,
                    'task': task,
                    'macro_f1': metrics.get('macro_f1', 0.0),
                    'weighted_f1': metrics.get('weighted_f1', 0.0),
                    'accuracy': metrics.get('accuracy', 0.0)
                })

if summary_rows:
    df_summary = pd.DataFrame(summary_rows)

    # CRITICAL FIX: Remove duplicates (like notebook 3)
    # This prevents pivot from failing or aggregating duplicate entries incorrectly
    df_summary = df_summary.drop_duplicates(
        subset=['model', 'classifier', 'task'],
        keep='first'
    )

    # Define desired column order: clarity first, then others
    desired_task_order = ['clarity', 'evasion_based_clarity',
                          'annotator1_based_clarity', 'annotator2_based_clarity',
                          'annotator3_based_clarity']

    # MODEL-WISE TABLES: For each model, Classifier × Tasks
    print("\n" + "="*100)
    print("FINAL SUMMARY — MODEL-WISE (Classifier × Tasks)")
    print("="*100)

    tables_dir = storage.data_path / 'results/FinalResultsType1/tables'
    tables_dir.mkdir(parents=True, exist_ok=True)

    for model_name in sorted(df_summary['model'].unique()):
        print(f"\nMODEL: {model_name.upper()}")

        df_model = df_summary[df_summary['model'] == model_name]

        # CRITICAL FIX: Remove duplicates for this model (like notebook 3)
        df_model = df_model.drop_duplicates(
            subset=['classifier', 'task'],
            keep='first'
        )

        # CRITICAL FIX: Use pivot() instead of pivot_table() (like notebook 3)
        # pivot() will raise an error if duplicates exist (which we've already removed)
        # pivot_table() would aggregate duplicates with mean(), causing wrong results
        try:
            pivot_model = df_model.pivot(
                index='classifier',
                columns='task',
                values='macro_f1'
            )
        except ValueError as e:
            print(f"  ⚠ Error creating pivot table: {e}")
            print(f"  This might indicate duplicate (classifier, task) combinations")
            print(f"  DataFrame shape: {df_model.shape}")
            duplicates = df_model[df_model.duplicated(subset=['classifier', 'task'], keep=False)]
            if not duplicates.empty:
                print(f"  Duplicates found:")
                print(duplicates)
            continue

        if not pivot_model.empty:
            # Reorder columns: clarity first, then others in desired order, then any remaining
            existing_cols = list(pivot_model.columns)
            ordered_cols = []

            # Add clarity first if it exists
            if 'clarity' in existing_cols:
                ordered_cols.append('clarity')

            # Add other desired columns in order (if they exist)
            for task in desired_task_order:
                if task in existing_cols and task not in ordered_cols:
                    ordered_cols.append(task)

            # Add any remaining columns (alphabetically)
            remaining_cols = sorted([col for col in existing_cols if col not in ordered_cols])
            ordered_cols.extend(remaining_cols)

            # Reorder the DataFrame columns
            pivot_model = pivot_model[ordered_cols]

            print(pivot_model.to_string())

            # Style and save table
            styled_model = style_table_paper(pivot_model, precision=4, apply_column_mapping=False)
            storage.save_table(
                styled_model,
                table_name=f'final_test_model_wise_{model_name}',
                formats=['csv', 'html', 'png', 'tex', 'md'],
                save_dir=str(tables_dir),
                use_paper_style=True
            )

    # CLASSIFIER-WISE TABLES: For each classifier, Model × Tasks
    print("\n" + "="*100)
    print("FINAL SUMMARY — CLASSIFIER-WISE (Model × Tasks)")
    print("="*100)

    for clf_name in sorted(df_summary['classifier'].unique()):
        print(f"\nCLASSIFIER: {clf_name.upper()}")

        df_clf = df_summary[df_summary['classifier'] == clf_name]

        # CRITICAL FIX: Remove duplicates for this classifier (like notebook 3)
        df_clf = df_clf.drop_duplicates(
            subset=['model', 'task'],
            keep='first'
        )

        # CRITICAL FIX: Use pivot() instead of pivot_table() (like notebook 3)
        try:
            pivot_clf = df_clf.pivot(
                index='model',
                columns='task',
                values='macro_f1'
            )
        except ValueError as e:
            print(f"  ⚠ Error creating pivot table: {e}")
            print(f"  This might indicate duplicate (model, task) combinations")
            print(f"  DataFrame shape: {df_clf.shape}")
            duplicates = df_clf[df_clf.duplicated(subset=['model', 'task'], keep=False)]
            if not duplicates.empty:
                print(f"  Duplicates found:")
                print(duplicates)
            continue

        if not pivot_clf.empty:
            # Reorder columns: clarity first, then others in desired order, then any remaining
            existing_cols = list(pivot_clf.columns)
            ordered_cols = []

            # Add clarity first if it exists
            if 'clarity' in existing_cols:
                ordered_cols.append('clarity')

            # Add other desired columns in order (if they exist)
            for task in desired_task_order:
                if task in existing_cols and task not in ordered_cols:
                    ordered_cols.append(task)

            # Add any remaining columns (alphabetically)
            remaining_cols = sorted([col for col in existing_cols if col not in ordered_cols])
            ordered_cols.extend(remaining_cols)

            # Reorder the DataFrame columns
            pivot_clf = pivot_clf[ordered_cols]

            print(pivot_clf.to_string())

            # Style and save table
            styled_clf = style_table_paper(pivot_clf, precision=4, apply_column_mapping=False)
            storage.save_table(
                styled_clf,
                table_name=f'final_test_classifier_wise_{clf_name}',
                formats=['csv', 'html', 'png', 'tex', 'md'],
                save_dir=str(tables_dir),
                use_paper_style=True
            )

    # Save all results dictionary
    storage.save_all_results_dict(
        {'final_results': all_results, 'hierarchical_results': hierarchical_results},
        filename='all_results_test.pkl',
        save_dir='results/FinalResultsType1'
    )
else:
    print("WARNING: No results available for summary tables")

print("\n" + "="*80)
print("FINAL SUMMARY TABLES COMPLETE")
print("="*80)