{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Final Evaluation: Test Set Performance\n",
        "\n",
        "================================================================================\n",
        "PURPOSE: Evaluate final models on the held-out test set\n",
        "================================================================================\n",
        "\n",
        "**CRITICAL**: This notebook evaluates on the **TEST set** which was separated\n",
        "at the beginning (in 01_data_split.ipynb) and has NEVER been used for training,\n",
        "model selection, feature selection, or any development decisions.\n",
        "\n",
        "**Evaluation Protocol:**\n",
        "1. Load best model/classifier combinations (based on Dev set results from\n",
        "   03_train_evaluate.ipynb and 04_early_fusion.ipynb)\n",
        "2. Extract features for TEST set (if not already extracted)\n",
        "3. Retrain models on combined Train+Dev data (final training)\n",
        "4. Evaluate on TEST set (unbiased final performance estimate)\n",
        "5. Generate comprehensive final reports, plots, and summary tables\n",
        "\n",
        "**Important Notes:**\n",
        "- Test set is ONLY accessed in this notebook\n",
        "- Models are retrained on Train+Dev before test evaluation\n",
        "- This provides an unbiased estimate of generalization performance\n",
        "- Results from this notebook represent the final competition submission metrics\n",
        "\n",
        "================================================================================\n",
        "INPUTS (What this notebook loads)\n",
        "================================================================================\n",
        "\n",
        "**From GitHub:**\n",
        "- Repository code (cloned automatically if not present)\n",
        "- Source modules from `src/` directory:\n",
        "  - `src.storage.manager` (StorageManager)\n",
        "  - `src.features.extraction` (feature extraction functions)\n",
        "  - `src.models.trainer` (training and evaluation functions)\n",
        "  - `src.models.classifiers` (classifier definitions)\n",
        "  - `src.evaluation.metrics` (metric computation functions)\n",
        "  - `src.evaluation.tables` (results table functions)\n",
        "  - `src.evaluation.visualizer` (plotting functions)\n",
        "\n",
        "**From Google Drive:**\n",
        "- Dataset splits: `splits/dataset_splits.pkl`\n",
        "  - Test split (ONLY accessed in this notebook, 308 samples)\n",
        "  - Train and Dev splits (for combined training)\n",
        "- Feature matrices (if already extracted):\n",
        "  - `features/raw/X_test_{model}_{task}.npy` (may not exist yet)\n",
        "  - `features/raw/X_train_{model}_{task}.npy` (from 02_feature_extraction_separate.ipynb)\n",
        "  - `features/raw/X_dev_{model}_{task}.npy` (from 02_feature_extraction_separate.ipynb)\n",
        "\n",
        "**From HuggingFace Hub:**\n",
        "- Transformer models (if test features need extraction):\n",
        "  - BERT, RoBERTa, DeBERTa, XLNet tokenizers and models\n",
        "\n",
        "================================================================================\n",
        "OUTPUTS (What this notebook saves)\n",
        "================================================================================\n",
        "\n",
        "**To Google Drive:**\n",
        "- Test features (if extracted): `features/raw/X_test_{model}_{task}.npy`\n",
        "  - For each model (bert, roberta, deberta, xlnet)\n",
        "  - For each task (clarity, evasion)\n",
        "  - Shape: (308_samples, 19_features)\n",
        "- Final predictions: `predictions/pred_test_{model}_{classifier}_{task}.npy`\n",
        "  - Hard label predictions for Test set\n",
        "  - For each model/classifier/task combination\n",
        "- Final probabilities: `features/probabilities/probs_test_{model}_{classifier}_{task}.npy`\n",
        "  - Probability distributions for Test set\n",
        "  - For each model/classifier/task combination\n",
        "- Final evaluation plots: `plots/final_evaluation/{model}_{task}_{classifier}/`\n",
        "  - Confusion matrices\n",
        "  - Precision-Recall curves\n",
        "  - ROC curves\n",
        "\n",
        "**To GitHub:**\n",
        "- Test feature metadata: `metadata/features_test_{model}_{task}.json`\n",
        "  - Feature names and dimensions\n",
        "  - Timestamp information\n",
        "- Final results metadata: `results/FINAL_TEST_{model}_{task}.json`\n",
        "  - Final test set metrics\n",
        "  - Model/classifier/task information\n",
        "  - Test sample counts\n",
        "\n",
        "**Evaluation Metrics Computed and Printed:**\n",
        "- Accuracy\n",
        "- Macro Precision, Recall, F1\n",
        "- Weighted Precision, Recall, F1\n",
        "- Per-class metrics (precision, recall, F1, support)\n",
        "- Cohen's Kappa\n",
        "- Matthews Correlation Coefficient\n",
        "- Hamming Loss\n",
        "- Jaccard Score (IoU)\n",
        "- Confusion Matrix\n",
        "\n",
        "**What represents final competition submission:**\n",
        "- All test set predictions and probabilities\n",
        "- Final evaluation metrics (computed on 308 test samples)\n",
        "- Complete evaluation results for all model/classifier/task combinations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# SETUP: Repository Clone, Drive Mount, and Path Configuration\n",
        "# ============================================================================\n",
        "# This cell performs minimal setup required for the notebook to run:\n",
        "# 1. Clones repository from GitHub (if not already present)\n",
        "# 2. Mounts Google Drive for persistent data storage\n",
        "# 3. Configures Python paths and initializes StorageManager\n",
        "# 4. Loads test split (ONLY accessed in this notebook)\n",
        "\n",
        "import shutil\n",
        "import os\n",
        "import subprocess\n",
        "import time\n",
        "import requests\n",
        "import zipfile\n",
        "import sys\n",
        "from pathlib import Path\n",
        "from google.colab import drive\n",
        "import numpy as np\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "# Repository configuration\n",
        "repo_dir = '/content/semeval-context-tree-modular'\n",
        "repo_url = 'https://github.com/EonTechie/semeval-context-tree-modular.git'\n",
        "zip_url = 'https://github.com/EonTechie/semeval-context-tree-modular/archive/refs/heads/main.zip'\n",
        "\n",
        "# Clone repository (if not already present)\n",
        "if not os.path.exists(repo_dir):\n",
        "    print(\"Cloning repository from GitHub...\")\n",
        "    max_retries = 2\n",
        "    clone_success = False\n",
        "    \n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            result = subprocess.run(\n",
        "                ['git', 'clone', repo_url],\n",
        "                cwd='/content',\n",
        "                capture_output=True,\n",
        "                text=True,\n",
        "                timeout=60\n",
        "            )\n",
        "            if result.returncode == 0:\n",
        "                print(\"Repository cloned successfully via git\")\n",
        "                clone_success = True\n",
        "                break\n",
        "            else:\n",
        "                if attempt < max_retries - 1:\n",
        "                    time.sleep(3)\n",
        "        except Exception as e:\n",
        "            if attempt < max_retries - 1:\n",
        "                time.sleep(3)\n",
        "    \n",
        "    # Fallback: Download as ZIP if git clone fails\n",
        "    if not clone_success:\n",
        "        print(\"Git clone failed. Downloading repository as ZIP archive...\")\n",
        "        zip_path = '/tmp/repo.zip'\n",
        "        try:\n",
        "            response = requests.get(zip_url, stream=True, timeout=60)\n",
        "            response.raise_for_status()\n",
        "            with open(zip_path, 'wb') as f:\n",
        "                for chunk in response.iter_content(chunk_size=8192):\n",
        "                    f.write(chunk)\n",
        "            with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "                zip_ref.extractall('/content')\n",
        "            extracted_dir = '/content/semeval-context-tree-modular-main'\n",
        "            if os.path.exists(extracted_dir):\n",
        "                os.rename(extracted_dir, repo_dir)\n",
        "            os.remove(zip_path)\n",
        "            print(\"Repository downloaded and extracted successfully\")\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(f\"Failed to obtain repository: {e}\")\n",
        "\n",
        "# Mount Google Drive (if not already mounted)\n",
        "try:\n",
        "    drive.mount('/content/drive', force_remount=False)\n",
        "except Exception:\n",
        "    pass  # Already mounted\n",
        "\n",
        "# Configure paths\n",
        "BASE_PATH = Path('/content/semeval-context-tree-modular')\n",
        "DATA_PATH = Path('/content/drive/MyDrive/semeval_data')\n",
        "\n",
        "# Verify repository structure exists\n",
        "if not BASE_PATH.exists():\n",
        "    raise RuntimeError(f\"Repository directory not found: {BASE_PATH}\")\n",
        "if not (BASE_PATH / 'src').exists():\n",
        "    raise RuntimeError(f\"src directory not found in repository: {BASE_PATH / 'src'}\")\n",
        "if not (BASE_PATH / 'src' / 'storage' / 'manager.py').exists():\n",
        "    raise RuntimeError(f\"Required file not found: {BASE_PATH / 'src' / 'storage' / 'manager.py'}\")\n",
        "\n",
        "# Add repository to Python path\n",
        "sys.path.insert(0, str(BASE_PATH))\n",
        "\n",
        "# Verify imports work\n",
        "try:\n",
        "    from src.storage.manager import StorageManager\n",
        "    from src.features.extraction import featurize_hf_dataset_in_batches_v2\n",
        "    from src.models.classifiers import get_classifier_dict\n",
        "    from src.evaluation.metrics import compute_all_metrics, print_classification_report\n",
        "    from src.evaluation.tables import print_results_table\n",
        "    from src.evaluation.visualizer import visualize_all_evaluation\n",
        "except ImportError as e:\n",
        "    raise ImportError(\n",
        "        f\"Failed to import required modules. \"\n",
        "        f\"Repository path: {BASE_PATH}, \"\n",
        "        f\"Python path: {sys.path[:3]}, \"\n",
        "        f\"Error: {e}\"\n",
        "    )\n",
        "\n",
        "# Initialize StorageManager\n",
        "storage = StorageManager(\n",
        "    base_path=str(BASE_PATH),\n",
        "    data_path=str(DATA_PATH),\n",
        "    github_path=str(BASE_PATH)\n",
        ")\n",
        "\n",
        "# Test splits will be loaded per-task in the evaluation loop\n",
        "# Clarity and Evasion have different test splits (Evasion uses majority voting)\n",
        "\n",
        "print(\"Setup complete\")\n",
        "print(f\"  Repository: {BASE_PATH}\")\n",
        "print(f\"  Data storage: {DATA_PATH}\")\n",
        "print(f\"\\nCRITICAL: Test sets will be loaded per-task (task-specific splits)\")\n",
        "print(\"         Clarity and Evasion have different test splits due to majority voting\")\n",
        "print(\"         These sets have NEVER been used for training or development!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CONFIGURE MODELS, TASKS, AND CLASSIFIERS\n",
        "# ============================================================================\n",
        "# Defines the models, tasks, and classifiers for final evaluation\n",
        "# NOTE: In practice, you should select best model/classifier based on Dev set\n",
        "# results. For comprehensive comparison, all combinations are evaluated here.\n",
        "\n",
        "MODELS = ['bert', 'bert_political', 'bert_ambiguity', 'roberta', 'deberta', 'xlnet']\n",
        "TASKS = ['clarity', 'evasion']\n",
        "\n",
        "# Label mappings for each task\n",
        "CLARITY_LABELS = ['Clear Reply', 'Ambiguous', 'Clear Non-Reply']\n",
        "EVASION_LABELS = ['Direct Answer', 'Partial Answer', 'Implicit Answer', \n",
        "                  'Uncertainty', 'Refusal', 'Clarification', \n",
        "                  'Question', 'Topic Shift', 'Other']\n",
        "\n",
        "# Initialize classifiers with fixed random seed for reproducibility\n",
        "classifiers = get_classifier_dict(random_state=42)\n",
        "\n",
        "# Configure device (GPU if available, otherwise CPU)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "print(\"Configuration for final evaluation:\")\n",
        "print(f\"  Models: {MODELS}\")\n",
        "print(f\"  Tasks: {TASKS}\")\n",
        "print(f\"  Classifiers: {list(classifiers.keys())}\")\n",
        "print(f\"  Device: {device}\")\n",
        "print(f\"\\nNOTE: Evaluating all model/classifier combinations on TEST set\")\n",
        "print(\"      In practice, select best combination based on Dev set results\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# EXTRACT TEST SET FEATURES (IF NOT ALREADY EXTRACTED)\n",
        "# ============================================================================\n",
        "# Checks if test features already exist, otherwise extracts them\n",
        "# Test features are extracted separately to ensure test set is only accessed\n",
        "# in this final evaluation notebook\n",
        "\n",
        "MODEL_CONFIGS = {\n",
        "    'bert': 'bert-base-uncased',\n",
        "    'bert_political': 'bert-base-uncased',  # TODO: Replace with actual political discourse BERT model\n",
        "    'bert_ambiguity': 'bert-base-uncased',  # TODO: Replace with actual ambiguity-focused BERT model\n",
        "    'roberta': 'roberta-base',\n",
        "    'deberta': 'microsoft/deberta-v3-base',\n",
        "    'xlnet': 'xlnet-base-cased'\n",
        "}\n",
        "\n",
        "for model_key, model_name in MODEL_CONFIGS.items():\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"Processing {model_key.upper()}\")\n",
        "    print(f\"{'='*80}\")\n",
        "    \n",
        "    # Load transformer model and tokenizer from HuggingFace Hub\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModel.from_pretrained(model_name)\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    \n",
        "    # Get model-specific max sequence length from tokenizer or model config\n",
        "    # Use tokenizer.model_max_length if available, otherwise use model.config.max_position_embeddings\n",
        "    if hasattr(tokenizer, 'model_max_length') and tokenizer.model_max_length is not None and tokenizer.model_max_length < 1e10:\n",
        "        max_seq_len = tokenizer.model_max_length\n",
        "    elif hasattr(model.config, 'max_position_embeddings'):\n",
        "        max_seq_len = model.config.max_position_embeddings\n",
        "    else:\n",
        "        # Fallback: use default based on model type\n",
        "        if 'xlnet' in model_name.lower():\n",
        "            max_seq_len = 1024  # XLNet typically supports 1024\n",
        "        else:\n",
        "            max_seq_len = 512   # BERT, RoBERTa, DeBERTa typically 512\n",
        "    \n",
        "    print(f\"Max sequence length for {model_key}: {max_seq_len}\")\n",
        "    \n",
        "    for task in TASKS:\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"Task: {task.upper()}\")\n",
        "        print(f\"{'='*60}\")\n",
        "        \n",
        "        # Check if TEST features already exist in persistent storage\n",
        "        try:\n",
        "            X_test = storage.load_features(model_key, task, 'test')\n",
        "            print(f\"Test features already exist: {model_key}_{task}\")\n",
        "        except FileNotFoundError:\n",
        "            print(f\"Test features not found for {model_key}_{task}, extracting...\")\n",
        "            \n",
        "            # Load task-specific train split to fit TF-IDF vectorizer for this task\n",
        "            # Each task gets its own TF-IDF vectorizer (as in siparismaili01)\n",
        "            train_ds_temp = storage.load_split('train', task=task)\n",
        "            \n",
        "            # Fit TF-IDF on train split for this task\n",
        "            print(f\"  Fitting TF-IDF vectorizer on train split for {task}...\")\n",
        "            _, _, tfidf_vectorizer = featurize_hf_dataset_in_batches_v2(\n",
        "                train_ds_temp,\n",
        "                tokenizer,\n",
        "                model,\n",
        "                device,\n",
        "                batch_size=8,\n",
        "                max_sequence_length=max_seq_len,  # Model-specific max sequence length\n",
        "                question_key='interview_question',  # Key for question text in dataset (original question, NOT 'question' which is paraphrased)\n",
        "                answer_key='interview_answer',  # Key for answer text in dataset (QEvasion uses 'interview_answer')\n",
        "                show_progress=False,  # No progress bar for TF-IDF fitting\n",
        "                tfidf_vectorizer=None  # Fit new TF-IDF on train for this task\n",
        "            )\n",
        "            \n",
        "            # Load task-specific test split\n",
        "            test_ds = storage.load_split('test', task=task)\n",
        "            \n",
        "            # Extract 19 Context Tree features for test set using TF-IDF fitted on train\n",
        "            print(f\"  Extracting test features using TF-IDF fitted on train for {task}...\")\n",
        "            X_test, feature_names, _ = featurize_hf_dataset_in_batches_v2(\n",
        "                test_ds,\n",
        "                tokenizer,\n",
        "                model,\n",
        "                device,\n",
        "                batch_size=8,              # Batch size for feature extraction\n",
        "                max_sequence_length=max_seq_len,  # Model-specific max sequence length\n",
        "                question_key='interview_question',  # Key for question text in dataset (original question, NOT 'question' which is paraphrased)\n",
        "                answer_key='interview_answer',  # Key for answer text in dataset (QEvasion uses 'interview_answer')\n",
        "                show_progress=True,         # Show progress bar\n",
        "                tfidf_vectorizer=tfidf_vectorizer  # Reuse TF-IDF from train (no leakage)\n",
        "            )\n",
        "            \n",
        "            # Save test features to persistent storage\n",
        "            storage.save_features(\n",
        "                X_test, model_key, task, 'test', feature_names\n",
        "            )\n",
        "            \n",
        "            print(f\"  Saved: {X_test.shape[0]} samples, {X_test.shape[1]} features\")\n",
        "    \n",
        "    # Free GPU memory after processing each model\n",
        "    del model, tokenizer\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "print(\"\\nTest feature extraction complete\")\n",
        "print(\"  All test features are now available for final evaluation\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# HIERARCHICAL EVALUATION ON TEST SET: EVASION PREDICTIONS → CLARITY PREDICTIONS\n",
        "# ============================================================================\n",
        "# For each model, uses evasion TEST predictions to generate clarity predictions\n",
        "# via hierarchical mapping, then evaluates against true TEST clarity labels\n",
        "# This is the final evaluation of the hierarchical approach on the test set\n",
        "\n",
        "from src.models.hierarchical import evaluate_hierarchical_approach\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import numpy as np\n",
        "\n",
        "hierarchical_final_results = {}\n",
        "\n",
        "for model in MODELS:\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"MODEL: {model.upper()} - HIERARCHICAL EVASION → CLARITY (TEST SET)\")\n",
        "    print(f\"{'='*80}\")\n",
        "    \n",
        "    # Check if we have evasion test predictions for this model\n",
        "    if 'evasion' not in final_results.get(model, {}):\n",
        "        print(f\"  Skipping {model}: No evasion test predictions available\")\n",
        "        continue\n",
        "    \n",
        "    # Get evasion test results\n",
        "    evasion_test_results = final_results[model]['evasion']\n",
        "    \n",
        "    # Use predictions from best classifier (by Macro F1) or MLP if available\n",
        "    best_classifier = None\n",
        "    best_f1 = -1\n",
        "    \n",
        "    for clf_name, clf_result in evasion_test_results.items():\n",
        "        if 'metrics' in clf_result:\n",
        "            f1 = clf_result['metrics'].get('macro_f1', 0.0)\n",
        "            if f1 > best_f1:\n",
        "                best_f1 = f1\n",
        "                best_classifier = clf_name\n",
        "    \n",
        "    if best_classifier is None:\n",
        "        print(f\"  Skipping {model}: No valid evasion test predictions found\")\n",
        "        continue\n",
        "    \n",
        "    print(f\"  Using evasion test predictions from: {best_classifier} (Macro F1: {best_f1:.4f})\")\n",
        "    \n",
        "    # Load test set for evasion task (275 samples after majority voting)\n",
        "    test_ds_evasion = storage.load_split('test', task='evasion')\n",
        "    \n",
        "    # Get evasion test predictions (string labels from classifier)\n",
        "    # These are the predictions from the best classifier on test set\n",
        "    y_evasion_pred_test_strings = evasion_test_results[best_classifier]['predictions']\n",
        "    \n",
        "    # Get true evasion labels from test set\n",
        "    y_evasion_true_test = np.array([test_ds_evasion[i]['evasion_label'] for i in range(len(test_ds_evasion))])\n",
        "    \n",
        "    # Get true clarity labels from test set (same 275 samples)\n",
        "    y_clarity_true_test = np.array([test_ds_evasion[i]['clarity_label'] for i in range(len(test_ds_evasion))])\n",
        "    \n",
        "    # Encode labels for evaluation (both true and predicted)\n",
        "    le_evasion = LabelEncoder()\n",
        "    le_clarity = LabelEncoder()\n",
        "    \n",
        "    # Fit encoders on true labels to ensure consistent encoding\n",
        "    y_evasion_true_encoded = le_evasion.fit_transform(y_evasion_true_test)\n",
        "    y_clarity_true_encoded = le_clarity.fit_transform(y_clarity_true_test)\n",
        "    \n",
        "    # Encode evasion predictions using the same encoder\n",
        "    # This ensures predictions are in the same encoding space as true labels\n",
        "    y_evasion_pred_test = le_evasion.transform(y_evasion_pred_test_strings)\n",
        "    \n",
        "    # Evaluate hierarchical approach on TEST set\n",
        "    hierarchical_metrics = evaluate_hierarchical_approach(\n",
        "        y_evasion_true_encoded,\n",
        "        y_evasion_pred_test,\n",
        "        y_clarity_true_encoded,\n",
        "        EVASION_LABELS,\n",
        "        CLARITY_LABELS\n",
        "    )\n",
        "    \n",
        "    hierarchical_final_results[model] = {\n",
        "        'classifier': best_classifier,\n",
        "        'metrics': hierarchical_metrics,\n",
        "        'evasion_f1': best_f1,\n",
        "        'n_test': len(y_clarity_true_test)\n",
        "    }\n",
        "    \n",
        "    print(f\"\\n  Hierarchical Clarity Performance (TEST SET):\")\n",
        "    print(f\"    Test samples: {len(y_clarity_true_test)}\")\n",
        "    print(f\"    Accuracy: {hierarchical_metrics['accuracy']:.4f}\")\n",
        "    print(f\"    Macro F1: {hierarchical_metrics['macro_f1']:.4f}\")\n",
        "    print(f\"    Weighted F1: {hierarchical_metrics['weighted_f1']:.4f}\")\n",
        "    \n",
        "    # Print detailed classification report\n",
        "    print_classification_report(\n",
        "        y_clarity_true_encoded,\n",
        "        hierarchical_metrics['predictions'],\n",
        "        CLARITY_LABELS,\n",
        "        task_name=f\"TEST - {model} - Hierarchical Evasion→Clarity\"\n",
        "    )\n",
        "    \n",
        "    # Add to final_results for summary\n",
        "    if 'hierarchical_evasion_to_clarity' not in final_results[model]:\n",
        "        final_results[model]['hierarchical_evasion_to_clarity'] = {}\n",
        "    \n",
        "    final_results[model]['hierarchical_evasion_to_clarity'][best_classifier] = {\n",
        "        'metrics': hierarchical_metrics,\n",
        "        'predictions': hierarchical_metrics['predictions'],\n",
        "        'probabilities': None  # Hierarchical approach doesn't produce probabilities\n",
        "    }\n",
        "    \n",
        "    # Save hierarchical test predictions\n",
        "    storage.save_predictions(\n",
        "        hierarchical_metrics['predictions'],\n",
        "        model, best_classifier, 'hierarchical_evasion_to_clarity', 'test'\n",
        "    )\n",
        "    \n",
        "    # Save hierarchical results to metadata\n",
        "    experiment_id = f\"FINAL_TEST_{model}_hierarchical_evasion_to_clarity\"\n",
        "    storage.save_results({\n",
        "        'split': 'test',\n",
        "        'model': model,\n",
        "        'task': 'hierarchical_evasion_to_clarity',\n",
        "        'n_test': len(y_clarity_true_test),\n",
        "        'evasion_classifier': best_classifier,\n",
        "        'evasion_f1': best_f1,\n",
        "        'results': {\n",
        "            best_classifier: {'metrics': hierarchical_metrics}\n",
        "        }\n",
        "    }, experiment_id)\n",
        "\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(\"HIERARCHICAL EVALUATION ON TEST SET COMPLETE\")\n",
        "print(f\"{'='*80}\")\n",
        "print(\"\\nSummary:\")\n",
        "print(\"  - Evasion test predictions mapped to clarity predictions\")\n",
        "print(\"  - Hierarchical approach evaluated on TEST set (275 samples)\")\n",
        "print(\"  - Test set clarity labels used as gold standard\")\n",
        "print(\"  - Results added to final_results for final summary\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# FINAL EVALUATION ON TEST SET\n",
        "# ============================================================================\n",
        "# Retrains models on combined Train+Dev data and evaluates on Test set\n",
        "# This provides an unbiased estimate of generalization performance\n",
        "# Results represent the final competition submission metrics\n",
        "\n",
        "final_results = {}\n",
        "\n",
        "for model in MODELS:\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"MODEL: {model.upper()} - FINAL EVALUATION ON TEST SET\")\n",
        "    print(f\"{'='*80}\")\n",
        "    \n",
        "    final_results[model] = {}\n",
        "    \n",
        "    for task in TASKS:\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"TASK: {task.upper()}\")\n",
        "        print(f\"{'='*60}\")\n",
        "        \n",
        "        # Select appropriate label list and dataset key for this task\n",
        "        if task == 'clarity':\n",
        "            label_list = CLARITY_LABELS\n",
        "            label_key = 'clarity_label'\n",
        "        else:  # evasion\n",
        "            label_list = EVASION_LABELS\n",
        "            label_key = 'evasion_label'\n",
        "        \n",
        "        # Load task-specific splits (Clarity and Evasion have different splits)\n",
        "        # Evasion splits are filtered by majority voting\n",
        "        test_ds = storage.load_split('test', task=task)\n",
        "        train_ds = storage.load_split('train', task=task)\n",
        "        dev_ds = storage.load_split('dev', task=task)\n",
        "        \n",
        "        print(f\"  Loaded splits for {task}:\")\n",
        "        print(f\"    Train: {len(train_ds)} samples\")\n",
        "        print(f\"    Dev: {len(dev_ds)} samples\")\n",
        "        print(f\"    Test: {len(test_ds)} samples\")\n",
        "        \n",
        "        # Extract test labels\n",
        "        y_test = np.array([test_ds[i][label_key] for i in range(len(test_ds))])\n",
        "        \n",
        "        # Load test features\n",
        "        X_test = storage.load_features(model, task, 'test')\n",
        "        \n",
        "        # Load train features and labels for final training\n",
        "        X_train = storage.load_features(model, task, 'train')\n",
        "        y_train = np.array([train_ds[i][label_key] for i in range(len(train_ds))])\n",
        "        \n",
        "        # Load dev features and labels (will be combined with train)\n",
        "        X_dev = storage.load_features(model, task, 'dev')\n",
        "        y_dev = np.array([dev_ds[i][label_key] for i in range(len(dev_ds))])\n",
        "        \n",
        "        # Combine train + dev for final training\n",
        "        # This maximizes training data before final evaluation\n",
        "        X_train_full = np.vstack([X_train, X_dev])\n",
        "        y_train_full = np.concatenate([y_train, y_dev])\n",
        "        \n",
        "        print(f\"  Training on: {X_train_full.shape[0]} samples (train+dev combined)\")\n",
        "        print(f\"  Testing on: {X_test.shape[0]} samples (test set)\")\n",
        "        \n",
        "        # Train all classifiers on full train+dev and evaluate on test\n",
        "        task_results = {}\n",
        "        \n",
        "        for classifier_name, clf in classifiers.items():\n",
        "            print(f\"\\n  Training {classifier_name} on full train+dev...\")\n",
        "            \n",
        "            # Train on combined train+dev data\n",
        "            clf.fit(X_train_full, y_train_full)\n",
        "            \n",
        "            # Predict on test set\n",
        "            y_test_pred = clf.predict(X_test)\n",
        "            \n",
        "            # Get probability distributions (if classifier supports it)\n",
        "            try:\n",
        "                y_test_proba = clf.predict_proba(X_test)\n",
        "            except AttributeError:\n",
        "                y_test_proba = None\n",
        "            \n",
        "            # Compute comprehensive evaluation metrics\n",
        "            metrics = compute_all_metrics(\n",
        "                y_test, y_test_pred, label_list, \n",
        "                task_name=f\"TEST_{model}_{task}_{classifier_name}\"\n",
        "            )\n",
        "            \n",
        "            # Print detailed classification report\n",
        "            print_classification_report(\n",
        "                y_test, y_test_pred, label_list,\n",
        "                task_name=f\"TEST - {model} - {task} - {classifier_name}\"\n",
        "            )\n",
        "            \n",
        "            # Generate evaluation plots (confusion matrix, PR curves, ROC curves)\n",
        "            if y_test_proba is not None:\n",
        "                visualize_all_evaluation(\n",
        "                    y_test, y_test_pred, y_test_proba, label_list,\n",
        "                    task_name=f\"TEST_{model}_{task}\",\n",
        "                    classifier_name=classifier_name,\n",
        "                    save_dir=str(DATA_PATH / 'plots' / 'final_evaluation')\n",
        "                )\n",
        "            \n",
        "            task_results[classifier_name] = {\n",
        "                'metrics': metrics,\n",
        "                'predictions': y_test_pred,\n",
        "                'probabilities': y_test_proba\n",
        "            }\n",
        "            \n",
        "            # Save test predictions and probabilities to persistent storage\n",
        "            storage.save_predictions(\n",
        "                y_test_pred, model, classifier_name, task, 'test'\n",
        "            )\n",
        "            if y_test_proba is not None:\n",
        "                storage.save_probabilities(\n",
        "                    y_test_proba, model, classifier_name, task, 'test'\n",
        "                )\n",
        "        \n",
        "        # Print results comparison table for this model/task\n",
        "        print_results_table(\n",
        "            {name: {'metrics': res['metrics']} for name, res in task_results.items()},\n",
        "            task_name=f\"TEST - {model} - {task}\",\n",
        "            sort_by=\"Macro F1\"\n",
        "        )\n",
        "        \n",
        "        final_results[model][task] = task_results\n",
        "        \n",
        "        # Save final results to metadata\n",
        "        experiment_id = f\"FINAL_TEST_{model}_{task}\"\n",
        "        storage.save_results({\n",
        "            'split': 'test',\n",
        "            'model': model,\n",
        "            'task': task,\n",
        "            'n_test': len(y_test),\n",
        "            'results': {\n",
        "                name: {'metrics': res['metrics']}\n",
        "                for name, res in task_results.items()\n",
        "            }\n",
        "        }, experiment_id)\n",
        "\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(\"FINAL EVALUATION ON TEST SET COMPLETE\")\n",
        "print(f\"{'='*80}\")\n",
        "print(\"\\nSummary:\")\n",
        "print(\"  - All models retrained on combined Train+Dev data\")\n",
        "print(\"  - Final evaluation performed on held-out Test set\")\n",
        "print(\"  - Test predictions and probabilities saved to Google Drive\")\n",
        "print(\"  - Final results tables and plots generated\")\n",
        "print(\"  - Results represent final competition submission metrics\")\n",
        "```\n",
        "\n",
        "Cell 5:\n",
        "```\n",
        "# ============================================================================\n",
        "# HIERARCHICAL EVALUATION ON TEST SET: EVASION PREDICTIONS → CLARITY PREDICTIONS\n",
        "# ============================================================================\n",
        "# For each model, uses evasion TEST predictions to generate clarity predictions\n",
        "# via hierarchical mapping, then evaluates against true TEST clarity labels\n",
        "# This is the final evaluation of the hierarchical approach on the test set\n",
        "\n",
        "from src.models.hierarchical import evaluate_hierarchical_approach\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import numpy as np\n",
        "\n",
        "hierarchical_final_results = {}\n",
        "\n",
        "for model in MODELS:\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"MODEL: {model.upper()} - HIERARCHICAL EVASION → CLARITY (TEST SET)\")\n",
        "    print(f\"{'='*80}\")\n",
        "    \n",
        "    # Check if we have evasion test predictions for this model\n",
        "    if 'evasion' not in final_results.get(model, {}):\n",
        "        print(f\"  Skipping {model}: No evasion test predictions available\")\n",
        "        continue\n",
        "    \n",
        "    # Get evasion test results\n",
        "    evasion_test_results = final_results[model]['evasion']\n",
        "    \n",
        "    # Use predictions from best classifier (by Macro F1) or MLP if available\n",
        "    best_classifier = None\n",
        "    best_f1 = -1\n",
        "    \n",
        "    for clf_name, clf_result in evasion_test_results.items():\n",
        "        if 'metrics' in clf_result:\n",
        "            f1 = clf_result['metrics'].get('macro_f1', 0.0)\n",
        "            if f1 > best_f1:\n",
        "                best_f1 = f1\n",
        "                best_classifier = clf_name\n",
        "    \n",
        "    if best_classifier is None:\n",
        "        print(f\"  Skipping {model}: No valid evasion test predictions found\")\n",
        "        continue\n",
        "    \n",
        "    print(f\"  Using evasion test predictions from: {best_classifier} (Macro F1: {best_f1:.4f})\")\n",
        "    \n",
        "    # Load test set for evasion task (275 samples after majority voting)\n",
        "    test_ds_evasion = storage.load_split('test', task='evasion')\n",
        "    \n",
        "    # Get evasion test predictions (encoded as integers)\n",
        "    # These are the predictions from the best classifier on test set\n",
        "    y_evasion_pred_test = evasion_test_results[best_classifier]['predictions']\n",
        "    \n",
        "    # Get true evasion labels from test set\n",
        "    y_evasion_true_test = np.array([test_ds_evasion[i]['evasion_label'] for i in range(len(test_ds_evasion))])\n",
        "    \n",
        "    # Get true clarity labels from test set (same 275 samples)\n",
        "    y_clarity_true_test = np.array([test_ds_evasion[i]['clarity_label'] for i in range(len(test_ds_evasion))])\n",
        "    \n",
        "    # Encode labels for evaluation\n",
        "    le_evasion = LabelEncoder()\n",
        "    le_clarity = LabelEncoder()\n",
        "    \n",
        "    y_evasion_true_encoded = le_evasion.fit_transform(y_evasion_true_test)\n",
        "    y_clarity_true_encoded = le_clarity.fit_transform(y_clarity_true_test)\n",
        "    \n",
        "    # Evaluate hierarchical approach on TEST set\n",
        "    hierarchical_metrics = evaluate_hierarchical_approach(\n",
        "        y_evasion_true_encoded,\n",
        "        y_evasion_pred_test,\n",
        "        y_clarity_true_encoded,\n",
        "        EVASION_LABELS,\n",
        "        CLARITY_LABELS\n",
        "    )\n",
        "    \n",
        "    hierarchical_final_results[model] = {\n",
        "        'classifier': best_classifier,\n",
        "        'metrics': hierarchical_metrics,\n",
        "        'evasion_f1': best_f1,\n",
        "        'n_test': len(y_clarity_true_test)\n",
        "    }\n",
        "    \n",
        "    print(f\"\\n  Hierarchical Clarity Performance (TEST SET):\")\n",
        "    print(f\"    Test samples: {len(y_clarity_true_test)}\")\n",
        "    print(f\"    Accuracy: {hierarchical_metrics['accuracy']:.4f}\")\n",
        "    print(f\"    Macro F1: {hierarchical_metrics['macro_f1']:.4f}\")\n",
        "    print(f\"    Weighted F1: {hierarchical_metrics['weighted_f1']:.4f}\")\n",
        "    \n",
        "    # Print detailed classification report\n",
        "    print_classification_report(\n",
        "        y_clarity_true_encoded,\n",
        "        hierarchical_metrics['predictions'],\n",
        "        CLARITY_LABELS,\n",
        "        task_name=f\"TEST - {model} - Hierarchical Evasion→Clarity\"\n",
        "    )\n",
        "    \n",
        "    # Add to final_results for summary\n",
        "    if 'hierarchical_evasion_to_clarity' not in final_results[model]:\n",
        "        final_results[model]['hierarchical_evasion_to_clarity'] = {}\n",
        "    \n",
        "    final_results[model]['hierarchical_evasion_to_clarity'][best_classifier] = {\n",
        "        'metrics': hierarchical_metrics,\n",
        "        'predictions': hierarchical_metrics['predictions'],\n",
        "        'probabilities': None  # Hierarchical approach doesn't produce probabilities\n",
        "    }\n",
        "    \n",
        "    # Save hierarchical test predictions\n",
        "    storage.save_predictions(\n",
        "        hierarchical_metrics['predictions'],\n",
        "        model, best_classifier, 'hierarchical_evasion_to_clarity', 'test'\n",
        "    )\n",
        "    \n",
        "    # Save hierarchical results to metadata\n",
        "    experiment_id = f\"FINAL_TEST_{model}_hierarchical_evasion_to_clarity\"\n",
        "    storage.save_results({\n",
        "        'split': 'test',\n",
        "        'model': model,\n",
        "        'task': 'hierarchical_evasion_to_clarity',\n",
        "        'n_test': len(y_clarity_true_test),\n",
        "        'evasion_classifier': best_classifier,\n",
        "        'evasion_f1': best_f1,\n",
        "        'results': {\n",
        "            best_classifier: {'metrics': hierarchical_metrics}\n",
        "        }\n",
        "    }, experiment_id)\n",
        "\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(\"HIERARCHICAL EVALUATION ON TEST SET COMPLETE\")\n",
        "print(f\"{'='*80}\")\n",
        "print(\"\\nSummary:\")\n",
        "print(\"  - Evasion test predictions mapped to clarity predictions\")\n",
        "print(\"  - Hierarchical approach evaluated on TEST set (275 samples)\")\n",
        "print(\"  - Test set clarity labels used as gold standard\")\n",
        "print(\"  - Results added to final_results for final summary\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
