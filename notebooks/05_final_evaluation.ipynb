{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size: 10px;\">\n",
    "\n",
    "# Final Evaluation: Test Set Performance\n",
    "\n",
    "================================================================================\n",
    "PURPOSE: Evaluate final models on the held-out test set\n",
    "================================================================================\n",
    "\n",
    "**CRITICAL**: This notebook evaluates on the **TEST set** which was separated\n",
    "at the beginning (in 01_data_split.ipynb) and has NEVER been used for training,\n",
    "model selection, feature selection, or any development decisions.\n",
    "\n",
    "**Evaluation Protocol:**\n",
    "1. Load best model/classifier combinations (based on Dev set results from\n",
    "   03_train_evaluate.ipynb and 04_early_fusion.ipynb)\n",
    "2. Extract features for TEST set (if not already extracted)\n",
    "3. Retrain models on combined Train+Dev data (final training)\n",
    "4. Evaluate on TEST set (unbiased final performance estimate)\n",
    "5. Generate comprehensive final reports, plots, and summary tables\n",
    "\n",
    "**Important Notes:**\n",
    "- Test set is ONLY accessed in this notebook\n",
    "- Models are retrained on Train+Dev before test evaluation\n",
    "- This provides an unbiased estimate of generalization performance\n",
    "- Results from this notebook represent the final competition submission metrics\n",
    "\n",
    "================================================================================\n",
    "INPUTS (What this notebook loads)\n",
    "================================================================================\n",
    "\n",
    "**From GitHub:**\n",
    "- Repository code (cloned automatically if not present)\n",
    "- Source modules from `src/` directory:\n",
    "  - `src.storage.manager` (StorageManager)\n",
    "  - `src.features.extraction` (feature extraction functions)\n",
    "  - `src.models.trainer` (training and evaluation functions)\n",
    "  - `src.models.classifiers` (classifier definitions)\n",
    "  - `src.evaluation.metrics` (metric computation functions)\n",
    "  - `src.evaluation.tables` (results table functions)\n",
    "  - `src.evaluation.visualizer` (plotting functions)\n",
    "\n",
    "**From Google Drive:**\n",
    "- Dataset splits: `splits/dataset_splits.pkl`\n",
    "  - Test split (ONLY accessed in this notebook, 308 samples)\n",
    "  - Train and Dev splits (for combined training)\n",
    "- Feature matrices (if already extracted):\n",
    "  - `features/raw/X_test_{model}_{task}.npy` (may not exist yet)\n",
    "  - `features/raw/X_train_{model}_{task}.npy` (from 02_feature_extraction_separate.ipynb)\n",
    "  - `features/raw/X_dev_{model}_{task}.npy` (from 02_feature_extraction_separate.ipynb)\n",
    "\n",
    "**From HuggingFace Hub:**\n",
    "- Transformer models (if test features need extraction):\n",
    "  - BERT, RoBERTa, DeBERTa, XLNet tokenizers and models\n",
    "\n",
    "================================================================================\n",
    "OUTPUTS (What this notebook saves)\n",
    "================================================================================\n",
    "\n",
    "**To Google Drive:**\n",
    "- Test features (if extracted): `features/raw/X_test_{model}_{task}.npy`\n",
    "  - For each model (bert, roberta, deberta, xlnet)\n",
    "  - For each task (clarity, evasion)\n",
    "  - Shape: (308_samples, 19_features)\n",
    "- Final predictions: `predictions/pred_test_{model}_{classifier}_{task}.npy`\n",
    "  - Hard label predictions for Test set\n",
    "  - For each model/classifier/task combination\n",
    "- Final probabilities: `features/probabilities/probs_test_{model}_{classifier}_{task}.npy`\n",
    "  - Probability distributions for Test set\n",
    "  - For each model/classifier/task combination\n",
    "- Final evaluation plots: `plots/final_evaluation/{model}_{task}_{classifier}/`\n",
    "  - Confusion matrices\n",
    "  - Precision-Recall curves\n",
    "  - ROC curves\n",
    "\n",
    "**To GitHub:**\n",
    "- Test feature metadata: `metadata/features_test_{model}_{task}.json`\n",
    "  - Feature names and dimensions\n",
    "  - Timestamp information\n",
    "- Final results metadata: `results/FINAL_TEST_{model}_{task}.json`\n",
    "  - Final test set metrics\n",
    "  - Model/classifier/task information\n",
    "  - Test sample counts\n",
    "\n",
    "**Evaluation Metrics Computed and Printed:**\n",
    "- Accuracy\n",
    "- Macro Precision, Recall, F1\n",
    "- Weighted Precision, Recall, F1\n",
    "- Per-class metrics (precision, recall, F1, support)\n",
    "- Cohen's Kappa\n",
    "- Matthews Correlation Coefficient\n",
    "- Hamming Loss\n",
    "- Jaccard Score (IoU)\n",
    "- Confusion Matrix\n",
    "\n",
    "**What represents final competition submission:**\n",
    "- All test set predictions and probabilities\n",
    "- Final evaluation metrics (computed on 308 test samples)\n",
    "- Complete evaluation results for all model/classifier/task combinations\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SETUP: Repository Clone, Drive Mount, and Path Configuration\n",
    "# ============================================================================\n",
    "# This cell performs minimal setup required for the notebook to run:\n",
    "# 1. Clones repository from GitHub (if not already present)\n",
    "# 2. Mounts Google Drive for persistent data storage\n",
    "# 3. Configures Python paths and initializes StorageManager\n",
    "# 4. Loads test split (ONLY accessed in this notebook)\n",
    "\n",
    "import shutil\n",
    "import os\n",
    "import subprocess\n",
    "import time\n",
    "import requests\n",
    "import zipfile\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from google.colab import drive\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# Repository configuration\n",
    "repo_dir = '/content/semeval-context-tree-modular'\n",
    "repo_url = 'https://github.com/EonTechie/semeval-context-tree-modular.git'\n",
    "zip_url = 'https://github.com/EonTechie/semeval-context-tree-modular/archive/refs/heads/main.zip'\n",
    "\n",
    "# Clone repository (if not already present)\n",
    "if not os.path.exists(repo_dir):\n",
    "    print(\"Cloning repository from GitHub...\")\n",
    "    max_retries = 2\n",
    "    clone_success = False\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            result = subprocess.run(\n",
    "                ['git', 'clone', repo_url],\n",
    "                cwd='/content',\n",
    "                capture_output=True,\n",
    "                text=True,\n",
    "                timeout=60\n",
    "            )\n",
    "            if result.returncode == 0:\n",
    "                print(\"Repository cloned successfully via git\")\n",
    "                clone_success = True\n",
    "                break\n",
    "            else:\n",
    "                if attempt < max_retries - 1:\n",
    "                    time.sleep(3)\n",
    "        except Exception as e:\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(3)\n",
    "    \n",
    "    # Fallback: Download as ZIP if git clone fails\n",
    "    if not clone_success:\n",
    "        print(\"Git clone failed. Downloading repository as ZIP archive...\")\n",
    "        zip_path = '/tmp/repo.zip'\n",
    "        try:\n",
    "            response = requests.get(zip_url, stream=True, timeout=60)\n",
    "            response.raise_for_status()\n",
    "            with open(zip_path, 'wb') as f:\n",
    "                for chunk in response.iter_content(chunk_size=8192):\n",
    "                    f.write(chunk)\n",
    "            with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "                zip_ref.extractall('/content')\n",
    "            extracted_dir = '/content/semeval-context-tree-modular-main'\n",
    "            if os.path.exists(extracted_dir):\n",
    "                os.rename(extracted_dir, repo_dir)\n",
    "            os.remove(zip_path)\n",
    "            print(\"Repository downloaded and extracted successfully\")\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to obtain repository: {e}\")\n",
    "\n",
    "# Mount Google Drive (if not already mounted)\n",
    "try:\n",
    "    drive.mount('/content/drive', force_remount=False)\n",
    "except Exception:\n",
    "    pass  # Already mounted\n",
    "\n",
    "# Configure paths\n",
    "BASE_PATH = Path('/content/semeval-context-tree-modular')\n",
    "DATA_PATH = Path('/content/drive/MyDrive/semeval_data')\n",
    "\n",
    "# Verify repository structure exists\n",
    "if not BASE_PATH.exists():\n",
    "    raise RuntimeError(f\"Repository directory not found: {BASE_PATH}\")\n",
    "if not (BASE_PATH / 'src').exists():\n",
    "    raise RuntimeError(f\"src directory not found in repository: {BASE_PATH / 'src'}\")\n",
    "if not (BASE_PATH / 'src' / 'storage' / 'manager.py').exists():\n",
    "    raise RuntimeError(f\"Required file not found: {BASE_PATH / 'src' / 'storage' / 'manager.py'}\")\n",
    "\n",
    "# Add repository to Python path\n",
    "sys.path.insert(0, str(BASE_PATH))\n",
    "\n",
    "# Verify imports work\n",
    "try:\n",
    "    from src.storage.manager import StorageManager\n",
    "    from src.features.extraction import featurize_hf_dataset_in_batches_v2\n",
    "    from src.models.classifiers import get_classifier_dict\n",
    "    from src.evaluation.metrics import compute_all_metrics, print_classification_report\n",
    "    from src.evaluation.tables import print_results_table\n",
    "    from src.evaluation.visualizer import visualize_all_evaluation\n",
    "except ImportError as e:\n",
    "    raise ImportError(\n",
    "        f\"Failed to import required modules. \"\n",
    "        f\"Repository path: {BASE_PATH}, \"\n",
    "        f\"Python path: {sys.path[:3]}, \"\n",
    "        f\"Error: {e}\"\n",
    "    )\n",
    "\n",
    "# Initialize StorageManager\n",
    "storage = StorageManager(\n",
    "    base_path=str(BASE_PATH),\n",
    "    data_path=str(DATA_PATH),\n",
    "    github_path=str(BASE_PATH)\n",
    ")\n",
    "\n",
    "# Test splits will be loaded per-task in the evaluation loop\n",
    "# Clarity and Evasion have different test splits (Evasion uses majority voting)\n",
    "\n",
    "print(\"Setup complete\")\n",
    "print(f\"  Repository: {BASE_PATH}\")\n",
    "print(f\"  Data storage: {DATA_PATH}\")\n",
    "print(f\"\\nCRITICAL: Test sets will be loaded per-task (task-specific splits)\")\n",
    "print(\"         Clarity and Evasion have different test splits due to majority voting\")\n",
    "print(\"         These sets have NEVER been used for training or development!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CONFIGURE MODELS, TASKS, AND CLASSIFIERS FOR FINAL EVALUATION\n",
    "# ============================================================================\n",
    "# Defines the models, tasks, and classifiers for final evaluation\n",
    "# NOTE: In practice, you should select best model/classifier based on Dev set\n",
    "# results. For comprehensive comparison, all combinations are evaluated here.\n",
    "\n",
    "from src.models.final_evaluation import run_final_evaluation\n",
    "\n",
    "MODELS = ['bert', 'bert_political', 'bert_ambiguity', 'roberta', 'deberta', 'xlnet']\n",
    "TASKS = ['clarity', 'evasion']\n",
    "\n",
    "# Label mappings for each task\n",
    "CLARITY_LABELS = ['Ambivalent', 'Clear Non-Reply', 'Clear Reply']\n",
    "EVASION_LABELS = ['Claims ignorance', 'Clarification', 'Declining to answer', \n",
    "                  'Deflection', 'Dodging', 'Explicit', \n",
    "                  'General', 'Implicit', 'Partial/half-answer']\n",
    "\n",
    "# Model configurations (HuggingFace model names)\n",
    "MODEL_CONFIGS = {\n",
    "    'bert': 'bert-base-uncased',\n",
    "    'bert_political': 'bert-base-uncased',  # TODO: Replace with actual political discourse BERT model\n",
    "    'bert_ambiguity': 'bert-base-uncased',  # TODO: Replace with actual ambiguity-focused BERT model\n",
    "    'roberta': 'roberta-base',\n",
    "    'deberta': 'microsoft/deberta-v3-base',\n",
    "    'xlnet': 'xlnet-base-cased'\n",
    "}\n",
    "\n",
    "# Max sequence lengths for each model\n",
    "MODEL_MAX_LENGTHS = {\n",
    "    'bert': 512,\n",
    "    'bert_political': 512,\n",
    "    'bert_ambiguity': 512,\n",
    "    'roberta': 512,\n",
    "    'deberta': 512,\n",
    "    'xlnet': 1024\n",
    "}\n",
    "\n",
    "# Label lists for each task\n",
    "LABEL_LISTS = {\n",
    "    'clarity': CLARITY_LABELS,\n",
    "    'evasion': EVASION_LABELS\n",
    "}\n",
    "\n",
    "# Initialize classifiers with fixed random seed for reproducibility\n",
    "classifiers = get_classifier_dict(random_state=42)\n",
    "\n",
    "# Configure device (GPU if available, otherwise CPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(\"Configuration for final evaluation:\")\n",
    "print(f\"  Models: {MODELS}\")\n",
    "print(f\"  Tasks: {TASKS}\")\n",
    "print(f\"  Classifiers: {list(classifiers.keys())}\")\n",
    "print(f\"  Device: {device}\")\n",
    "print(f\"\\nNOTE: Evaluating all model/classifier combinations on TEST set\")\n",
    "print(\"      In practice, select best combination based on Dev set results\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# FINAL EVALUATION: YARIŞMAYA UYGUN TEK FONKSİYON ÇAĞRISI\n",
    "# ============================================================================\n",
    "# Bu fonksiyon tüm final evaluation işlemlerini yapar:\n",
    "# 1. Test feature'larını extract eder (yoksa) veya Drive'dan yükler\n",
    "# 2. Train+Dev üzerinde tüm model×classifier kombinasyonlarını eğitir\n",
    "# 3. Test setinde tahmin yapar ve metrikleri hesaplar\n",
    "# 4. Hierarchical evaluation yapar (evasion → clarity)\n",
    "# 5. Sonuçları kaydeder ve görselleştirir\n",
    "\n",
    "results = run_final_evaluation(\n",
    "    storage=storage,\n",
    "    models=MODELS,\n",
    "    tasks=TASKS,\n",
    "    model_configs=MODEL_CONFIGS,\n",
    "    model_max_lengths=MODEL_MAX_LENGTHS,\n",
    "    label_lists=LABEL_LISTS,\n",
    "    device=device,\n",
    "    classifiers=classifiers,\n",
    "    random_state=42,\n",
    "    batch_size=8,\n",
    "    save_results=True,    # Sonuçları kaydet\n",
    "    create_plots=True     # Plot'ları oluştur\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL EVALUATION COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nResults are available in the 'results' dictionary:\")\n",
    "print(\"  - results['final_results']: Model/task/classifier predictions and metrics\")\n",
    "print(\"  - results['hierarchical_results']: Hierarchical evasion→clarity results\")\n",
    "print(\"\\nAll predictions and plots have been saved to Google Drive.\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
