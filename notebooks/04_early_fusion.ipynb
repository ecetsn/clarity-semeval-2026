{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Early Fusion: Concatenated Multi-Model Features\n",
        "\n",
        "================================================================================\n",
        "PURPOSE: Perform early fusion by concatenating features from all models\n",
        "================================================================================\n",
        "\n",
        "This notebook implements early fusion at the feature level by concatenating\n",
        "Context Tree features extracted from multiple transformer models. This approach\n",
        "combines complementary information from different models to potentially improve\n",
        "classification performance.\n",
        "\n",
        "**Fusion Strategy:**\n",
        "- **Early Fusion (Feature-level)**: Concatenate features from all models\n",
        "- Fused feature vector = [BERT features | RoBERTa features | DeBERTa features | XLNet features]\n",
        "- Total feature dimension = sum of individual model feature dimensions\n",
        "\n",
        "**Workflow:**\n",
        "1. Load features from all models (saved by 02_feature_extraction_separate.ipynb)\n",
        "2. Concatenate features horizontally to create fused feature vectors\n",
        "3. Train classifiers on fused features\n",
        "4. Evaluate on Dev set and compare with individual model performance\n",
        "5. Save fused features, predictions, and results\n",
        "\n",
        "**Output:**\n",
        "- Fused feature matrices saved to Google Drive\n",
        "- Predictions and probabilities for fused features\n",
        "- Results tables and evaluation plots\n",
        "- Comparison with individual model results\n",
        "\n",
        "================================================================================\n",
        "INPUTS (What this notebook loads)\n",
        "================================================================================\n",
        "\n",
        "**From GitHub:**\n",
        "- Repository code (cloned automatically if not present)\n",
        "- Source modules from `src/` directory:\n",
        "  - `src.storage.manager` (StorageManager)\n",
        "  - `src.features.fusion` (feature fusion functions)\n",
        "  - `src.models.trainer` (training and evaluation functions)\n",
        "  - `src.models.classifiers` (classifier definitions)\n",
        "\n",
        "**From Google Drive:**\n",
        "- Dataset splits: `splits/dataset_splits.pkl`\n",
        "  - Train split (for label extraction)\n",
        "  - Dev split (for label extraction)\n",
        "- Feature matrices: `features/raw/X_{split}_{model}_{task}.npy`\n",
        "  - For all models (bert, roberta, deberta, xlnet)\n",
        "  - For each task (clarity, evasion)\n",
        "  - For Train and Dev splits\n",
        "  - Loaded via `storage.load_features(model, task, split)`\n",
        "- Feature metadata: `metadata/features_{split}_{model}_{task}.json`\n",
        "  - For feature name extraction\n",
        "\n",
        "**From HuggingFace Hub:**\n",
        "- Nothing (all features already extracted)\n",
        "\n",
        "================================================================================\n",
        "OUTPUTS (What this notebook saves)\n",
        "================================================================================\n",
        "\n",
        "**To Google Drive:**\n",
        "- Fused features: `features/fused/X_{split}_fused_{models}_{task}.npy`\n",
        "  - Concatenated features from all models\n",
        "  - For Train and Dev splits\n",
        "  - Shape: (N_samples, sum_of_all_model_features)\n",
        "- Predictions: `predictions/pred_{split}_fused_{classifier}_{task}.npy`\n",
        "  - Hard label predictions for Dev set\n",
        "  - For each classifier/task combination\n",
        "- Probabilities: `features/probabilities/probs_{split}_fused_{classifier}_{task}.npy`\n",
        "  - Probability distributions for Dev set\n",
        "  - For each classifier/task combination\n",
        "- Evaluation plots: `plots/early_fusion/{task}_{classifier}/`\n",
        "  - Confusion matrices\n",
        "  - Precision-Recall curves\n",
        "  - ROC curves\n",
        "\n",
        "**To GitHub:**\n",
        "- Fused feature metadata: `metadata/fused_{split}_{models}_{task}.json`\n",
        "  - Fused feature names\n",
        "  - Component model information\n",
        "  - Timestamp and data paths\n",
        "- Results metadata: `results/early_fusion_{task}.json`\n",
        "  - Metrics for each classifier\n",
        "  - Fusion method information\n",
        "  - Train/Dev sample counts\n",
        "\n",
        "**Evaluation Metrics Computed and Printed:**\n",
        "- Same comprehensive metrics as 03_train_evaluate.ipynb\n",
        "- Comparison with individual model results from 03_train_evaluate.ipynb\n",
        "\n",
        "**What gets passed to next notebook:**\n",
        "- Fused features saved to persistent storage\n",
        "- Predictions and probabilities for comparison\n",
        "- Results metadata for final evaluation comparison\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# SETUP: Repository Clone, Drive Mount, and Path Configuration\n",
        "# ============================================================================\n",
        "# This cell performs minimal setup required for the notebook to run:\n",
        "# 1. Clones repository from GitHub (if not already present)\n",
        "# 2. Mounts Google Drive for persistent data storage\n",
        "# 3. Configures Python paths and initializes StorageManager\n",
        "# 4. Loads data splits and features created in previous notebooks\n",
        "\n",
        "import shutil\n",
        "import os\n",
        "import subprocess\n",
        "import time\n",
        "import requests\n",
        "import zipfile\n",
        "import sys\n",
        "from pathlib import Path\n",
        "from google.colab import drive\n",
        "import numpy as np\n",
        "\n",
        "# Repository configuration\n",
        "repo_dir = '/content/semeval-context-tree-modular'\n",
        "repo_url = 'https://github.com/EonTechie/semeval-context-tree-modular.git'\n",
        "zip_url = 'https://github.com/EonTechie/semeval-context-tree-modular/archive/refs/heads/main.zip'\n",
        "\n",
        "# Clone repository (if not already present)\n",
        "if not os.path.exists(repo_dir):\n",
        "    print(\"Cloning repository from GitHub...\")\n",
        "    max_retries = 2\n",
        "    clone_success = False\n",
        "    \n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            result = subprocess.run(\n",
        "                ['git', 'clone', repo_url],\n",
        "                cwd='/content',\n",
        "                capture_output=True,\n",
        "                text=True,\n",
        "                timeout=60\n",
        "            )\n",
        "            if result.returncode == 0:\n",
        "                print(\"Repository cloned successfully via git\")\n",
        "                clone_success = True\n",
        "                break\n",
        "            else:\n",
        "                if attempt < max_retries - 1:\n",
        "                    time.sleep(3)\n",
        "        except Exception as e:\n",
        "            if attempt < max_retries - 1:\n",
        "                time.sleep(3)\n",
        "    \n",
        "    # Fallback: Download as ZIP if git clone fails\n",
        "    if not clone_success:\n",
        "        print(\"Git clone failed. Downloading repository as ZIP archive...\")\n",
        "        zip_path = '/tmp/repo.zip'\n",
        "        try:\n",
        "            response = requests.get(zip_url, stream=True, timeout=60)\n",
        "            response.raise_for_status()\n",
        "            with open(zip_path, 'wb') as f:\n",
        "                for chunk in response.iter_content(chunk_size=8192):\n",
        "                    f.write(chunk)\n",
        "            with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "                zip_ref.extractall('/content')\n",
        "            extracted_dir = '/content/semeval-context-tree-modular-main'\n",
        "            if os.path.exists(extracted_dir):\n",
        "                os.rename(extracted_dir, repo_dir)\n",
        "            os.remove(zip_path)\n",
        "            print(\"Repository downloaded and extracted successfully\")\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(f\"Failed to obtain repository: {e}\")\n",
        "\n",
        "# Mount Google Drive (if not already mounted)\n",
        "try:\n",
        "    drive.mount('/content/drive', force_remount=False)\n",
        "except Exception:\n",
        "    pass  # Already mounted\n",
        "\n",
        "# Configure paths\n",
        "BASE_PATH = Path('/content/semeval-context-tree-modular')\n",
        "DATA_PATH = Path('/content/drive/MyDrive/semeval_data')\n",
        "\n",
        "# Verify repository structure exists\n",
        "if not BASE_PATH.exists():\n",
        "    raise RuntimeError(f\"Repository directory not found: {BASE_PATH}\")\n",
        "if not (BASE_PATH / 'src').exists():\n",
        "    raise RuntimeError(f\"src directory not found in repository: {BASE_PATH / 'src'}\")\n",
        "if not (BASE_PATH / 'src' / 'storage' / 'manager.py').exists():\n",
        "    raise RuntimeError(f\"Required file not found: {BASE_PATH / 'src' / 'storage' / 'manager.py'}\")\n",
        "\n",
        "# Add repository to Python path\n",
        "sys.path.insert(0, str(BASE_PATH))\n",
        "\n",
        "# Verify imports work\n",
        "try:\n",
        "    from src.storage.manager import StorageManager\n",
        "    from src.features.fusion import fuse_attention_features\n",
        "    from src.models.trainer import train_and_evaluate\n",
        "    from src.models.classifiers import get_classifier_dict\n",
        "except ImportError as e:\n",
        "    raise ImportError(\n",
        "        f\"Failed to import required modules. \"\n",
        "        f\"Repository path: {BASE_PATH}, \"\n",
        "        f\"Python path: {sys.path[:3]}, \"\n",
        "        f\"Error: {e}\"\n",
        "    )\n",
        "\n",
        "# Initialize StorageManager\n",
        "storage = StorageManager(\n",
        "    base_path=str(BASE_PATH),\n",
        "    data_path=str(DATA_PATH),\n",
        "    github_path=str(BASE_PATH)\n",
        ")\n",
        "\n",
        "# Load data splits for label extraction\n",
        "train_ds = storage.load_split('train')\n",
        "dev_ds = storage.load_split('dev')\n",
        "\n",
        "print(\"Setup complete\")\n",
        "print(f\"  Repository: {BASE_PATH}\")\n",
        "print(f\"  Data storage: {DATA_PATH}\")\n",
        "print(f\"  Train samples: {len(train_ds)}\")\n",
        "print(f\"  Dev samples: {len(dev_ds)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CONFIGURE MODELS, TASKS, AND CLASSIFIERS\n",
        "# ============================================================================\n",
        "# Defines the models to fuse, tasks to perform, and classifiers to train\n",
        "# Label mappings are defined for clarity (3-class) and evasion (9-class) tasks\n",
        "\n",
        "MODELS = ['bert', 'bert_political', 'bert_ambiguity', 'roberta', 'deberta', 'xlnet']\n",
        "TASKS = ['clarity', 'evasion']\n",
        "\n",
        "# Label mappings for each task\n",
        "CLARITY_LABELS = ['Clear Reply', 'Ambiguous', 'Clear Non-Reply']\n",
        "EVASION_LABELS = ['Direct Answer', 'Partial Answer', 'Implicit Answer', \n",
        "                  'Uncertainty', 'Refusal', 'Clarification', \n",
        "                  'Question', 'Topic Shift', 'Other']\n",
        "\n",
        "# Initialize classifiers with fixed random seed for reproducibility\n",
        "classifiers = get_classifier_dict(random_state=42)\n",
        "\n",
        "print(\"Configuration:\")\n",
        "print(f\"  Models to fuse: {MODELS}\")\n",
        "print(f\"  Tasks: {TASKS}\")\n",
        "print(f\"  Classifiers: {list(classifiers.keys())}\")\n",
        "print(f\"  Fusion method: Early fusion (feature concatenation)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# PERFORM EARLY FUSION AND TRAIN CLASSIFIERS\n",
        "# ============================================================================\n",
        "# For each task, loads features from all models, concatenates them horizontally,\n",
        "# trains classifiers on fused features, and evaluates on Dev set\n",
        "\n",
        "for task in TASKS:\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"TASK: {task.upper()} - EARLY FUSION\")\n",
        "    print(f\"{'='*80}\")\n",
        "    \n",
        "    # Select appropriate label list and dataset key for this task\n",
        "    if task == 'clarity':\n",
        "        label_list = CLARITY_LABELS\n",
        "        label_key = 'clarity_label'\n",
        "    else:  # evasion\n",
        "        label_list = EVASION_LABELS\n",
        "        label_key = 'evasion_label'\n",
        "    \n",
        "    # Load features from all models\n",
        "    print(\"Loading features from all models...\")\n",
        "    model_features = {}\n",
        "    model_feature_names = {}\n",
        "    \n",
        "    for model in MODELS:\n",
        "        X_train = storage.load_features(model, task, 'train')\n",
        "        X_dev = storage.load_features(model, task, 'dev')\n",
        "        \n",
        "        # Get feature names from metadata for proper labeling\n",
        "        meta = storage.load_metadata(model, task, 'train')\n",
        "        feature_names = meta['feature_names']\n",
        "        \n",
        "        model_features[model] = {\n",
        "            'train': X_train,\n",
        "            'dev': X_dev\n",
        "        }\n",
        "        model_feature_names[model] = feature_names\n",
        "        \n",
        "        print(f\"  {model}: {X_train.shape[1]} features\")\n",
        "    \n",
        "    # Fuse features by horizontal concatenation\n",
        "    # Fused feature vector = [BERT | RoBERTa | DeBERTa | XLNet]\n",
        "    print(\"\\nFusing features (concatenation)...\")\n",
        "    X_train_fused, fused_feature_names = fuse_attention_features(\n",
        "        {model: model_features[model]['train'] for model in MODELS},\n",
        "        model_feature_names\n",
        "    )\n",
        "    X_dev_fused, _ = fuse_attention_features(\n",
        "        {model: model_features[model]['dev'] for model in MODELS},\n",
        "        model_feature_names\n",
        "    )\n",
        "    \n",
        "    print(f\"  Fused features: {X_train_fused.shape[1]} features (sum of all models)\")\n",
        "    print(f\"  Train: {X_train_fused.shape[0]} samples\")\n",
        "    print(f\"  Dev: {X_dev_fused.shape[0]} samples\")\n",
        "    \n",
        "    # Save fused features to persistent storage\n",
        "    storage.save_fused_features(\n",
        "        X_train_fused, MODELS, task, 'train',\n",
        "        fused_feature_names, fusion_method='concat'\n",
        "    )\n",
        "    storage.save_fused_features(\n",
        "        X_dev_fused, MODELS, task, 'dev',\n",
        "        fused_feature_names, fusion_method='concat'\n",
        "    )\n",
        "    \n",
        "    # Extract labels from dataset splits\n",
        "    y_train = np.array([train_ds[i][label_key] for i in range(len(train_ds))])\n",
        "    y_dev = np.array([dev_ds[i][label_key] for i in range(len(dev_ds))])\n",
        "    \n",
        "    # Train all classifiers on fused features and evaluate on Dev set\n",
        "    print(\"\\nTraining classifiers on fused features...\")\n",
        "    results = train_and_evaluate(\n",
        "        X_train_fused, y_train, X_dev_fused, y_dev,\n",
        "        label_list=label_list,\n",
        "        task_name=f\"early_fusion_{task}\",\n",
        "        classifiers=classifiers,\n",
        "        random_state=42,\n",
        "        print_report=True,      # Print detailed classification report\n",
        "        print_table=True,       # Print results comparison table\n",
        "        create_plots=True,      # Generate confusion matrices and PR/ROC curves\n",
        "        save_plots_dir=str(DATA_PATH / 'plots' / 'early_fusion')\n",
        "    )\n",
        "    \n",
        "    # Save predictions and probabilities to persistent storage\n",
        "    for classifier_name, result in results.items():\n",
        "        # Save hard label predictions\n",
        "        storage.save_predictions(\n",
        "            result['dev_pred'],\n",
        "            'fused', classifier_name, task, 'dev'\n",
        "        )\n",
        "        \n",
        "        # Save probability distributions (if classifier supports it)\n",
        "        if result['dev_proba'] is not None:\n",
        "            storage.save_probabilities(\n",
        "                result['dev_proba'],\n",
        "                'fused', classifier_name, task, 'dev'\n",
        "            )\n",
        "    \n",
        "    # Save results summary to metadata for comparison with individual models\n",
        "    experiment_id = f\"early_fusion_{task}\"\n",
        "    storage.save_results({\n",
        "        'fusion_method': 'early_concat',\n",
        "        'models': MODELS,\n",
        "        'task': task,\n",
        "        'results': {\n",
        "            name: {\n",
        "                'metrics': res['metrics'],\n",
        "                'n_train': len(y_train),\n",
        "                'n_dev': len(y_dev)\n",
        "            }\n",
        "            for name, res in results.items()\n",
        "        }\n",
        "    }, experiment_id)\n",
        "\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(\"Early fusion complete for all tasks\")\n",
        "print(f\"{'='*80}\")\n",
        "print(\"\\nSummary:\")\n",
        "print(\"  - Features from all models concatenated\")\n",
        "print(\"  - Classifiers trained and evaluated on fused features\")\n",
        "print(\"  - Predictions and probabilities saved to Google Drive\")\n",
        "print(\"  - Results tables and plots generated\")\n",
        "print(\"  - Compare with individual model results from 03_train_evaluate.ipynb\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
