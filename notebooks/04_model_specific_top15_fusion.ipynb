{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model-Specific Top-15 Fusion: Greedy-Based Early Fusion\n",
    "\n",
    "================================================================================\n",
    "PURPOSE: Train models using greedy-selected top-15 features per model\n",
    "================================================================================\n",
    "\n",
    "**Methodology:**\n",
    "- For each model×task combination, use greedy-selected top-15 features\n",
    "- Select best 2 classifiers per model×task (based on dev set macro F1 from 3. notebook)\n",
    "- Train on Train+Dev combined data (final training)\n",
    "- **CRITICAL**: Only save probabilities (soft labels), NO hard predictions\n",
    "- Save trained models for use in 5. notebook Type 2 evaluation\n",
    "\n",
    "**Workflow:**\n",
    "1. Load best classifiers from 3. notebook results (`all_results_dev.pkl`)\n",
    "2. Load greedy-selected features from 3.5. notebook (`selected_features_all.json`)\n",
    "3. For each model×task:\n",
    "   - Extract top-15 greedy-selected features\n",
    "   - Select best 2 classifiers (by macro F1)\n",
    "   - Load Train+Dev features and labels\n",
    "   - Train on combined Train+Dev data\n",
    "   - Save trained models (only probabilities, no hard predictions)\n",
    "4. Save metadata (model paths, selected features, F1 scores)\n",
    "\n",
    "**Outputs:**\n",
    "- Trained models: `models/fusion/method2_{model}_{task}_{classifier}.pkl`\n",
    "- Metadata: `results/method2_trained_models.json` (Drive + GitHub)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SETUP: Repository Clone, Drive Mount, and Path Configuration\n",
    "# ============================================================================\n",
    "import shutil\n",
    "import os\n",
    "import subprocess\n",
    "import time\n",
    "import requests\n",
    "import zipfile\n",
    "import sys\n",
    "import json\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from google.colab import drive\n",
    "import numpy as np\n",
    "\n",
    "!rm -rf /content/semeval-context-tree-modular\n",
    "!git clone https://github.com/EonTechie/semeval-context-tree-modular.git\n",
    "!cd /content/semeval-context-tree-modular && git pull\n",
    "\n",
    "# Repository configuration\n",
    "repo_dir = '/content/semeval-context-tree-modular'\n",
    "repo_url = 'https://github.com/EonTechie/semeval-context-tree-modular.git'\n",
    "zip_url = 'https://github.com/EonTechie/semeval-context-tree-modular/archive/refs/heads/main.zip'\n",
    "\n",
    "# Clone repository (if not already present)\n",
    "if not os.path.exists(repo_dir):\n",
    "    print(\"Cloning repository from GitHub...\")\n",
    "    max_retries = 2\n",
    "    clone_success = False\n",
    "\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            result = subprocess.run(\n",
    "                ['git', 'clone', repo_url],\n",
    "                cwd='/content',\n",
    "                capture_output=True,\n",
    "                text=True,\n",
    "                timeout=60\n",
    "            )\n",
    "            if result.returncode == 0:\n",
    "                print(\"Repository cloned successfully via git\")\n",
    "                clone_success = True\n",
    "                break\n",
    "            else:\n",
    "                if attempt < max_retries - 1:\n",
    "                    time.sleep(3)\n",
    "        except Exception as e:\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(3)\n",
    "\n",
    "    # Fallback: Download as ZIP if git clone fails\n",
    "    if not clone_success:\n",
    "        print(\"Git clone failed. Downloading repository as ZIP archive...\")\n",
    "        zip_path = '/tmp/repo.zip'\n",
    "        try:\n",
    "            response = requests.get(zip_url, stream=True, timeout=60)\n",
    "            response.raise_for_status()\n",
    "            with open(zip_path, 'wb') as f:\n",
    "                for chunk in response.iter_content(chunk_size=8192):\n",
    "                    f.write(chunk)\n",
    "            with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "                zip_ref.extractall('/content')\n",
    "            extracted_dir = '/content/semeval-context-tree-modular-main'\n",
    "            if os.path.exists(extracted_dir):\n",
    "                os.rename(extracted_dir, repo_dir)\n",
    "            os.remove(zip_path)\n",
    "            print(\"Repository downloaded and extracted successfully\")\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to obtain repository: {e}\")\n",
    "\n",
    "# Mount Google Drive (if not already mounted)\n",
    "try:\n",
    "    drive.mount('/content/drive', force_remount=False)\n",
    "except Exception:\n",
    "    pass  # Already mounted\n",
    "\n",
    "# Configure paths\n",
    "BASE_PATH = Path('/content/semeval-context-tree-modular')\n",
    "DATA_PATH = Path('/content/drive/MyDrive/semeval_data')\n",
    "\n",
    "# Add repository to Python path\n",
    "if str(BASE_PATH) not in sys.path:\n",
    "    sys.path.insert(0, str(BASE_PATH))\n",
    "\n",
    "# Verify imports work\n",
    "try:\n",
    "    from src.storage.manager import StorageManager\n",
    "    from src.models.classifiers import get_classifier_dict, train_classifiers\n",
    "    from src.features.extraction import get_feature_names\n",
    "except ImportError as e:\n",
    "    raise ImportError(\n",
    "        f\"Failed to import required modules. \"\n",
    "        f\"Repository path: {BASE_PATH}, \"\n",
    "        f\"Python path: {sys.path[:3]}, \"\n",
    "        f\"Error: {e}\"\n",
    "    )\n",
    "\n",
    "# Initialize StorageManager\n",
    "storage = StorageManager(\n",
    "    base_path=str(BASE_PATH),\n",
    "    data_path=str(DATA_PATH),\n",
    "    github_path=str(BASE_PATH)\n",
    ")\n",
    "\n",
    "print(\"Setup complete\")\n",
    "print(f\"  Repository: {BASE_PATH}\")\n",
    "print(f\"  Data storage: {DATA_PATH}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ============================================================================\n",
    "# CONFIGURE MODELS, TASKS, AND CLASSIFIERS\n",
    "# ============================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models to process\n",
    "MODELS = ['bert', 'bert_political', 'bert_ambiguity', 'roberta', 'deberta', 'xlnet']\n",
    "\n",
    "# Tasks to process\n",
    "TASKS = ['clarity', 'evasion']\n",
    "\n",
    "# Label lists\n",
    "CLARITY_LABELS = ['Ambivalent', 'Clear Non-Reply', 'Clear Reply']\n",
    "EVASION_LABELS = ['Direct Answer', 'Indirect Answer', 'No Answer']\n",
    "\n",
    "# Random seed\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# Number of top classifiers to use per model×task\n",
    "TOP_K_CLASSIFIERS = 2\n",
    "\n",
    "# Number of greedy-selected features to use\n",
    "TOP_N_FEATURES = 15\n",
    "\n",
    "print(f\"Models: {MODELS}\")\n",
    "print(f\"Tasks: {TASKS}\")\n",
    "print(f\"Top-K Classifiers: {TOP_K_CLASSIFIERS}\")\n",
    "print(f\"Top-N Features: {TOP_N_FEATURES}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ============================================================================\n",
    "# STEP 1: LOAD BEST CLASSIFIERS FROM 3. NOTEBOOK RESULTS\n",
    "# ============================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all results from 3. notebook\n",
    "# Expected path: data_path/results/all_results_dev.pkl\n",
    "expected_path = storage.data_path / 'results/all_results_dev.pkl'\n",
    "print(f\"Looking for all_results_dev.pkl at: {expected_path}\")\n",
    "print(f\"File exists: {expected_path.exists()}\")\n",
    "\n",
    "if expected_path.exists():\n",
    "    file_size = expected_path.stat().st_size / (1024 * 1024)  # Size in MB\n",
    "    print(f\"File size: {file_size:.2f} MB\")\n",
    "\n",
    "all_results = storage.load_all_results_dict(filename='all_results_dev.pkl')\n",
    "\n",
    "if not all_results:\n",
    "    raise FileNotFoundError(\n",
    "        f\"all_results_dev.pkl not found at: {expected_path}\\n\"\n",
    "        f\"Make sure you have run 03_train_evaluate.ipynb first.\\n\"\n",
    "        f\"The file should be saved to: {storage.data_path / 'results'}\"\n",
    "    )\n",
    "\n",
    "# Verify loaded data structure\n",
    "print(f\"\\n✓ Loaded all_results_dev.pkl successfully\")\n",
    "print(f\"  Number of models: {len(all_results)}\")\n",
    "if all_results:\n",
    "    first_model = list(all_results.keys())[0]\n",
    "    print(f\"  First model: {first_model}\")\n",
    "    if first_model in all_results:\n",
    "        print(f\"  Tasks for {first_model}: {list(all_results[first_model].keys())}\")\n",
    "        if all_results[first_model]:\n",
    "            first_task = list(all_results[first_model].keys())[0]\n",
    "            print(f\"  Classifiers for {first_model}-{first_task}: {list(all_results[first_model][first_task].keys())}\")\n",
    "\n",
    "# Find best classifiers for each model×task combination\n",
    "best_classifiers = {}\n",
    "\n",
    "for model in MODELS:\n",
    "    if model not in all_results:\n",
    "        print(f\"⚠ Warning: Model '{model}' not found in results\")\n",
    "        continue\n",
    "    \n",
    "    best_classifiers[model] = {}\n",
    "    \n",
    "    for task in TASKS:\n",
    "        if task not in all_results[model]:\n",
    "            print(f\"⚠ Warning: Task '{task}' not found for model '{model}'\")\n",
    "            continue\n",
    "        \n",
    "        results = all_results[model][task]\n",
    "        \n",
    "        # Find best classifiers by macro F1\n",
    "        classifier_scores = []\n",
    "        for clf_name, result in results.items():\n",
    "            metrics = result.get('metrics', {})\n",
    "            macro_f1 = metrics.get('macro_f1', 0.0)\n",
    "            classifier_scores.append((clf_name, macro_f1))\n",
    "        \n",
    "        # Sort by macro F1 (descending)\n",
    "        classifier_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Get top-K classifiers\n",
    "        top_classifiers = classifier_scores[:TOP_K_CLASSIFIERS]\n",
    "        \n",
    "        best_classifiers[model][task] = {\n",
    "            'best_clf': top_classifiers[0][0] if len(top_classifiers) > 0 else None,\n",
    "            'best_f1': top_classifiers[0][1] if len(top_classifiers) > 0 else 0.0,\n",
    "            '2nd_best_clf': top_classifiers[1][0] if len(top_classifiers) > 1 else None,\n",
    "            '2nd_best_f1': top_classifiers[1][1] if len(top_classifiers) > 1 else 0.0,\n",
    "            'all_scores': classifier_scores\n",
    "        }\n",
    "        \n",
    "        print(f\"{model} - {task}:\")\n",
    "        print(f\"  Best: {top_classifiers[0][0]} (F1={top_classifiers[0][1]:.4f})\" if len(top_classifiers) > 0 else \"  No classifiers found\")\n",
    "        print(f\"  2nd:  {top_classifiers[1][0]} (F1={top_classifiers[1][1]:.4f})\" if len(top_classifiers) > 1 else \"  No 2nd classifier\")\n",
    "\n",
    "print(f\"\\n✓ Loaded best classifiers for {len(best_classifiers)} models\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ============================================================================\n",
    "# STEP 2: LOAD GREEDY-SELECTED FEATURES FROM 3.5. NOTEBOOK\n",
    "# ============================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load greedy-selected features\n",
    "ablation_dir = storage.data_path / 'results/ablation'\n",
    "greedy_features_path = ablation_dir / 'selected_features_all.json'\n",
    "\n",
    "if not greedy_features_path.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"Greedy selected features not found: {greedy_features_path}\\n\"\n",
    "        f\"Make sure you have run 03_5_ablation_study.ipynb (Greedy Forward Selection)\"\n",
    "    )\n",
    "\n",
    "with open(greedy_features_path, 'r') as f:\n",
    "    selected_features_dict = json.load(f)\n",
    "\n",
    "# Convert to model_name -> task -> features format\n",
    "greedy_features = {}\n",
    "for model in MODELS:\n",
    "    greedy_features[model] = {}\n",
    "    for task in TASKS:\n",
    "        key = f\"{model}_{task}\"\n",
    "        if key in selected_features_dict:\n",
    "            selected_features = selected_features_dict[key].get('selected_features', [])\n",
    "            # Take top-N features\n",
    "            greedy_features[model][task] = selected_features[:TOP_N_FEATURES]\n",
    "            print(f\"{model} - {task}: {len(greedy_features[model][task])} features\")\n",
    "        else:\n",
    "            print(f\"⚠ Warning: No greedy features found for {key}\")\n",
    "            greedy_features[model][task] = []\n",
    "\n",
    "print(f\"\\n✓ Loaded greedy-selected features\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ============================================================================\n",
    "# STEP 3: GET FEATURE NAME TO INDEX MAPPING\n",
    "# ============================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all feature names (19 features total)\n",
    "all_feature_names = get_feature_names()\n",
    "feature_name_to_idx = {name: idx for idx, name in enumerate(all_feature_names)}\n",
    "\n",
    "print(f\"Total features: {len(all_feature_names)}\")\n",
    "print(f\"Feature names: {all_feature_names}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ============================================================================\n",
    "# STEP 4: TRAIN MODELS WITH GREEDY-SELECTED FEATURES\n",
    "# ============================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CRITICAL: Create output directories upfront (CHECKPOINT)\n",
    "models_fusion_dir = storage.data_path / 'models/fusion'\n",
    "results_dir = storage.data_path / 'results'\n",
    "github_results_dir = storage.github_path / 'results'\n",
    "\n",
    "models_fusion_dir.mkdir(parents=True, exist_ok=True)\n",
    "results_dir.mkdir(parents=True, exist_ok=True)\n",
    "github_results_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Created output directories:\")\n",
    "print(f\"  Drive models: {models_fusion_dir}\")\n",
    "print(f\"  Drive results: {results_dir}\")\n",
    "print(f\"  GitHub results: {github_results_dir}\")\n",
    "\n",
    "# Get classifier dictionary\n",
    "classifiers = get_classifier_dict(random_state=RANDOM_STATE)\n",
    "\n",
    "# Store results metadata\n",
    "results_method2 = {}\n",
    "\n",
    "# Process each model×task combination\n",
    "for model in MODELS:\n",
    "    if model not in best_classifiers:\n",
    "        continue\n",
    "    \n",
    "    results_method2[model] = {}\n",
    "    \n",
    "    for task in TASKS:\n",
    "        if task not in best_classifiers[model]:\n",
    "            continue\n",
    "        \n",
    "        # Get best classifiers for this model×task\n",
    "        best_info = best_classifiers[model][task]\n",
    "        best_clf_name = best_info['best_clf']\n",
    "        second_clf_name = best_info['2nd_best_clf']\n",
    "        \n",
    "        if best_clf_name is None:\n",
    "            print(f\"⚠ Skipping {model} - {task}: No best classifier found\")\n",
    "            continue\n",
    "        \n",
    "        # Get greedy-selected features\n",
    "        selected_feature_names = greedy_features[model][task]\n",
    "        \n",
    "        if len(selected_feature_names) == 0:\n",
    "            print(f\"⚠ Skipping {model} - {task}: No greedy features found\")\n",
    "            continue\n",
    "        \n",
    "        # Get feature indices\n",
    "        selected_indices = [feature_name_to_idx[name] for name in selected_feature_names if name in feature_name_to_idx]\n",
    "        \n",
    "        if len(selected_indices) == 0:\n",
    "            print(f\"⚠ Skipping {model} - {task}: No valid feature indices\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"MODEL: {model.upper()} | TASK: {task.upper()}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"Selected features ({len(selected_indices)}): {selected_feature_names}\")\n",
    "        print(f\"Best classifiers: {best_clf_name}, {second_clf_name if second_clf_name else 'N/A'}\")\n",
    "        \n",
    "        # Select appropriate label list and dataset key for this task\n",
    "        if task == 'clarity':\n",
    "            label_list = CLARITY_LABELS\n",
    "            label_key = 'clarity_label'\n",
    "        else:  # evasion\n",
    "            label_list = EVASION_LABELS\n",
    "            label_key = 'evasion_label'\n",
    "        \n",
    "        # Load splits\n",
    "        train_ds = storage.load_split('train', task=task)\n",
    "        dev_ds = storage.load_split('dev', task=task)\n",
    "        \n",
    "        # Load features\n",
    "        X_train_full = storage.load_features(model, task, 'train')\n",
    "        X_dev_full = storage.load_features(model, task, 'dev')\n",
    "        \n",
    "        # Extract selected features only\n",
    "        X_train = X_train_full[:, selected_indices]\n",
    "        X_dev = X_dev_full[:, selected_indices]\n",
    "        \n",
    "        # Combine train+dev for final training\n",
    "        X_train_combined = np.vstack([X_train, X_dev])\n",
    "        \n",
    "        # Extract labels\n",
    "        y_train = np.array([train_ds[i][label_key] for i in range(len(train_ds))])\n",
    "        y_dev = np.array([dev_ds[i][label_key] for i in range(len(dev_ds))])\n",
    "        y_train_combined = np.concatenate([y_train, y_dev])\n",
    "        \n",
    "        print(f\"Training: {X_train_combined.shape[0]} samples (train+dev combined)\")\n",
    "        print(f\"Features: {X_train_combined.shape[1]} (selected from {X_train_full.shape[1]})\")\n",
    "        \n",
    "        # Train best 2 classifiers\n",
    "        classifiers_to_train = [best_clf_name]\n",
    "        if second_clf_name:\n",
    "            classifiers_to_train.append(second_clf_name)\n",
    "        \n",
    "        results_method2[model][task] = {}\n",
    "        \n",
    "        for clf_name in classifiers_to_train:\n",
    "            print(f\"\\nTraining {clf_name}...\")\n",
    "            \n",
    "            # Get classifier instance\n",
    "            clf = classifiers[clf_name]\n",
    "            \n",
    "            # Train using train_classifiers function (handles label encoding)\n",
    "            # We pass train_combined as train, and empty dev (we don't need dev predictions)\n",
    "            # But we need to pass something for dev, so we use train_combined again\n",
    "            training_results = train_classifiers(\n",
    "                X_train_combined, y_train_combined,\n",
    "                X_train_combined, y_train_combined,  # Dummy dev (we don't use predictions)\n",
    "                classifiers={clf_name: clf},\n",
    "                random_state=RANDOM_STATE\n",
    "            )\n",
    "            \n",
    "            trained_model = training_results[clf_name]['model']\n",
    "            \n",
    "            # Save trained model\n",
    "            model_filename = f\"method2_{model}_{task}_{clf_name}.pkl\"\n",
    "            model_save_path = models_fusion_dir / model_filename\n",
    "            \n",
    "            with open(model_save_path, 'wb') as f:\n",
    "                pickle.dump({\n",
    "                    'model': trained_model,\n",
    "                    'selected_indices': selected_indices,\n",
    "                    'selected_feature_names': selected_feature_names,\n",
    "                    'label_list': label_list,\n",
    "                    'label_encoder': training_results[clf_name]['label_encoder']\n",
    "                }, f)\n",
    "            \n",
    "            print(f\"  ✓ Saved: {model_filename}\")\n",
    "            \n",
    "            # Store metadata\n",
    "            results_method2[model][task][clf_name] = {\n",
    "                'selected_features': selected_feature_names,\n",
    "                'selected_indices': selected_indices,\n",
    "                'model_path': str(model_save_path),\n",
    "                'macro_f1_dev': best_info['best_f1'] if clf_name == best_clf_name else best_info['2nd_best_f1']\n",
    "            }\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"✓ Training complete for all model×task combinations\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 5: SAVE METADATA (DRIVE + GITHUB)\n",
    "# ============================================================================\n",
    "\n",
    "# Helper function to make JSON serializable\n",
    "def make_json_serializable(obj):\n",
    "    \"\"\"Recursively convert numpy arrays and types to JSON-serializable Python types\"\"\"\n",
    "    import numpy as np\n",
    "    if isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    elif isinstance(obj, (np.integer, np.int64, np.int32)):\n",
    "        return int(obj)\n",
    "    elif isinstance(obj, (np.floating, np.float64, np.float32)):\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, (bool, np.bool_)):\n",
    "        return bool(obj)\n",
    "    elif isinstance(obj, dict):\n",
    "        return {key: make_json_serializable(value) for key, value in obj.items()}\n",
    "    elif isinstance(obj, (list, tuple)):\n",
    "        return [make_json_serializable(item) for item in obj]\n",
    "    elif hasattr(obj, 'item'):  # numpy scalar\n",
    "        return obj.item()\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "# Convert to JSON-serializable format\n",
    "results_serializable = make_json_serializable(results_method2)\n",
    "\n",
    "# Save to Drive\n",
    "results_path_drive = results_dir / 'method2_trained_models.json'\n",
    "with open(results_path_drive, 'w') as f:\n",
    "    json.dump(results_serializable, f, indent=2)\n",
    "\n",
    "print(f\"✓ Saved metadata to Drive: {results_path_drive}\")\n",
    "\n",
    "# Save to GitHub (copy same content)\n",
    "results_path_github = github_results_dir / 'method2_trained_models.json'\n",
    "with open(results_path_github, 'w') as f:\n",
    "    json.dump(results_serializable, f, indent=2)\n",
    "\n",
    "print(f\"✓ Saved metadata to GitHub: {results_path_github}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"✓ METHOD 2 TRAINING COMPLETE\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Trained models saved to: {models_fusion_dir}\")\n",
    "print(f\"Metadata saved to: {results_path_drive} (Drive) and {results_path_github} (GitHub)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
