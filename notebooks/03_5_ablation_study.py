# -*- coding: utf-8 -*-
"""05_final_evaluation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/github/EonTechie/semeval-context-tree-modular/blob/main/notebooks/05_final_evaluation.ipynb
"""

# ============================================================================
# SETUP: Repository Clone, Drive Mount, and Path Configuration
# ============================================================================
# This cell performs minimal setup required for the notebook to run:
# 1. Clones repository from GitHub (if not already present)
# 2. Mounts Google Drive for persistent data storage
# 3. Configures Python paths and initializes StorageManager
# 4. Loads test split (ONLY accessed in this notebook)

import shutil
import os
import subprocess
import time
import requests
import zipfile
import sys
from pathlib import Path
from google.colab import drive
import numpy as np
import torch
from transformers import AutoTokenizer, AutoModel
!rm -rf /content/semeval-context-tree-modular
!git clone https://github.com/EonTechie/semeval-context-tree-modular.git
!cd /content/semeval-context-tree-modular && git pull
# Repository configuration
repo_dir = '/content/semeval-context-tree-modular'
repo_url = 'https://github.com/EonTechie/semeval-context-tree-modular.git'
zip_url = 'https://github.com/EonTechie/semeval-context-tree-modular/archive/refs/heads/main.zip'

# Clone repository (if not already present)
if not os.path.exists(repo_dir):
    print("Cloning repository from GitHub...")
    max_retries = 2
    clone_success = False

    for attempt in range(max_retries):
        try:
            result = subprocess.run(
                ['git', 'clone', repo_url],
                cwd='/content',
                capture_output=True,
                text=True,
                timeout=60
            )
            if result.returncode == 0:
                print("Repository cloned successfully via git")
                clone_success = True
                break
            else:
                if attempt < max_retries - 1:
                    time.sleep(3)
        except Exception as e:
            if attempt < max_retries - 1:
                time.sleep(3)

    # Fallback: Download as ZIP if git clone fails
    if not clone_success:
        print("Git clone failed. Downloading repository as ZIP archive...")
        zip_path = '/tmp/repo.zip'
        try:
            response = requests.get(zip_url, stream=True, timeout=60)
            response.raise_for_status()
            with open(zip_path, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    f.write(chunk)
            with zipfile.ZipFile(zip_path, 'r') as zip_ref:
                zip_ref.extractall('/content')
            extracted_dir = '/content/semeval-context-tree-modular-main'
            if os.path.exists(extracted_dir):
                os.rename(extracted_dir, repo_dir)
            os.remove(zip_path)
            print("Repository downloaded and extracted successfully")
        except Exception as e:
            raise RuntimeError(f"Failed to obtain repository: {e}")

# Mount Google Drive (if not already mounted)
try:
    drive.mount('/content/drive', force_remount=False)
except Exception:
    pass  # Already mounted

# Configure paths
BASE_PATH = Path('/content/semeval-context-tree-modular')
DATA_PATH = Path('/content/drive/MyDrive/semeval_data')

# Verify repository structure exists
if not BASE_PATH.exists():
    raise RuntimeError(f"Repository directory not found: {BASE_PATH}")
if not (BASE_PATH / 'src').exists():
    raise RuntimeError(f"src directory not found in repository: {BASE_PATH / 'src'}")
if not (BASE_PATH / 'src' / 'storage' / 'manager.py').exists():
    raise RuntimeError(f"Required file not found: {BASE_PATH / 'src' / 'storage' / 'manager.py'}")

# Add repository to Python path
sys.path.insert(0, str(BASE_PATH))

# Verify imports work
try:
    from src.storage.manager import StorageManager
    from src.features.extraction import featurize_hf_dataset_in_batches_v2
    from src.models.classifiers import get_classifier_dict
    from src.evaluation.metrics import compute_all_metrics, print_classification_report
    from src.evaluation.tables import print_results_table
    from src.evaluation.visualizer import visualize_all_evaluation
except ImportError as e:
    raise ImportError(
        f"Failed to import required modules. "
        f"Repository path: {BASE_PATH}, "
        f"Python path: {sys.path[:3]}, "
        f"Error: {e}"
    )

# Initialize StorageManager
storage = StorageManager(
    base_path=str(BASE_PATH),
    data_path=str(DATA_PATH),
    github_path=str(BASE_PATH)
)

# Test splits will be loaded per-task in the evaluation loop
# Clarity and Evasion have different test splits (Evasion uses majority voting)

print("Setup complete")
print(f"  Repository: {BASE_PATH}")
print(f"  Data storage: {DATA_PATH}")
print(f"\nCRITICAL: Test sets will be loaded per-task (task-specific splits)")
print("         Clarity and Evasion have different test splits due to majority voting")
print("         These sets have NEVER been used for training or development!")

# KOD HÜCRESİ 1
# ==============
# ============================================================================
# SETUP: Repository Clone, Drive Mount, and Path Configuration
# ============================================================================
import shutil
import os
import subprocess
import time
import requests
import zipfile
import sys
from pathlib import Path
from google.colab import drive
import numpy as np
import pandas as pd

# Repository configuration
repo_dir = '/content/semeval-context-tree-modular'
repo_url = 'https://github.com/EonTechie/semeval-context-tree-modular.git'
zip_url = 'https://github.com/EonTechie/semeval-context-tree-modular/archive/refs/heads/main.zip'

# Clone repository (if not already present)
if not os.path.exists(repo_dir):
    print("Cloning repository from GitHub...")
    max_retries = 2
    clone_success = False

    for attempt in range(max_retries):
        try:
            result = subprocess.run(
                ['git', 'clone', repo_url],
                cwd='/content',
                capture_output=True,
                text=True,
                timeout=60
            )
            if result.returncode == 0:
                print("Repository cloned successfully via git")
                clone_success = True
                break
            else:
                if attempt < max_retries - 1:
                    time.sleep(3)
        except Exception as e:
            if attempt < max_retries - 1:
                time.sleep(3)

    # Fallback: Download as ZIP if git clone fails
    if not clone_success:
        print("Git clone failed. Downloading repository as ZIP archive...")
        zip_path = '/tmp/repo.zip'
        try:
            response = requests.get(zip_url, stream=True, timeout=60)
            response.raise_for_status()
            with open(zip_path, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    f.write(chunk)
            with zipfile.ZipFile(zip_path, 'r') as zip_ref:
                zip_ref.extractall('/content')
            extracted_dir = '/content/semeval-context-tree-modular-main'
            if os.path.exists(extracted_dir):
                os.rename(extracted_dir, repo_dir)
            os.remove(zip_path)
            print("Repository downloaded and extracted successfully")
        except Exception as e:
            raise RuntimeError(f"Failed to obtain repository: {e}")

# Mount Google Drive (if not already mounted)
try:
    drive.mount('/content/drive', force_remount=False)
except Exception:
    pass  # Already mounted

# Configure paths
BASE_PATH = Path('/content/semeval-context-tree-modular')
DATA_PATH = Path('/content/drive/MyDrive/semeval_data')

# Verify repository structure exists
if not BASE_PATH.exists():
    raise RuntimeError(f"Repository directory not found: {BASE_PATH}")
if not (BASE_PATH / 'src').exists():
    raise RuntimeError(f"src directory not found in repository: {BASE_PATH / 'src'}")
if not (BASE_PATH / 'src' / 'storage' / 'manager.py').exists():
    raise RuntimeError(f"Required file not found: {BASE_PATH / 'src' / 'storage' / 'manager.py'}")

# Add repository to Python path
sys.path.insert(0, str(BASE_PATH))

# Verify imports work
try:
    from src.storage.manager import StorageManager
    from src.models.classifiers import get_classifier_dict
    from src.features.extraction import get_feature_names
    from sklearn.metrics import f1_score
    from sklearn.pipeline import Pipeline
    from sklearn.preprocessing import StandardScaler, LabelEncoder
    from sklearn.base import clone
except ImportError as e:
    raise ImportError(
        f"Failed to import required modules. "
        f"Repository path: {BASE_PATH}, "
        f"Python path: {sys.path[:3]}, "
        f"Error: {e}"
    )

# Initialize StorageManager
storage = StorageManager(
    base_path=str(BASE_PATH),
    data_path=str(DATA_PATH),
    github_path=str(BASE_PATH)
)

# Create ablation results directory
ablation_dir = DATA_PATH / 'results' / 'FinalResultsType2' / 'ablation'
ablation_dir.mkdir(parents=True, exist_ok=True)

print("Setup complete")
print(f"  Repository: {BASE_PATH}")
print(f"  Data storage: {DATA_PATH}")
print(f"  Ablation results: {ablation_dir}")

# ============================================================================
# KOD HÜCRESİ 2
# ==============
# ============================================================================
# REPRODUCIBILITY SETUP: Set Random Seeds for All Libraries
# ============================================================================
from src.utils.reproducibility import set_all_seeds

# Set all random seeds to 42 for full reproducibility
# deterministic=True ensures PyTorch operations are deterministic (slower but fully reproducible)
set_all_seeds(seed=42, deterministic=True)

print("✓ Reproducibility configured: All random seeds set to 42")
print("✓ PyTorch deterministic mode enabled")
print("\nNOTE: If you encounter performance issues or non-deterministic behavior,")
print("      you can set deterministic=False in set_all_seeds() call above.")

# ============================================================================
# KOD HÜCRESİ 3
# ==============
# ============================================================================
# CONFIGURE MODELS, TASKS, AND CLASSIFIERS
# ============================================================================
# Check if get_classifier_dict is imported (from Cell 1 - Setup)
if 'get_classifier_dict' not in globals():
    raise NameError(
        "get_classifier_dict not found. Please run Cell 1 (Setup) first.\n"
        "Cell 1 imports get_classifier_dict from src.models.classifiers."
    )

MODELS = ['bert', 'bert_political', 'bert_ambiguity', 'roberta', 'deberta', 'xlnet']
# NOTE: Only clarity and hierarchical_evasion_to_clarity for greedy selection
# 'evasion' task is NOT included (it's only used for training in 3. notebook)
# Best classifier selection happens in 4. notebook, not here
TASKS = ['clarity', 'hierarchical_evasion_to_clarity']  # 2 tasks for greedy selection

# Label mappings for each task
CLARITY_LABELS = ['Ambivalent', 'Clear Non-Reply', 'Clear Reply']
EVASION_LABELS = ['Claims ignorance', 'Clarification', 'Declining to answer',
                  'Deflection', 'Dodging', 'Explicit',
                  'General', 'Implicit', 'Partial/half-answer']

# Initialize classifiers with fixed random seed for reproducibility
# Includes MLP (Multi-Layer Perceptron) as requested
classifiers = get_classifier_dict(random_state=42)

print("="*80)
print("CONFIGURATION")
print("="*80)
print(f"  Models: {len(MODELS)} models")
print(f"    {MODELS}")
print(f"  Tasks: {len(TASKS)} tasks")
print(f"    {TASKS}")
print(f"  Classifiers: {len(classifiers)} classifiers")
print(f"    {list(classifiers.keys())}")
print(f"  Total combinations per task: {len(MODELS)} × {len(classifiers)} = {len(MODELS) * len(classifiers)}")
print(f"  Evaluation set: Dev set (not test)")
print("="*80)

# ============================================================================
# KOD HÜCRESİ 4
# ==============
# ============================================================================
# SINGLE-FEATURE ABLATION STUDY
# ============================================================================
def eval_single_feature(X_train, X_dev, y_train, y_dev, feature_idx, clf):
    """
    Evaluate a single feature using a classifier.

    This function trains a classifier using only one feature and evaluates its
    performance on the dev set. StandardScaler is applied to normalize the
    single feature before classification.

    Args:
        X_train: Training feature matrix (N, F) where F is total number of features
        X_dev: Dev feature matrix (M, F)
        y_train: Training labels (N,)
        y_dev: Dev labels (M,)
        feature_idx: Index of the feature to evaluate (0 to F-1)
        clf: Classifier instance (will be cloned to avoid state issues)

    Returns:
        Macro F1 score on dev set (float)
    """
    # Encode labels to numeric (required for MLP, XGBoost, LightGBM)
    # This matches the approach in siparismaili01 notebook
    label_encoder = LabelEncoder()
    y_train_encoded = label_encoder.fit_transform(y_train)
    y_dev_encoded = label_encoder.transform(y_dev)

    # Select only the specified feature (single column)
    X_train_f = X_train[:, [feature_idx]]
    X_dev_f = X_dev[:, [feature_idx]]

    # Pipeline with scaling (critical for single features to work properly)
    # StandardScaler normalizes the feature to have zero mean and unit variance
    pipe = Pipeline([
        ("scaler", StandardScaler()),
        ("clf", clone(clf))  # Clone to avoid modifying the original classifier
    ])

    # Train on single feature and evaluate on dev set
    pipe.fit(X_train_f, y_train_encoded)
    pred = pipe.predict(X_dev_f)
    macro_f1 = f1_score(y_dev_encoded, pred, average='macro')

    return macro_f1

# Check if required variables are defined (from Cell 2 - Configuration)
if 'TASKS' not in globals() or 'MODELS' not in globals() or 'classifiers' not in globals():
    raise NameError(
        "Required variables not defined. Please run Cell 2 (Configuration) first.\n"
        "Cell 2 defines: TASKS, MODELS, CLARITY_LABELS, EVASION_LABELS, and classifiers."
    )

# Check if storage is defined (from Cell 1 - Setup)
if 'storage' not in globals():
    raise NameError(
        "storage not found. Please run Cell 1 (Setup) first.\n"
        "Cell 1 initializes StorageManager as 'storage'."
    )

print("="*80)
print("SINGLE-FEATURE ABLATION STUDY")
print("="*80)
print("Evaluating each feature individually across all model×task×classifier combinations")
print(f"Total evaluations: {len(TASKS)} tasks × {len(MODELS)} models × {len(classifiers)} classifiers × 19 features")
print("This may take 15-30 minutes depending on your hardware...\n")

# Store all ablation results
# Each entry contains: model, task, classifier, feature, feature_idx, macro_f1
ablation_results = []

for task in TASKS:
    print(f"\n{'='*80}")
    print(f"TASK: {task.upper()}")
    print(f"{'='*80}")

    # Select appropriate label list and dataset key based on task
    if task == 'clarity':
        label_list = CLARITY_LABELS
        label_key = 'clarity_label'
        task_for_split = 'clarity'
    elif task == 'evasion':
        label_list = EVASION_LABELS
        label_key = 'evasion_label'
        task_for_split = 'evasion'
    else:  # hierarchical_evasion_to_clarity
        # For hierarchical task, we need to load evasion dev set to get clarity labels
        # (hierarchical uses evasion predictions mapped to clarity labels)
        label_list = CLARITY_LABELS
        label_key = 'clarity_label'
        # We'll load from evasion dev set (same filtered samples)
        task_for_split = 'evasion'

    # Load task-specific splits
    # For hierarchical task, we load evasion split (which has clarity labels)
    train_ds = storage.load_split('train', task=task_for_split)
    dev_ds = storage.load_split('dev', task=task_for_split)

    # Extract labels
    y_train = np.array([train_ds[i][label_key] for i in range(len(train_ds))])
    y_dev = np.array([dev_ds[i][label_key] for i in range(len(dev_ds))])

    print(f"  Train: {len(y_train)} samples")
    print(f"  Dev: {len(y_dev)} samples")

    # Get feature names directly from extraction module (same for all models)
    # This avoids dependency on metadata files in GitHub
    # Feature names are the same across all models (19 Context Tree features)
    feature_names = get_feature_names()
    n_features = len(feature_names)

    print(f"  Features: {n_features} features")
    print(f"  Feature names: {feature_names}\n")

    # For hierarchical task, we need to use evasion features
    # (hierarchical approach uses evasion predictions, so we evaluate on evasion features)
    feature_task = 'evasion' if task == 'hierarchical_evasion_to_clarity' else task

    # For each model
    for model in MODELS:
        print(f"  Model: {model}")

        # Load features
        # For hierarchical task, use evasion features (since we're evaluating
        # how well evasion features predict clarity via hierarchical mapping)
        try:
            X_train = storage.load_features(model, feature_task, 'train')
            X_dev = storage.load_features(model, feature_task, 'dev')
        except FileNotFoundError:
            print(f"    ⚠ Features not found for {model} × {feature_task}, skipping...")
            continue

        # Verify feature count matches
        if X_train.shape[1] != n_features:
            print(f"    ⚠ Feature count mismatch: expected {n_features}, got {X_train.shape[1]}, skipping...")
            continue

        # For each classifier
        for clf_name, clf in classifiers.items():
            print(f"    Classifier: {clf_name}")

            # Evaluate each feature individually
            for feature_idx, feature_name in enumerate(feature_names):
                try:
                    macro_f1 = eval_single_feature(
                        X_train, X_dev,
                        y_train, y_dev,
                        feature_idx, clf
                    )

                    ablation_results.append({
                        'model': model,
                        'task': task,
                        'classifier': clf_name,
                        'feature': feature_name,
                        'feature_idx': feature_idx,
                        'macro_f1': float(macro_f1)
                    })
                except Exception as e:
                    print(f"      ⚠ Error evaluating feature {feature_name}: {e}")
                    continue

print(f"\n{'='*80}")
print("SINGLE-FEATURE ABLATION COMPLETE")
print(f"{'='*80}")
print(f"Total evaluations completed: {len(ablation_results)}")
print(f"Expected: {len(TASKS)} tasks × {len(MODELS)} models × {len(classifiers)} classifiers × {n_features} features = {len(TASKS) * len(MODELS) * len(classifiers) * n_features}")

# ============

# ============================================================================
# KOD HÜCRESİ 5
# ==============
# ============================================================================
# FEATURE RANKING AND STATISTICAL ANALYSIS
# ============================================================================
# Check if ablation_results exists (from Cell 4 - Single-Feature Ablation)
if 'ablation_results' not in globals():
    raise NameError(
        "ablation_results not found. Please run Cell 4 (Single-Feature Ablation) first.\n"
        "Cell 4 performs the ablation study and creates ablation_results list."
    )

# Check if storage and ablation_dir are defined (from Cell 1 - Setup)
if 'storage' not in globals():
    raise NameError(
        "storage not found. Please run Cell 1 (Setup) first.\n"
        "Cell 1 initializes StorageManager as 'storage'."
    )
if 'ablation_dir' not in globals():
    raise NameError(
        "ablation_dir not found. Please run Cell 1 (Setup) first.\n"
        "Cell 1 creates ablation_dir directory."
    )

df_ablation = pd.DataFrame(ablation_results)

if len(df_ablation) == 0:
    print("⚠ No ablation results found. Make sure Cell 4 completed successfully.")
    print("  You need to run Cell 4 (Single-Feature Ablation) first.")
else:
    print("="*80)
    print("FEATURE RANKING AND STATISTICAL ANALYSIS")
    print("="*80)
    print(f"Total ablation results: {len(df_ablation)} evaluations")
    print(f"Expected per task: {len(MODELS)} models × {len(classifiers)} classifiers × 19 features = {len(MODELS) * len(classifiers) * 19}")

    # Save raw ablation results for each task
    print(f"\n{'='*80}")
    print("SAVING RAW ABLATION RESULTS")
    print(f"{'='*80}")

    for task in TASKS:
        df_task = df_ablation[df_ablation['task'] == task]
        if len(df_task) > 0:
            # Ensure directory exists before saving
            ablation_dir.mkdir(parents=True, exist_ok=True)
            csv_path = ablation_dir / f'single_feature_{task}.csv'
            df_task.to_csv(csv_path, index=False)
            print(f"  Saved {task}: {len(df_task)} evaluations → {csv_path}")

    # ========================================================================
    # STATISTICAL AGGREGATION AND WEIGHTED SCORE CALCULATION
    # ========================================================================
    # Aggregate results across all 36 model×classifier combinations for each feature
    # Compute comprehensive statistics and calculate weighted score for ranking

    print(f"\n{'='*80}")
    print("STATISTICAL AGGREGATION AND FEATURE RANKING")
    print(f"{'='*80}")
    print("Computing statistics across all 36 model×classifier combinations...")

    # Calculate comprehensive statistics for each feature×task combination
    # Using 'median' in addition to mean/std/min/max to get more robust statistics
    df_stats = df_ablation.groupby(['task', 'feature'])['macro_f1'].agg([
        'min',      # Minimum F1 (worst-case)
        'median',   # Median F1 (typical performance)
        'mean',     # Mean F1 (average performance)
        'std',      # Standard deviation (consistency)
        'max',      # Maximum F1 (best-case, same as best_f1)
        'count'     # Number of evaluations (should be 36)
    ]).reset_index()

    # Rename columns for clarity
    df_stats.columns = ['task', 'feature', 'min_f1', 'median_f1', 'mean_f1', 'std_f1', 'best_f1', 'runs']

    # Calculate weighted score
    # Formula: weighted_score = 0.5*mean + 0.3*best + 0.2*(1 - normalized_std)
    # This balances:
    # - Average performance (50% weight)
    # - Peak performance (30% weight)
    # - Consistency (20% weight, lower std = higher score)
    #
    # Normalize std by mean to account for scale differences:
    # normalized_std = std_f1 / (mean_f1 + epsilon)
    # where epsilon prevents division by zero
    EPSILON = 1e-6
    df_stats['normalized_std'] = df_stats['std_f1'] / (df_stats['mean_f1'] + EPSILON)

    # Calculate weighted score
    # Higher is better: we want high mean, high best, and low std (high 1-normalized_std)
    df_stats['weighted_score'] = (
        0.5 * df_stats['mean_f1'] +
        0.3 * df_stats['best_f1'] +
        0.2 * (1 - df_stats['normalized_std'])
    )

    # Sort by weighted_score (descending) for ranking
    # Secondary sort by mean_f1 for tie-breaking
    df_stats = df_stats.sort_values(['weighted_score', 'mean_f1'], ascending=False)

    # ========================================================================
    # DISPLAY AND SAVE RANKINGS FOR EACH TASK
    # ========================================================================

    for task in TASKS:
        print(f"\n{'='*80}")
        print(f"TASK: {task.upper()} - FEATURE RANKING")
        print(f"{'='*80}")

        df_task = df_stats[df_stats['task'] == task].copy()

        if len(df_task) == 0:
            print(f"  ⚠ No results found for task: {task}")
            continue

        # Round all numeric columns for display
        numeric_cols = ['min_f1', 'median_f1', 'mean_f1', 'std_f1', 'best_f1', 'runs', 'normalized_std', 'weighted_score']
        df_task[numeric_cols] = df_task[numeric_cols].round(4)

        # Display top 15 features with all statistics
        print(f"\nTop 15 Features (ranked by weighted_score):")
        print("Weighted Score Formula: 0.5*mean + 0.3*best + 0.2*(1 - normalized_std)")
        print("\nColumns:")
        print("  - min_f1: Minimum Macro F1 across 36 combinations (worst-case)")
        print("  - median_f1: Median Macro F1 (typical performance)")
        print("  - mean_f1: Mean Macro F1 (average performance)")
        print("  - std_f1: Standard deviation (lower = more consistent)")
        print("  - best_f1: Maximum Macro F1 (best-case)")
        print("  - runs: Number of evaluations (should be 36)")
        print("  - normalized_std: std_f1 / mean_f1 (scale-normalized consistency)")
        print("  - weighted_score: Combined score for ranking\n")

        from IPython.display import display
        display(df_task[['feature', 'min_f1', 'median_f1', 'mean_f1', 'std_f1', 'best_f1', 'runs', 'normalized_std', 'weighted_score']].head(15))

        # Save complete ranking to CSV
        # Ensure directory exists before saving
        ablation_dir.mkdir(parents=True, exist_ok=True)
        csv_path = ablation_dir / f'feature_ranking_{task}.csv'
        df_task.to_csv(csv_path, index=False)
        print(f"\n  ✓ Saved complete ranking: {csv_path}")
        print(f"    Total features ranked: {len(df_task)}")
        print(f"    Expected runs per feature: 36 (6 models × 6 classifiers)")

        # Verify data completeness
        incomplete = df_task[df_task['runs'] < 36]
        if len(incomplete) > 0:
            print(f"    ⚠ Warning: {len(incomplete)} features have incomplete data (< 36 runs)")

    # ========================================================================
    # TOP-K FEATURE SELECTION FOR EARLY FUSION
    # ========================================================================
    # Select top-K features for each task to use in Early Fusion
    # These features will be used across all models in Early Fusion

    print(f"\n{'='*80}")
    print("TOP-K FEATURE SELECTION FOR EARLY FUSION")
    print(f"{'='*80}")
    print("Selecting top-K features for each task (to be used in Early Fusion)")

    TOP_K_FEATURES = 10  # Number of top features to select for Early Fusion

    selected_features_for_fusion = {}

    for task in TASKS:
        df_task = df_stats[df_stats['task'] == task].copy()

        if len(df_task) == 0:
            print(f"  ⚠ No ranking data found for task: {task}")
            continue

        # Select top-K features by weighted_score
        top_k_features = df_task.head(TOP_K_FEATURES)['feature'].tolist()

        selected_features_for_fusion[task] = {
            'top_k': TOP_K_FEATURES,
            'features': top_k_features,
            'ranking': df_task.head(TOP_K_FEATURES)[['feature', 'weighted_score', 'mean_f1', 'best_f1', 'std_f1']].to_dict('records')
        }

        print(f"\n  {task.upper()} - Top {TOP_K_FEATURES} Features:")
        for i, feat in enumerate(top_k_features, 1):
            row = df_task[df_task['feature'] == feat].iloc[0]
            print(f"    {i:2d}. {feat}")
            print(f"        weighted_score={row['weighted_score']:.4f}, mean_f1={row['mean_f1']:.4f}, best_f1={row['best_f1']:.4f}")

    # Save selected features for Early Fusion
    import json
    # Ensure directory exists before saving
    ablation_dir.mkdir(parents=True, exist_ok=True)
    fusion_features_path = ablation_dir / 'selected_features_for_early_fusion.json'
    with open(fusion_features_path, 'w') as f:
        json.dump(selected_features_for_fusion, f, indent=2)

    print(f"\n{'='*80}")
    print("FEATURE RANKING COMPLETE")
    print(f"{'='*80}")
    print("Rankings saved separately for each task:")
    for task in TASKS:
        print(f"  - {task}: {ablation_dir / f'feature_ranking_{task}.csv'}")
    print(f"\nTop-K features for Early Fusion saved:")
    print(f"  - {fusion_features_path}")
    print(f"  - Top {TOP_K_FEATURES} features per task (to be used across all models in Early Fusion)")

# ============================================================================
# KOD HÜCRESİ 6
# ==============
# ============================================================================
# GREEDY FORWARD SELECTION (OPTIONAL)
# ============================================================================
import json
from tqdm import tqdm

def greedy_forward_selection(X_train, X_dev, y_train, y_dev, feature_names,
                            seed_features, clf, max_features=None):
    """
    Greedy forward selection: iteratively add best feature

    Args:
        X_train: Training features
        X_dev: Dev features
        y_train: Training labels
        y_dev: Dev labels
        feature_names: List of feature names
        seed_features: Initial feature set (list of feature names)
        clf: Classifier instance
        max_features: Maximum number of features to select (None = all)

    Returns:
        selected_features: List of selected feature names
        trajectory: List of (n_features, macro_f1) tuples
    """
    # Encode labels to numeric (required for MLP, XGBoost, LightGBM)
    # This matches the approach in siparismaili01 notebook
    label_encoder = LabelEncoder()
    y_train_encoded = label_encoder.fit_transform(y_train)
    y_dev_encoded = label_encoder.transform(y_dev)

    selected_indices = [feature_names.index(f) for f in seed_features]
    available_indices = [i for i in range(len(feature_names)) if i not in selected_indices]

    trajectory = []

    # Evaluate initial set
    X_train_selected = X_train[:, selected_indices]
    X_dev_selected = X_dev[:, selected_indices]

    pipe = Pipeline([
        ("scaler", StandardScaler()),
        ("clf", clone(clf))
    ])
    pipe.fit(X_train_selected, y_train_encoded)
    pred = pipe.predict(X_dev_selected)
    current_f1 = f1_score(y_dev_encoded, pred, average='macro')
    trajectory.append((len(selected_indices), current_f1))

    # Greedy selection
    max_iter = max_features if max_features else len(available_indices)

    for iteration in tqdm(range(max_iter), desc="Greedy selection"):
        best_f1 = current_f1
        best_idx = None

        # Try each available feature
        for idx in available_indices:
            candidate_indices = selected_indices + [idx]
            X_train_candidate = X_train[:, candidate_indices]
            X_dev_candidate = X_dev[:, candidate_indices]

            pipe = Pipeline([
                ("scaler", StandardScaler()),
                ("clf", clone(clf))
            ])
            pipe.fit(X_train_candidate, y_train_encoded)
            pred = pipe.predict(X_dev_candidate)
            candidate_f1 = f1_score(y_dev_encoded, pred, average='macro')

            if candidate_f1 > best_f1:
                best_f1 = candidate_f1
                best_idx = idx

        # If no improvement, stop
        if best_idx is None:
            break

        # Add best feature
        selected_indices.append(best_idx)
        available_indices.remove(best_idx)
        current_f1 = best_f1
        trajectory.append((len(selected_indices), current_f1))

    selected_features = [feature_names[i] for i in selected_indices]
    return selected_features, trajectory

# Check if required variables are defined
if 'df_stats' not in globals():
    raise NameError(
        "df_stats not found. Please run Cell 5 (Feature Ranking) first.\n"
        "Cell 5 performs statistical analysis and creates df_stats DataFrame."
    )
if 'df_ablation' not in globals():
    raise NameError(
        "df_ablation not found. Please run Cell 5 (Feature Ranking) first.\n"
        "Cell 5 creates df_ablation DataFrame from ablation_results."
    )
if 'TASKS' not in globals() or 'MODELS' not in globals() or 'classifiers' not in globals():
    raise NameError(
        "Required variables not defined. Please run Cell 3 (Configuration) first."
    )
if 'storage' not in globals() or 'ablation_dir' not in globals():
    raise NameError(
        "storage or ablation_dir not found. Please run Cell 1 (Setup) first."
    )

print("="*80)
print("GREEDY FORWARD SELECTION")
print("="*80)
print("Starting with top-K features from weighted score ranking (Cell 5)\n")

# Use global top 10 as seed (same as selected_features_for_early_fusion.json)
TOP_K_SEED = 10  # Start with global top 10 features (from weighted_score ranking)
MAX_FEATURES = 20  # Maximum features to select (global 10 + classifier-specific 10)

selected_features_dict = {}
greedy_trajectories = {}

for task in TASKS:
    print(f"\n{'='*80}")
    print(f"TASK: {task.upper()} - GREEDY FORWARD SELECTION")
    print(f"{'='*80}")

    # Get top-K features for this task (from Cell 5, ranked by weighted_score)
    df_task_stats = df_stats[df_stats['task'] == task].copy()

    if len(df_task_stats) == 0:
        print(f"  ⚠ No ranking data found for task: {task}")
        continue

    # Select top-K features by weighted_score
    top_k_features = df_task_stats.head(TOP_K_SEED)['feature'].tolist()

    print(f"  Top {TOP_K_SEED} seed features (by weighted_score):")
    for i, feat in enumerate(top_k_features, 1):
        row = df_task_stats[df_task_stats['feature'] == feat].iloc[0]
        print(f"    {i}. {feat}")
        print(f"       weighted_score={row['weighted_score']:.4f}, mean={row['mean_f1']:.4f}, best={row['best_f1']:.4f}, std={row['std_f1']:.4f}")

    # Determine which task to use for loading data
    if task == 'hierarchical_evasion_to_clarity':
        # Hierarchical task: use evasion features but evaluate against clarity labels
        # (hierarchical approach maps evasion predictions to clarity)
        data_task = 'evasion'  # Use evasion features and splits
        label_key = 'clarity_label'  # But evaluate against clarity labels
    elif task == 'clarity':
        data_task = 'clarity'
        label_key = 'clarity_label'
    else:  # evasion
        data_task = 'evasion'
        label_key = 'evasion_label'

    # Load task-specific splits
    train_ds = storage.load_split('train', task=data_task)
    dev_ds = storage.load_split('dev', task=data_task)

    # Extract labels
    y_train = np.array([train_ds[i][label_key] for i in range(len(train_ds))])
    y_dev = np.array([dev_ds[i][label_key] for i in range(len(dev_ds))])

    # Get feature names directly from extraction module (same for all tasks and models)
    # This avoids dependency on metadata files in GitHub
    feature_names = get_feature_names()

    # For each model, use the best classifier for this model×task
    for model in MODELS:
        try:
            X_train = storage.load_features(model, data_task, 'train')
            X_dev = storage.load_features(model, data_task, 'dev')
        except FileNotFoundError:
            print(f"  ⚠ Features not found for {model} × {data_task}, skipping...")
            continue

        # Find all classifiers for this model×task from ablation results
        df_model_task = df_ablation[
            (df_ablation['task'] == task) &
            (df_ablation['model'] == model)
        ]

        if len(df_model_task) == 0:
            continue

        # Get classifier scores (by mean F1 across all features)
        classifier_scores = df_model_task.groupby('classifier')['macro_f1'].mean().sort_values(ascending=False)

        # Run greedy selection for EACH classifier (classifier-specific)
        # IMPORTANT: Don't select best classifier here - save ALL classifier results
        # Best classifier selection happens in 4. notebook based on 3. notebook F1 scores
        print(f"\n  {model.upper()} - Running greedy for {len(classifier_scores)} classifiers:")

        for clf_name, clf_mean_f1 in classifier_scores.items():
            clf = classifiers[clf_name]

            # Run greedy selection (starts with global top 10, adds up to 10 more)
            # Each classifier gets: Global top 10 (seed) + classifier-specific greedy 10 = max 20
            selected_features, trajectory = greedy_forward_selection(
                X_train, X_dev, y_train, y_dev,
                feature_names, top_k_features, clf,
                max_features=MAX_FEATURES  # Max 20: global 10 + classifier-specific 10
            )

            final_f1 = trajectory[-1][1] if trajectory else 0.0
            print(f"    {clf_name}: {len(selected_features)} features, F1={final_f1:.4f}")

            # Save EACH classifier's greedy selection (key: model_task_classifier)
            key = f"{model}_{task}_{clf_name}"
            selected_features_dict[key] = {
                'model': model,
                'task': task,
                'classifier': clf_name,
                'selected_features': selected_features,
                'n_features': len(selected_features),
                'greedy_f1': final_f1
            }

            greedy_trajectories[key] = trajectory

            # Save trajectory for each classifier
            # Ensure directory exists before saving
            ablation_dir.mkdir(parents=True, exist_ok=True)
            df_traj = pd.DataFrame(trajectory, columns=['n_features', 'macro_f1'])
            csv_path = ablation_dir / f'greedy_trajectory_{model}_{task}_{clf_name}.csv'
            df_traj.to_csv(csv_path, index=False)
            print(f"      Saved trajectory: {csv_path.name}")

# Save selected features
if selected_features_dict:
    # Ensure directory exists before saving
    ablation_dir.mkdir(parents=True, exist_ok=True)
    json_path = ablation_dir / 'selected_features_all.json'
    with open(json_path, 'w') as f:
        json.dump(selected_features_dict, f, indent=2)
    print(f"\n{'='*80}")
    print(f"Saved selected features: {json_path}")
    print(f"{'='*80}")

print("\n" + "="*80)
print("ABLATION STUDY COMPLETE")
print("="*80)
print("\nSummary:")
print("  ✓ Single-feature ablation completed")
print("  ✓ Feature rankings generated")
print("  ✓ Greedy forward selection completed")
print("  ✓ All results saved to Google Drive")

# ============================================================================
# KOD HÜCRESİ 7
# ==============
# ============================================================================
# CLASSIFIER-SPECIFIC FEATURE SELECTION: Global Top 20 + Classifier-Specific Greedy 20
# ============================================================================
# CHECKPOINT ENABLED: Her adımda kayıt yapılır, tekrar çalıştırıldığında kaldığı yerden devam eder

import numpy as np
import json
import pandas as pd
import pickle
from pathlib import Path
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.metrics import f1_score
from sklearn.base import clone
from tqdm import tqdm
import torch

from src.features.extraction import (
    get_model_independent_feature_names,
    get_model_dependent_feature_names
)
from src.models.classifiers import get_classifier_dict
from src.evaluation.metrics import compute_all_metrics

# ========================================================================
# CHECKPOINT DIRECTORY SETUP
# ========================================================================
results_dir_type2 = storage.data_path / 'results/FinalResultsType2/classifier_specific'
checkpoint_dir = results_dir_type2 / 'checkpoint'
checkpoint_dir.mkdir(parents=True, exist_ok=True)

predictions_dir = results_dir_type2 / 'predictions'
probabilities_dir = results_dir_type2 / 'probabilities'
metrics_dir = results_dir_type2 / 'metrics'
ablation_60_dir = results_dir_type2

predictions_dir.mkdir(parents=True, exist_ok=True)
probabilities_dir.mkdir(parents=True, exist_ok=True)
metrics_dir.mkdir(parents=True, exist_ok=True)
ablation_60_dir.mkdir(parents=True, exist_ok=True)

# ========================================================================
# HELPER FUNCTIONS FOR CHECKPOINT
# ========================================================================
def load_checkpoint(filepath):
    """Load checkpoint file if exists"""
    if filepath.exists():
        try:
            if filepath.suffix == '.pkl':
                with open(filepath, 'rb') as f:
                    return pickle.load(f)
            elif filepath.suffix == '.json':
                with open(filepath, 'r') as f:
                    return json.load(f)
            elif filepath.suffix == '.csv':
                return pd.read_csv(filepath)
            elif filepath.suffix == '.npy':
                return np.load(filepath)
        except Exception as e:
            print(f"    ⚠ Warning: Could not load {filepath.name}: {e}")
            return None
    return None

def save_checkpoint(data, filepath):
    """Save checkpoint file"""
    filepath.parent.mkdir(parents=True, exist_ok=True)
    try:
        if filepath.suffix == '.pkl':
            with open(filepath, 'wb') as f:
                pickle.dump(data, f)
        elif filepath.suffix == '.json':
            with open(filepath, 'w') as f:
                json.dump(data, f, indent=2)
        elif filepath.suffix == '.csv':
            if isinstance(data, pd.DataFrame):
                data.to_csv(filepath, index=False)
            else:
                pd.DataFrame(data).to_csv(filepath, index=False)
        elif filepath.suffix == '.npy':
            np.save(filepath, data)
    except Exception as e:
        print(f"    ⚠ Warning: Could not save {filepath.name}: {e}")

# ========================================================================
# STEP 1: Create 60 Feature Names
# ========================================================================
print("\n" + "="*80)
print("STEP 1: CREATE 60 FEATURE NAMES")
print("="*80)

# Checkpoint for feature names
fused_feature_names_60_path = checkpoint_dir / 'fused_feature_names_60.json'
feature_name_to_idx_60_path = checkpoint_dir / 'feature_name_to_idx_60.json'

fused_feature_names_60 = load_checkpoint(fused_feature_names_60_path)
feature_name_to_idx_60 = load_checkpoint(feature_name_to_idx_60_path)

if fused_feature_names_60 is None:
    # Get base feature names
    indep_feature_names = get_model_independent_feature_names()  # 18 features
    dep_feature_names = get_model_dependent_feature_names()  # 7 features

    # Create 60 feature names
    fused_feature_names_60 = indep_feature_names.copy()
    MODELS_60 = ['bert', 'bert_political', 'bert_ambiguity', 'roberta', 'deberta', 'xlnet']
    for model in MODELS_60:
        for dep_name in dep_feature_names:
            fused_feature_names_60.append(f"{model}_{dep_name}")

    feature_name_to_idx_60 = {name: idx for idx, name in enumerate(fused_feature_names_60)}

    save_checkpoint(fused_feature_names_60, fused_feature_names_60_path)
    save_checkpoint(feature_name_to_idx_60, feature_name_to_idx_60_path)
    print(f"✓ Created and saved 60 feature names")
else:
    print(f"✓ Loaded 60 feature names from checkpoint")

print(f"  - Model-independent: 18 features")
print(f"  - Model-dependent: 6 models × 7 features = 42 features")
print(f"  - Total: {len(fused_feature_names_60)} features")

# ========================================================================
# STEP 2: Load 60 Features for Train/Dev/Test
# ========================================================================
print("\n" + "="*80)
print("STEP 2: LOAD 60 FEATURES (EARLY FUSION)")
print("="*80)

TASKS_60 = ['clarity', 'hierarchical_evasion_to_clarity']
features_60 = {}  # {task: {'train': X_train_60, 'dev': X_dev_60, 'test': X_test_60}}

# Checkpoint for features
features_60_path = checkpoint_dir / 'features_60.pkl'
features_60 = load_checkpoint(features_60_path)

if features_60 is None:
    features_60 = {}

    for task in TASKS_60:
        print(f"\n{'-'*60}")
        print(f"Task: {task.upper()}")
        print(f"{'-'*60}")

        split_task = 'evasion' if task == 'hierarchical_evasion_to_clarity' else task

        # Load model-independent features
        X_train_indep = storage.load_model_independent_features('train', task=split_task)
        X_dev_indep = storage.load_model_independent_features('dev', task=split_task)

        # Load test model-independent features
        X_test_indep = None
        test_indep_path_type1 = storage.data_path / f'results/FinalResultsType1/test/X_test_independent_{split_task}.npy'
        if test_indep_path_type1.exists():
            X_test_indep = np.load(test_indep_path_type1)
            print(f"  ✓ Loaded test model-independent features from FinalResultsType1: {X_test_indep.shape}")
        else:
            test_indep_path_early = storage.data_path / f'results/FinalResultsType3/test_features/X_test_independent_{split_task}.npy'
            if test_indep_path_early.exists():
                X_test_indep = np.load(test_indep_path_early)
                print(f"  ✓ Loaded test model-independent features from early fusion: {X_test_indep.shape}")
            else:
                try:
                    X_test_indep = storage.load_model_independent_features('test', task=split_task)
                    print(f"  ✓ Loaded test model-independent features from standard location: {X_test_indep.shape}")
                except FileNotFoundError:
                    print(f"  ⚠ Test model-independent features not found. Extracting...")
                    from src.features.extraction import featurize_model_independent_features
                    from transformers import pipeline

                    test_ds = storage.load_split('test', task=split_task)
                    sentiment_pipeline = pipeline(
                        "sentiment-analysis",
                        model="cardiffnlp/twitter-roberta-base-sentiment-latest",
                        device=0 if torch.cuda.is_available() else -1,
                        return_all_scores=True
                    ) if torch.cuda.is_available() else None

                    metadata_keys = {
                        'inaudible': 'inaudible',
                        'multiple_questions': 'multiple_questions',
                        'affirmative_questions': 'affirmative_questions'
                    }

                    X_test_indep, _ = featurize_model_independent_features(
                        test_ds, question_key='interview_question', answer_key='interview_answer',
                        batch_size=32, show_progress=True, sentiment_pipeline=sentiment_pipeline,
                        metadata_keys=metadata_keys,
                    )

                    test_indep_path_type1.parent.mkdir(parents=True, exist_ok=True)
                    np.save(test_indep_path_type1, X_test_indep)
                    print(f"  ✓ Extracted and saved test model-independent features: {X_test_indep.shape}")

                    if sentiment_pipeline is not None:
                        del sentiment_pipeline
                        if torch.cuda.is_available():
                            torch.cuda.empty_cache()

        # Load model-dependent features
        model_dep_train_list = []
        model_dep_dev_list = []
        model_dep_test_list = []

        MODELS_60 = ['bert', 'bert_political', 'bert_ambiguity', 'roberta', 'deberta', 'xlnet']
        for model in MODELS_60:
            X_train_full = storage.load_features(model, split_task, 'train')
            X_dev_full = storage.load_features(model, split_task, 'dev')

            X_test_full = None
            test_feature_path_type1 = storage.data_path / f'results/FinalResultsType1/test/X_{model}_{split_task}_test.npy'
            if test_feature_path_type1.exists():
                X_test_full = np.load(test_feature_path_type1)
                print(f"    ✓ Loaded {model} test features from FinalResultsType1: {X_test_full.shape}")
            else:
                try:
                    X_test_full = storage.load_features(model, split_task, 'test')
                    print(f"    ✓ Loaded {model} test features from standard location: {X_test_full.shape}")
                except FileNotFoundError:
                    raise FileNotFoundError(
                        f"Test features not found for model '{model}' and task '{split_task}'.\n"
                        f"  Checked locations:\n"
                        f"    1. {test_feature_path_type1}\n"
                        f"    2. Standard storage location\n"
                        f"  Please run notebook 5 (Final Evaluation) or early fusion notebook to extract test features first."
                    )

            model_dep_train_list.append(X_train_full[:, :7])
            model_dep_dev_list.append(X_dev_full[:, :7])
            model_dep_test_list.append(X_test_full[:, :7])

        X_train_dep_concat = np.hstack(model_dep_train_list)
        X_dev_dep_concat = np.hstack(model_dep_dev_list)
        X_test_dep_concat = np.hstack(model_dep_test_list)

        X_train_60 = np.hstack([X_train_indep, X_train_dep_concat])
        X_dev_60 = np.hstack([X_dev_indep, X_dev_dep_concat])
        X_test_60 = np.hstack([X_test_indep, X_test_dep_concat])

        features_60[task] = {
            'train': X_train_60,
            'dev': X_dev_60,
            'test': X_test_60
        }

        print(f"  Train: {X_train_60.shape[0]} samples, {X_train_60.shape[1]} features")
        print(f"  Dev: {X_dev_60.shape[0]} samples, {X_dev_60.shape[1]} features")
        print(f"  Test: {X_test_60.shape[0]} samples, {X_test_60.shape[1]} features")

    save_checkpoint(features_60, features_60_path)
    print(f"\n✓ Saved features_60 to checkpoint")
else:
    print(f"✓ Loaded features_60 from checkpoint")
    for task in TASKS_60:
        X_train_60 = features_60[task]['train']
        X_dev_60 = features_60[task]['dev']
        X_test_60 = features_60[task]['test']
        print(f"  {task}: Train={X_train_60.shape}, Dev={X_dev_60.shape}, Test={X_test_60.shape}")

# ========================================================================
# STEP 3: Select Global Top 20 Features
# ========================================================================
print("\n" + "="*80)
print("STEP 3: SELECT GLOBAL TOP 20 FEATURES (60 FEATURE'LI SİSTEM ÜZERİNDE)")
print("="*80)

# Checkpoint paths
df_ablation_60_path = ablation_60_dir / 'ablation_results_60_features.csv'
df_stats_60_path = ablation_60_dir / 'feature_ranking_60_features.csv'
global_top_20_dict_path = checkpoint_dir / 'global_top_20_dict.json'

# Load checkpoints
df_ablation_60 = load_checkpoint(df_ablation_60_path)
df_stats_60 = load_checkpoint(df_stats_60_path)
global_top_20_dict = load_checkpoint(global_top_20_dict_path)

if df_ablation_60 is None or df_stats_60 is None or global_top_20_dict is None:
    print("60 feature'lı sistem üzerinde 6 classifier ile single-feature ablation yapılıyor...")

    classifiers_60 = get_classifier_dict(random_state=42)
    ablation_results_60 = []

    for task in TASKS_60:
        print(f"\n{'-'*60}")
        print(f"Task: {task.upper()}")
        print(f"{'-'*60}")

        X_train_60 = features_60[task]['train']
        X_dev_60 = features_60[task]['dev']

        split_task = 'evasion' if task == 'hierarchical_evasion_to_clarity' else task
        label_key = 'clarity_label' if task == 'hierarchical_evasion_to_clarity' else ('clarity_label' if task == 'clarity' else 'evasion_label')

        train_ds = storage.load_split('train', task=split_task)
        dev_ds = storage.load_split('dev', task=split_task)

        y_train = np.array([train_ds[i][label_key] for i in range(len(train_ds))])
        y_dev = np.array([dev_ds[i][label_key] for i in range(len(dev_ds))])

        le = LabelEncoder()
        y_train_encoded = le.fit_transform(y_train)
        y_dev_encoded = le.transform(y_dev)

        print(f"  Train: {len(y_train)} samples")
        print(f"  Dev: {len(y_dev)} samples")
        print(f"  Features: {X_train_60.shape[1]} features")

        for clf_name, clf in classifiers_60.items():
            print(f"  Classifier: {clf_name}...")

            for feature_idx, feature_name in enumerate(fused_feature_names_60):
                X_train_single = X_train_60[:, [feature_idx]]
                X_dev_single = X_dev_60[:, [feature_idx]]

                pipe = Pipeline([("scaler", StandardScaler()), ("clf", clone(clf))])
                pipe.fit(X_train_single, y_train_encoded)
                pred = pipe.predict(X_dev_single)
                macro_f1 = f1_score(y_dev_encoded, pred, average='macro')

                ablation_results_60.append({
                    'task': task,
                    'classifier': clf_name,
                    'feature': feature_name,
                    'feature_idx': feature_idx,
                    'macro_f1': float(macro_f1)
                })

        print(f"  ✓ Completed ablation for {task}: {len(classifiers_60)} classifiers × {len(fused_feature_names_60)} features = {len(classifiers_60) * len(fused_feature_names_60)} evaluations")

    print("\nCalculating weighted scores for 60 features...")

    df_ablation_60 = pd.DataFrame(ablation_results_60)

    df_stats_60 = df_ablation_60.groupby(['task', 'feature'])['macro_f1'].agg([
        'min', 'median', 'mean', 'std', 'max', 'count'
    ]).reset_index()

    df_stats_60.columns = ['task', 'feature', 'min_f1', 'median_f1', 'mean_f1', 'std_f1', 'best_f1', 'runs']

    EPSILON = 1e-6
    df_stats_60['normalized_std'] = df_stats_60['std_f1'] / (df_stats_60['mean_f1'] + EPSILON)
    df_stats_60['weighted_score'] = (
        0.5 * df_stats_60['mean_f1'] +
        0.3 * df_stats_60['best_f1'] +
        0.2 * (1 - df_stats_60['normalized_std'])
    )

    df_stats_60 = df_stats_60.sort_values(['task', 'weighted_score'], ascending=[True, False])

    global_top_20_dict = {}
    for task in TASKS_60:
        df_task_stats = df_stats_60[df_stats_60['task'] == task].copy()
        if len(df_task_stats) == 0:
            global_top_20_dict[task] = fused_feature_names_60[:20]
        else:
            global_top_20_dict[task] = df_task_stats.head(20)['feature'].tolist()

        print(f"\n  {task.upper()} - Global Top 20 Features:")
        for i, feat in enumerate(global_top_20_dict[task][:10], 1):
            row = df_task_stats[df_task_stats['feature'] == feat].iloc[0]
            print(f"    {i:2d}. {feat}")
            print(f"        weighted_score={row['weighted_score']:.4f}, mean_f1={row['mean_f1']:.4f}, best_f1={row['best_f1']:.4f}")
        print(f"    ... (showing first 10 of 20)")

    save_checkpoint(df_ablation_60, df_ablation_60_path)
    save_checkpoint(df_stats_60, df_stats_60_path)
    save_checkpoint(global_top_20_dict, global_top_20_dict_path)
    print(f"\n✓ Saved STEP 3 results to checkpoint")
else:
    print(f"✓ Loaded STEP 3 results from checkpoint")

# ========================================================================
# STEP 4: Greedy Forward Selection for Each Classifier
# ========================================================================
print("\n" + "="*80)
print("STEP 4: GREEDY FORWARD SELECTION (PER CLASSIFIER)")
print("="*80)



def greedy_forward_selection_60_with_checkpoint(X_train, X_dev, y_train, y_dev, feature_names_60,
                                                 seed_features, clf, max_features=None,
                                                 checkpoint_path=None):
    """
    Greedy forward selection with checkpoint support
    """
    label_encoder = LabelEncoder()
    y_train_encoded = label_encoder.fit_transform(y_train)
    y_dev_encoded = label_encoder.transform(y_dev)

    # Try to load checkpoint
    if checkpoint_path and checkpoint_path.exists():
        try:
            checkpoint_data = load_checkpoint(checkpoint_path)
            if checkpoint_data and isinstance(checkpoint_data, dict):
                selected_indices = checkpoint_data.get('selected_indices', [])
                available_indices = checkpoint_data.get('available_indices', [])
                trajectory = checkpoint_data.get('trajectory', [])
                initial_f1 = checkpoint_data.get('initial_f1', 0.0)
                print(f"      ✓ Resuming from checkpoint: {len(selected_indices)} features already selected")
            else:
                raise ValueError("Invalid checkpoint format")
        except Exception as e:
            print(f"      ⚠ Could not load checkpoint: {e}, starting fresh")
            selected_indices = [feature_names_60.index(f) for f in seed_features if f in feature_names_60]
            available_indices = [i for i in range(len(feature_names_60)) if i not in selected_indices]
            trajectory = []
            initial_f1 = 0.0
    else:
        # Start fresh
        selected_indices = [feature_names_60.index(f) for f in seed_features if f in feature_names_60]
        available_indices = [i for i in range(len(feature_names_60)) if i not in selected_indices]
        trajectory = []
        initial_f1 = 0.0

    # Evaluate initial set if not loaded from checkpoint
    if len(trajectory) == 0:
        X_train_selected = X_train[:, selected_indices]
        X_dev_selected = X_dev[:, selected_indices]

        pipe = Pipeline([("scaler", StandardScaler()), ("clf", clone(clf))])
        pipe.fit(X_train_selected, y_train_encoded)
        pred = pipe.predict(X_dev_selected)
        initial_f1 = f1_score(y_dev_encoded, pred, average='macro')
        trajectory.append((len(selected_indices), initial_f1))

    # Iteratively add best feature
    iteration = 0
    while len(available_indices) > 0:
        if max_features is not None and len(selected_indices) >= max_features:
            break

        best_f1 = initial_f1
        best_idx = None

        # Save checkpoint before starting iteration
        if checkpoint_path:
            checkpoint_data = {
                'selected_indices': selected_indices,
                'available_indices': available_indices,
                'trajectory': trajectory,
                'initial_f1': initial_f1,
                'iteration': iteration
            }
            save_checkpoint(checkpoint_data, checkpoint_path)

        for idx in tqdm(available_indices, desc=f"Greedy ({len(selected_indices)}/{max_features})", leave=False):
            test_indices = selected_indices + [idx]
            X_train_test = X_train[:, test_indices]
            X_dev_test = X_dev[:, test_indices]

            pipe = Pipeline([("scaler", StandardScaler()), ("clf", clone(clf))])
            pipe.fit(X_train_test, y_train_encoded)
            pred = pipe.predict(X_dev_test)
            f1 = f1_score(y_dev_encoded, pred, average='macro')

            if f1 > best_f1:
                best_f1 = f1
                best_idx = idx

        if best_idx is None or best_f1 <= initial_f1:
            break

        selected_indices.append(best_idx)
        available_indices.remove(best_idx)
        initial_f1 = best_f1
        trajectory.append((len(selected_indices), best_f1))
        iteration += 1

        # Save checkpoint after each successful iteration
        if checkpoint_path:
            checkpoint_data = {
                'selected_indices': selected_indices,
                'available_indices': available_indices,
                'trajectory': trajectory,
                'initial_f1': initial_f1,
                'iteration': iteration
            }
            save_checkpoint(checkpoint_data, checkpoint_path)

    selected_features = [feature_names_60[i] for i in selected_indices]

    # Delete checkpoint after successful completion
    if checkpoint_path and checkpoint_path.exists():
        try:
            checkpoint_path.unlink()
        except:
            pass

    return selected_features, trajectory

# Initialize classifier_specific_results
classifier_specific_results = {}  # {task: {classifier: {features, metrics, predictions, probabilities}}}

for task in TASKS_60:
    print(f"\n{'-'*80}")
    print(f"TASK: {task.upper()}")
    print(f"{'-'*80}")

    if task not in classifier_specific_results:
        classifier_specific_results[task] = {}

    X_train_60 = features_60[task]['train']
    X_dev_60 = features_60[task]['dev']
    X_test_60 = features_60[task]['test']

    split_task = 'evasion' if task == 'hierarchical_evasion_to_clarity' else task
    label_key = 'clarity_label' if task == 'hierarchical_evasion_to_clarity' else ('clarity_label' if task == 'clarity' else 'evasion_label')
    label_list = CLARITY_LABELS if 'clarity' in task else EVASION_LABELS

    train_ds = storage.load_split('train', task=split_task)
    dev_ds = storage.load_split('dev', task=split_task)
    test_ds = storage.load_split('test', task=split_task)

    y_train = np.array([train_ds[i][label_key] for i in range(len(train_ds))])
    y_dev = np.array([dev_ds[i][label_key] for i in range(len(dev_ds))])
    y_test = np.array([test_ds[i][label_key] for i in range(len(test_ds))])

    global_top_20 = global_top_20_dict[task]
    classifiers_60 = get_classifier_dict(random_state=42)

    for clf_name, clf in classifiers_60.items():
        print(f"\n  Classifier: {clf_name}")
        max_features = 25 if clf_name == "LightGBM" else 40

        # Checkpoint paths for this classifier
        selected_features_path = checkpoint_dir / f'selected_features_{clf_name}_{task}.json'
        trajectory_path = checkpoint_dir / f'trajectory_{clf_name}_{task}.csv'
        predictions_path = predictions_dir / f'{clf_name}_{task}_predictions.npy'
        probabilities_path = probabilities_dir / f'{clf_name}_{task}_probabilities.npy'
        metrics_path = checkpoint_dir / f'metrics_{clf_name}_{task}.json'
        greedy_checkpoint_path = checkpoint_dir / f'greedy_checkpoint_{clf_name}_{task}.pkl'  # NEW

        # CRITICAL FIX 1: Check if predictions exist first (this means classifier is complete)
        y_test_pred = load_checkpoint(predictions_path)

        if y_test_pred is not None:
            print(f"    ✓ Found predictions for {clf_name}, loading from checkpoint...")

            # Load other data if available
            selected_features = load_checkpoint(selected_features_path)
            trajectory_data = load_checkpoint(trajectory_path)
            y_test_proba = load_checkpoint(probabilities_path)
            metrics = load_checkpoint(metrics_path)

            # If selected_features not saved, we can't reconstruct it, but predictions exist
            if selected_features is None:
                print(f"    ⚠ Note: selected_features not found, but predictions exist (from previous run)")
                selected_features = []

            # Load trajectory
            if isinstance(trajectory_data, pd.DataFrame):
                trajectory = [(row['n_features'], row['macro_f1']) for _, row in trajectory_data.iterrows()]
            else:
                trajectory = trajectory_data if isinstance(trajectory_data, list) else []

            # Store in results
            classifier_specific_results[task][clf_name] = {
                'selected_features': selected_features,
                'n_features': len(selected_features) if selected_features else 0,
                'metrics': metrics if metrics else {},
                'predictions': y_test_pred,
                'probabilities': y_test_proba,
                'trajectory': trajectory
            }

            print(f"    ✓ Loaded: predictions shape: {y_test_pred.shape}")
            if y_test_proba is not None:
                print(f"    ✓ Probabilities available")
            continue  # ← ATLA, tekrar çalıştırma

        # If checkpoint not found, run greedy selection WITH checkpoint support
        print(f"    Running greedy selection (starting with global top 20, max 40 features)...")
        selected_features, trajectory = greedy_forward_selection_60_with_checkpoint(
            X_train_60, X_dev_60, y_train, y_dev,
            fused_feature_names_60, global_top_20, clf,
            max_features=max_features,
            checkpoint_path=greedy_checkpoint_path  # NEW: Pass checkpoint path
        )

        # CRITICAL FIX 2: Print mesajını daha açıklayıcı yap
        final_f1 = trajectory[-1][1] if trajectory else 0.0
        n_global = len(global_top_20)
        n_greedy = len(selected_features) - n_global
        print(f"    ✓ Selected {len(selected_features)} features (Global: {n_global} + Greedy: {n_greedy}), Dev F1={final_f1:.4f}")

        # Save selected_features and trajectory immediately
        save_checkpoint(selected_features, selected_features_path)
        traj_df = pd.DataFrame(trajectory, columns=['n_features', 'macro_f1'])
        save_checkpoint(traj_df, trajectory_path)
        print(f"    ✓ Saved greedy selection results to checkpoint")

        # Get feature indices and extract
        selected_indices = [feature_name_to_idx_60[name] for name in selected_features if name in feature_name_to_idx_60]
        X_train_selected = X_train_60[:, selected_indices]
        X_dev_selected = X_dev_60[:, selected_indices]
        X_test_selected = X_test_60[:, selected_indices]

        # Train final model
        print(f"    Training on Train+Dev combined ({X_train_selected.shape[0] + X_dev_selected.shape[0]} samples)...")
        X_train_combined = np.vstack([X_train_selected, X_dev_selected])
        y_train_combined = np.concatenate([y_train, y_dev])

        le = LabelEncoder()
        y_train_combined_encoded = le.fit_transform(y_train_combined)
        y_test_encoded = le.transform(y_test)

        pipe = Pipeline([("scaler", StandardScaler()), ("clf", clone(clf))])
        pipe.fit(X_train_combined, y_train_combined_encoded)

        # Predict on test
        y_test_pred_encoded = pipe.predict(X_test_selected)
        y_test_pred = le.inverse_transform(y_test_pred_encoded)

        # Get probabilities
        y_test_proba = None
        if hasattr(pipe.named_steps['clf'], 'predict_proba'):
            try:
                X_test_scaled = pipe.named_steps['scaler'].transform(X_test_selected)
                y_test_proba = pipe.named_steps['clf'].predict_proba(X_test_scaled)
            except Exception as e:
                print(f"      ⚠ Warning: Could not get probabilities for {clf_name}: {e}")

        # Compute metrics
        metrics = compute_all_metrics(
            y_test_encoded, y_test_pred_encoded, label_list,
            task_name=f"TEST_{task}_{clf_name}"
        )

        print(f"    Test Macro F1: {metrics.get('macro_f1', 0.0):.4f}")

        # Save predictions, probabilities, and metrics immediately
        save_checkpoint(y_test_pred, predictions_path)
        if y_test_proba is not None:
            save_checkpoint(y_test_proba, probabilities_path)
            print(f"    ✓ Saved probabilities: {probabilities_path}")
        else:
            print(f"    ⚠ No probabilities available for {clf_name}")

        # Save metrics (convert to JSON-serializable)
        metrics_serializable = {}
        for k, v in metrics.items():
            if isinstance(v, (int, float, np.integer, np.floating)):
                metrics_serializable[k] = float(v)
            elif isinstance(v, (list, np.ndarray)):
                metrics_serializable[k] = v.tolist() if isinstance(v, np.ndarray) else v
            elif isinstance(v, dict):
                metrics_serializable[k] = v
            else:
                metrics_serializable[k] = str(v)
        save_checkpoint(metrics_serializable, metrics_path)

        # Store in results
        classifier_specific_results[task][clf_name] = {
            'selected_features': selected_features,
            'n_features': len(selected_features),
            'metrics': metrics,
            'predictions': y_test_pred,
            'probabilities': y_test_proba,
            'trajectory': trajectory
        }

print("\n" + "="*80)
print("GREEDY FORWARD SELECTION COMPLETE")
print("="*80)

# ========================================================================
# STEP 5: Weighted Average Ensemble
# ========================================================================
print("\n" + "="*80)
print("STEP 5: WEIGHTED AVERAGE ENSEMBLE")
print("="*80)

ensemble_results = {}

for task in TASKS_60:
    print(f"\n{'-'*80}")
    print(f"TASK: {task.upper()}")
    print(f"{'-'*80}")

    # Checkpoint paths for ensemble
    ensemble_pred_path = predictions_dir / f'ensemble_hard_labels_from_weighted_proba_{task}.npy'
    ensemble_proba_path = probabilities_dir / f'ensemble_weighted_average_probabilities_{task}.npy'
    ensemble_weights_path = metrics_dir / f'ensemble_classifier_weights_{task}.json'
    ensemble_metrics_path = metrics_dir / f'ensemble_evaluation_metrics_{task}.json'

    # Check if ensemble already computed
    ensemble_pred = load_checkpoint(ensemble_pred_path)
    ensemble_proba = load_checkpoint(ensemble_proba_path)
    ensemble_weights = load_checkpoint(ensemble_weights_path)
    ensemble_metrics = load_checkpoint(ensemble_metrics_path)

    if ensemble_pred is not None and ensemble_proba is not None:
        print(f"  ✓ Found ensemble checkpoint, loading...")
        ensemble_results[task] = {
            'predictions': ensemble_pred,
            'probabilities': ensemble_proba,
            'weights': ensemble_weights if ensemble_weights else {},
            'classifiers_used': ensemble_weights.get('classifiers', []) if ensemble_weights else []
        }
        print(f"  ✓ Ensemble predictions shape: {ensemble_pred.shape}")
        if ensemble_metrics:
            ensemble_f1 = ensemble_metrics.get('metrics', {}).get('macro_f1', 0.0)
            print(f"  ✓ Ensemble Test Macro F1: {ensemble_f1:.4f}")
        continue

    if task not in classifier_specific_results:
        print(f"  ⚠ Skipping {task}: No results available")
        continue

    label_list = CLARITY_LABELS if 'clarity' in task else EVASION_LABELS

    # Collect probabilities and weights
    probabilities_list = []
    weights_list = []
    classifier_names_list = []

    for clf_name, result in classifier_specific_results[task].items():
        y_proba = result.get('probabilities')
        if y_proba is None:
            print(f"  ⚠ Skipping {clf_name}: No probabilities available")
            continue

        metrics = result.get('metrics', {})
        macro_f1 = metrics.get('macro_f1', 0.0)
        weight = max(macro_f1, 0.0001)

        probabilities_list.append(y_proba)
        weights_list.append(weight)
        classifier_names_list.append(clf_name)

    if len(probabilities_list) == 0:
        print(f"  ⚠ No probabilities available for {task}. Skipping ensemble.")
        continue

    # Normalize weights
    total_weight = sum(weights_list)
    normalized_weights = [w / total_weight for w in weights_list] if total_weight > 0 else [1.0 / len(weights_list)] * len(weights_list)

    print(f"\n  Normalized weights (based on Macro F1):")
    for clf_name, norm_weight, macro_f1 in zip(classifier_names_list, normalized_weights, weights_list):
        print(f"    {clf_name}: {norm_weight:.4f} (Macro F1: {macro_f1:.4f})")

    # Weighted average ensemble
    ensemble_proba = np.zeros_like(probabilities_list[0])
    for proba, weight in zip(probabilities_list, normalized_weights):
        ensemble_proba += weight * proba

    ensemble_pred_indices = np.argmax(ensemble_proba, axis=1)
    ensemble_pred = np.array([label_list[i] for i in ensemble_pred_indices])

    print(f"    ✓ Ensemble predictions shape: {ensemble_pred.shape}")

    # Save ensemble results immediately
    save_checkpoint(ensemble_pred, ensemble_pred_path)
    save_checkpoint(ensemble_proba, ensemble_proba_path)
    print(f"    ✓ Saved ensemble predictions and probabilities")

    # Evaluate ensemble
    split_task = 'evasion' if task == 'hierarchical_evasion_to_clarity' else task
    test_ds = storage.load_split('test', task=split_task)
    label_key = 'clarity_label' if 'clarity' in task else 'evasion_label'
    y_test_true = np.array([test_ds[i][label_key] for i in range(len(test_ds))])

    le = LabelEncoder()
    y_test_true_encoded = le.fit_transform(y_test_true)
    ensemble_pred_encoded = le.transform(ensemble_pred)

    ensemble_metrics = compute_all_metrics(
        y_test_true_encoded, ensemble_pred_encoded, label_list,
        task_name=f"ENSEMBLE_{task}"
    )

    print(f"    Ensemble Test Macro F1: {ensemble_metrics.get('macro_f1', 0.0):.4f}")

    # Save weights
    weights_metadata = {
        'task': task,
        'method': 'weighted_average',
        'weight_metric': 'macro_f1',
        'n_classifiers': len(classifier_names_list),
        'classifiers': classifier_names_list,
        'weights': {name: float(weight) for name, weight in zip(classifier_names_list, normalized_weights)},
        'n_samples': len(ensemble_pred),
        'label_list': label_list
    }
    save_checkpoint(weights_metadata, ensemble_weights_path)

    # Save metrics
    metrics_serializable = {}
    for k, v in ensemble_metrics.items():
        if isinstance(v, (int, float, np.integer, np.floating)):
            metrics_serializable[k] = float(v)
        elif isinstance(v, (list, np.ndarray)):
            metrics_serializable[k] = v.tolist() if isinstance(v, np.ndarray) else v
        elif isinstance(v, dict):
            metrics_serializable[k] = v
        else:
            metrics_serializable[k] = str(v)

    save_checkpoint({
        'task': task,
        'metrics': metrics_serializable,
        'n_samples': len(ensemble_pred)
    }, ensemble_metrics_path)

    ensemble_results[task] = {
        'predictions': ensemble_pred,
        'probabilities': ensemble_proba,
        'weights': {name: weight for name, weight in zip(classifier_names_list, normalized_weights)},
        'classifiers_used': classifier_names_list
    }

print("\n" + "="*80)
print("CLASSIFIER-SPECIFIC FEATURE SELECTION COMPLETE")
print("="*80)
print(f"\nResults saved to: {results_dir_type2}")
print(f"  - Checkpoint directory: {checkpoint_dir}")
print(f"  - All intermediate results are saved and can be resumed from checkpoint")

# ============================================================================
# KOD HÜCRESİ 8
# ==============
# ============================================================================
# STEP 6: Generate Summary Report Tables
# ============================================================================
# This cell can run independently after Cell 7 completes STEP 5
# It loads all results from checkpoints and generates report tables

import numpy as np
import pandas as pd
import json
import pickle
from pathlib import Path

# Check if required variables exist
if 'storage' not in globals():
    raise NameError("storage not found. Please run Cell 1 (Setup) first.")

if 'CLARITY_LABELS' not in globals() or 'EVASION_LABELS' not in globals():
    raise NameError("CLARITY_LABELS and EVASION_LABELS not found. Please run Cell 3 (Configuration) first.")

# Setup directories
results_dir_type2 = storage.data_path / 'results/FinalResultsType2/classifier_specific'
checkpoint_dir = results_dir_type2 / 'checkpoint'
predictions_dir = results_dir_type2 / 'predictions'
probabilities_dir = results_dir_type2 / 'probabilities'
metrics_dir = results_dir_type2 / 'metrics'
tables_dir = results_dir_type2 / 'tables'

tables_dir.mkdir(parents=True, exist_ok=True)

# Helper function to load checkpoints
def load_checkpoint(filepath):
    """Load checkpoint file if exists"""
    if filepath.exists():
        try:
            if filepath.suffix == '.pkl':
                with open(filepath, 'rb') as f:
                    return pickle.load(f)
            elif filepath.suffix == '.json':
                with open(filepath, 'r') as f:
                    return json.load(f)
            elif filepath.suffix == '.csv':
                return pd.read_csv(filepath)
            elif filepath.suffix == '.npy':
                return np.load(filepath)
        except Exception as e:
            print(f"    ⚠ Warning: Could not load {filepath.name}: {e}")
            return None
    return None

# Task order
TASK_ORDER = ['clarity', 'hierarchical_evasion_to_clarity']

# ========================================================================
# STEP 6.1: Load Classifier Results from Checkpoints
# ========================================================================
print("\n" + "="*80)
print("STEP 6: GENERATE SUMMARY REPORT TABLES")
print("="*80)
print("\n" + "-"*80)
print("STEP 6.1: Loading Classifier Results from Checkpoints")
print("-"*80)

classifier_specific_results = {}

for task in TASK_ORDER:
    classifier_specific_results[task] = {}
    
    for clf_name in ['LogisticRegression', 'LinearSVC', 'RandomForest', 'MLP', 'XGBoost', 'LightGBM']:
        predictions_path = predictions_dir / f'{clf_name}_{task}_predictions.npy'
        probabilities_path = probabilities_dir / f'{clf_name}_{task}_probabilities.npy'
        metrics_path = checkpoint_dir / f'metrics_{clf_name}_{task}.json'
        selected_features_path = checkpoint_dir / f'selected_features_{clf_name}_{task}.json'
        
        y_test_pred = load_checkpoint(predictions_path)
        if y_test_pred is not None:
            y_test_proba = load_checkpoint(probabilities_path)
            metrics = load_checkpoint(metrics_path)
            selected_features = load_checkpoint(selected_features_path)
            
            classifier_specific_results[task][clf_name] = {
                'selected_features': selected_features if selected_features else [],
                'n_features': len(selected_features) if selected_features else 0,
                'metrics': metrics if metrics else {},
                'predictions': y_test_pred,
                'probabilities': y_test_proba,
            }
            print(f"    ✓ Loaded {clf_name} for {task}")
        else:
            print(f"    ⚠ {clf_name} for {task} not found")

# ========================================================================
# STEP 6.2: Collect All Classifier Results (Individual Classifiers)
# ========================================================================
print("\n" + "-"*80)
print("STEP 6.2: Individual Classifier Results")
print("-"*80)

summary_rows = []

for task in TASK_ORDER:
    if task not in classifier_specific_results:
        continue
    
    for clf_name, result in classifier_specific_results[task].items():
        metrics = result.get('metrics', {})
        n_features = result.get('n_features', 0)
        
        summary_rows.append({
            'classifier': clf_name,
            'task': task,
            'n_features': n_features,
            'macro_f1': metrics.get('macro_f1', 0.0),
            'accuracy': metrics.get('accuracy', 0.0),
            'macro_precision': metrics.get('macro_precision', 0.0),
            'macro_recall': metrics.get('macro_recall', 0.0),
            'weighted_f1': metrics.get('weighted_f1', 0.0),
        })

# ========================================================================
# STEP 6.3: Add Ensemble Results
# ========================================================================
print("\n" + "-"*80)
print("STEP 6.3: Ensemble Results (Weighted Average)")
print("-"*80)

for task in TASK_ORDER:
    ensemble_metrics_path = metrics_dir / f'ensemble_evaluation_metrics_{task}.json'
    ensemble_metrics = load_checkpoint(ensemble_metrics_path)
    
    if ensemble_metrics:
        metrics_dict = ensemble_metrics.get('metrics', {})
        summary_rows.append({
            'classifier': 'Ensemble (Weighted)',
            'task': task,
            'n_features': 'N/A',
            'macro_f1': metrics_dict.get('macro_f1', 0.0),
            'accuracy': metrics_dict.get('accuracy', 0.0),
            'macro_precision': metrics_dict.get('macro_precision', 0.0),
            'macro_recall': metrics_dict.get('macro_recall', 0.0),
            'weighted_f1': metrics_dict.get('weighted_f1', 0.0),
        })
        print(f"  ✓ Added ensemble results for {task}")
    else:
        print(f"  ⚠ Ensemble results for {task} not found")

# Create summary DataFrame
df_summary = pd.DataFrame(summary_rows)

if len(df_summary) == 0:
    print("  ⚠ No results found for summary table")
else:
    # Remove duplicates
    df_summary = df_summary.drop_duplicates(
        subset=['classifier', 'task'],
        keep='first'
    )

    # ========================================================================
    # STEP 6.4: Detailed Summary Table (All Metrics)
    # ========================================================================
    print("\n" + "-"*80)
    print("STEP 6.4: Detailed Summary Table (All Metrics)")
    print("-"*80)
    
    # Display detailed table
    print("\nDetailed Summary Table:")
    try:
        from IPython.display import display
        display(df_summary.style.format({
            'macro_f1': '{:.4f}',
            'accuracy': '{:.4f}',
            'macro_precision': '{:.4f}',
            'macro_recall': '{:.4f}',
            'weighted_f1': '{:.4f}',
            'n_features': '{:.0f}' if df_summary['n_features'].dtype in [int, float] else '{}'
        }))
    except:
        print(df_summary.to_string())
    
    # Save detailed table
    detailed_path = tables_dir / 'summary_detailed.csv'
    df_summary.to_csv(detailed_path, index=False)
    print(f"\n  ✓ Saved detailed table: {detailed_path.name}")
    
    # Save HTML version
    html_detailed_path = tables_dir / 'summary_detailed.html'
    df_summary.to_html(html_detailed_path, index=False, float_format='{:.4f}'.format)
    print(f"  ✓ Saved HTML: {html_detailed_path.name}")

    # ========================================================================
    # STEP 6.5: Pivot Table (Classifier × Task) - Macro F1
    # ========================================================================
    print("\n" + "-"*80)
    print("STEP 6.5: Pivot Table - Classifier × Task (Macro F1)")
    print("-"*80)
    
    # Create pivot table
    df_pivot = df_summary.pivot(
        index='classifier',
        columns='task',
        values='macro_f1'
    )
    
    # Reorder columns: clarity first, then hierarchical
    available_tasks = [t for t in TASK_ORDER if t in df_pivot.columns]
    remaining_tasks = sorted([t for t in df_pivot.columns if t not in available_tasks])
    column_order = available_tasks + remaining_tasks
    df_pivot = df_pivot[column_order]
    
    # Display pivot table
    print("\nPivot Table (Macro F1):")
    try:
        display(df_pivot.style.format(precision=4))
    except:
        print(df_pivot.to_string())
    
    # Save pivot table
    pivot_path = tables_dir / 'summary_pivot_classifier_wise.csv'
    df_pivot.to_csv(pivot_path)
    print(f"\n  ✓ Saved pivot table: {pivot_path.name}")
    
    # Save HTML version
    html_pivot_path = tables_dir / 'summary_pivot_classifier_wise.html'
    df_pivot.to_html(html_pivot_path, float_format='{:.4f}'.format)
    print(f"  ✓ Saved HTML: {html_pivot_path.name}")

    # ========================================================================
    # STEP 6.6: Pivot Table (Classifier × Task) - Accuracy
    # ========================================================================
    print("\n" + "-"*80)
    print("STEP 6.6: Pivot Table - Classifier × Task (Accuracy)")
    print("-"*80)
    
    df_pivot_acc = df_summary.pivot(
        index='classifier',
        columns='task',
        values='accuracy'
    )
    
    df_pivot_acc = df_pivot_acc[column_order]
    
    print("\nPivot Table (Accuracy):")
    try:
        display(df_pivot_acc.style.format(precision=4))
    except:
        print(df_pivot_acc.to_string())
    
    pivot_acc_path = tables_dir / 'summary_pivot_accuracy.csv'
    df_pivot_acc.to_csv(pivot_acc_path)
    print(f"\n  ✓ Saved pivot table: {pivot_acc_path.name}")

    # ========================================================================
    # STEP 6.7: Summary by Task (Individual Classifiers Only)
    # ========================================================================
    print("\n" + "-"*80)
    print("STEP 6.7: Summary by Task (Individual Classifiers)")
    print("-"*80)
    
    for task in TASK_ORDER:
        df_task = df_summary[
            (df_summary['task'] == task) & 
            (df_summary['classifier'] != 'Ensemble (Weighted)')
        ].copy()
        
        if len(df_task) == 0:
            continue
        
        print(f"\n  {task.upper()} - Individual Classifiers:")
        print(f"  {'-'*60}")
        
        # Sort by macro_f1 descending
        df_task = df_task.sort_values('macro_f1', ascending=False)
        
        # Display
        try:
            display(df_task[['classifier', 'n_features', 'macro_f1', 'accuracy', 'macro_precision', 'macro_recall']].style.format({
                'macro_f1': '{:.4f}',
                'accuracy': '{:.4f}',
                'macro_precision': '{:.4f}',
                'macro_recall': '{:.4f}',
                'n_features': '{:.0f}' if df_task['n_features'].dtype in [int, float] else '{}'
            }))
        except:
            print(df_task[['classifier', 'n_features', 'macro_f1', 'accuracy', 'macro_precision', 'macro_recall']].to_string())
        
        # Save per-task summary
        task_path = tables_dir / f'summary_{task}_individual.csv'
        df_task.to_csv(task_path, index=False)
        print(f"  ✓ Saved: {task_path.name}")

    # ========================================================================
    # STEP 6.8: Ensemble Comparison Table
    # ========================================================================
    print("\n" + "-"*80)
    print("STEP 6.8: Ensemble vs Best Individual Classifier")
    print("-"*80)
    
    ensemble_comparison_rows = []
    
    for task in TASK_ORDER:
        # Get best individual classifier
        df_task_individual = df_summary[
            (df_summary['task'] == task) & 
            (df_summary['classifier'] != 'Ensemble (Weighted)')
        ]
        
        if len(df_task_individual) == 0:
            continue
        
        best_individual = df_task_individual.loc[df_task_individual['macro_f1'].idxmax()]
        
        # Get ensemble result
        df_task_ensemble = df_summary[
            (df_summary['task'] == task) & 
            (df_summary['classifier'] == 'Ensemble (Weighted)')
        ]
        
        if len(df_task_ensemble) == 0:
            continue
        
        ensemble_result = df_task_ensemble.iloc[0]
        
        ensemble_comparison_rows.append({
            'task': task,
            'best_classifier': best_individual['classifier'],
            'best_macro_f1': best_individual['macro_f1'],
            'best_n_features': best_individual['n_features'],
            'ensemble_macro_f1': ensemble_result['macro_f1'],
            'improvement': ensemble_result['macro_f1'] - best_individual['macro_f1'],
            'ensemble_accuracy': ensemble_result['accuracy'],
            'best_accuracy': best_individual['accuracy'],
        })
    
    if ensemble_comparison_rows:
        df_ensemble_comparison = pd.DataFrame(ensemble_comparison_rows)
        
        print("\nEnsemble vs Best Individual:")
        try:
            display(df_ensemble_comparison.style.format({
                'best_macro_f1': '{:.4f}',
                'ensemble_macro_f1': '{:.4f}',
                'improvement': '{:.4f}',
                'ensemble_accuracy': '{:.4f}',
                'best_accuracy': '{:.4f}',
                'best_n_features': '{:.0f}' if df_ensemble_comparison['best_n_features'].dtype in [int, float] else '{}'
            }))
        except:
            print(df_ensemble_comparison.to_string())
        
        comparison_path = tables_dir / 'ensemble_comparison.csv'
        df_ensemble_comparison.to_csv(comparison_path, index=False)
        print(f"\n  ✓ Saved: {comparison_path.name}")

print("\n" + "="*80)
print("SUMMARY REPORT TABLES COMPLETE")
print("="*80)
print(f"\nAll tables saved to: {tables_dir}")
print(f"  - Detailed summary: summary_detailed.csv")
print(f"  - Pivot (Macro F1): summary_pivot_classifier_wise.csv")
print(f"  - Pivot (Accuracy): summary_pivot_accuracy.csv")
print(f"  - Per-task summaries: summary_{{task}}_individual.csv")
print(f"  - Ensemble comparison: ensemble_comparison.csv")