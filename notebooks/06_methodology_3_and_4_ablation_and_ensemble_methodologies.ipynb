{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y5NSFY0Joi8Z",
    "outputId": "7118911c-22f4-4105-e2ae-fa1ebeec988a"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Cloning into 'semeval-context-tree-modular'...\n",
      "remote: Enumerating objects: 859, done.\u001b[K\n",
      "remote: Counting objects: 100% (101/101), done.\u001b[K\n",
      "remote: Compressing objects: 100% (80/80), done.\u001b[K\n",
      "remote: Total 859 (delta 45), reused 52 (delta 21), pack-reused 758 (from 2)\u001b[K\n",
      "Receiving objects: 100% (859/859), 66.41 MiB | 13.05 MiB/s, done.\n",
      "Resolving deltas: 100% (525/525), done.\n",
      "Already up to date.\n",
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
      "Setup complete\n",
      "  Repository: /content/semeval-context-tree-modular\n",
      "  Data storage: /content/drive/MyDrive/semeval_data\n",
      "\n",
      "CRITICAL: Test sets will be loaded per-task (task-specific splits)\n",
      "         Clarity and Evasion have different test splits due to majority voting\n",
      "         These sets have NEVER been used for training or development!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# SETUP: Repository Clone, Drive Mount, and Path Configuration\n",
    "# ============================================================================\n",
    "# This cell performs minimal setup required for the notebook to run:\n",
    "# 1. Clones repository from GitHub (if not already present)\n",
    "# 2. Mounts Google Drive for persistent data storage\n",
    "# 3. Configures Python paths and initializes StorageManager\n",
    "# 4. Loads test split (ONLY accessed in this notebook)\n",
    "\n",
    "import shutil\n",
    "import os\n",
    "import subprocess\n",
    "import time\n",
    "import requests\n",
    "import zipfile\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from google.colab import drive\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "!rm -rf /content/semeval-context-tree-modular\n",
    "!git clone https://github.com/EonTechie/semeval-context-tree-modular.git\n",
    "!cd /content/semeval-context-tree-modular && git pull\n",
    "# Repository configuration\n",
    "repo_dir = '/content/semeval-context-tree-modular'\n",
    "repo_url = 'https://github.com/EonTechie/semeval-context-tree-modular.git'\n",
    "zip_url = 'https://github.com/EonTechie/semeval-context-tree-modular/archive/refs/heads/main.zip'\n",
    "\n",
    "# Clone repository (if not already present)\n",
    "if not os.path.exists(repo_dir):\n",
    "    print(\"Cloning repository from GitHub...\")\n",
    "    max_retries = 2\n",
    "    clone_success = False\n",
    "\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            result = subprocess.run(\n",
    "                ['git', 'clone', repo_url],\n",
    "                cwd='/content',\n",
    "                capture_output=True,\n",
    "                text=True,\n",
    "                timeout=60\n",
    "            )\n",
    "            if result.returncode == 0:\n",
    "                print(\"Repository cloned successfully via git\")\n",
    "                clone_success = True\n",
    "                break\n",
    "            else:\n",
    "                if attempt < max_retries - 1:\n",
    "                    time.sleep(3)\n",
    "        except Exception as e:\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(3)\n",
    "\n",
    "    # Fallback: Download as ZIP if git clone fails\n",
    "    if not clone_success:\n",
    "        print(\"Git clone failed. Downloading repository as ZIP archive...\")\n",
    "        zip_path = '/tmp/repo.zip'\n",
    "        try:\n",
    "            response = requests.get(zip_url, stream=True, timeout=60)\n",
    "            response.raise_for_status()\n",
    "            with open(zip_path, 'wb') as f:\n",
    "                for chunk in response.iter_content(chunk_size=8192):\n",
    "                    f.write(chunk)\n",
    "            with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "                zip_ref.extractall('/content')\n",
    "            extracted_dir = '/content/semeval-context-tree-modular-main'\n",
    "            if os.path.exists(extracted_dir):\n",
    "                os.rename(extracted_dir, repo_dir)\n",
    "            os.remove(zip_path)\n",
    "            print(\"Repository downloaded and extracted successfully\")\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to obtain repository: {e}\")\n",
    "\n",
    "# Mount Google Drive (if not already mounted)\n",
    "try:\n",
    "    drive.mount('/content/drive', force_remount=False)\n",
    "except Exception:\n",
    "    pass  # Already mounted\n",
    "\n",
    "# Configure paths\n",
    "BASE_PATH = Path('/content/semeval-context-tree-modular')\n",
    "DATA_PATH = Path('/content/drive/MyDrive/semeval_data')\n",
    "\n",
    "# Verify repository structure exists\n",
    "if not BASE_PATH.exists():\n",
    "    raise RuntimeError(f\"Repository directory not found: {BASE_PATH}\")\n",
    "if not (BASE_PATH / 'src').exists():\n",
    "    raise RuntimeError(f\"src directory not found in repository: {BASE_PATH / 'src'}\")\n",
    "if not (BASE_PATH / 'src' / 'storage' / 'manager.py').exists():\n",
    "    raise RuntimeError(f\"Required file not found: {BASE_PATH / 'src' / 'storage' / 'manager.py'}\")\n",
    "\n",
    "# Add repository to Python path\n",
    "sys.path.insert(0, str(BASE_PATH))\n",
    "\n",
    "# Verify imports work\n",
    "try:\n",
    "    from src.storage.manager import StorageManager\n",
    "    from src.features.extraction import featurize_hf_dataset_in_batches_v2\n",
    "    from src.models.classifiers import get_classifier_dict\n",
    "    from src.evaluation.metrics import compute_all_metrics, print_classification_report\n",
    "    from src.evaluation.tables import print_results_table\n",
    "    from src.evaluation.visualizer import visualize_all_evaluation\n",
    "except ImportError as e:\n",
    "    raise ImportError(\n",
    "        f\"Failed to import required modules. \"\n",
    "        f\"Repository path: {BASE_PATH}, \"\n",
    "        f\"Python path: {sys.path[:3]}, \"\n",
    "        f\"Error: {e}\"\n",
    "    )\n",
    "\n",
    "# Initialize StorageManager\n",
    "storage = StorageManager(\n",
    "    base_path=str(BASE_PATH),\n",
    "    data_path=str(DATA_PATH),\n",
    "    github_path=str(BASE_PATH)\n",
    ")\n",
    "\n",
    "# Test splits will be loaded per-task in the evaluation loop\n",
    "# Clarity and Evasion have different test splits (Evasion uses majority voting)\n",
    "\n",
    "print(\"Setup complete\")\n",
    "print(f\"  Repository: {BASE_PATH}\")\n",
    "print(f\"  Data storage: {DATA_PATH}\")\n",
    "print(f\"\\nCRITICAL: Test sets will be loaded per-task (task-specific splits)\")\n",
    "print(\"         Clarity and Evasion have different test splits due to majority voting\")\n",
    "print(\"         These sets have NEVER been used for training or development!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# STEP 1\n",
    "# ==============\n",
    "# ============================================================================\n",
    "# SETUP: Repository Clone, Drive Mount, and Path Configuration\n",
    "# ============================================================================\n",
    "import shutil\n",
    "import os\n",
    "import subprocess\n",
    "import time\n",
    "import requests\n",
    "import zipfile\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from google.colab import drive\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Repository configuration\n",
    "repo_dir = '/content/semeval-context-tree-modular'\n",
    "repo_url = 'https://github.com/EonTechie/semeval-context-tree-modular.git'\n",
    "zip_url = 'https://github.com/EonTechie/semeval-context-tree-modular/archive/refs/heads/main.zip'\n",
    "\n",
    "# Clone repository (if not already present)\n",
    "if not os.path.exists(repo_dir):\n",
    "    print(\"Cloning repository from GitHub...\")\n",
    "    max_retries = 2\n",
    "    clone_success = False\n",
    "\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            result = subprocess.run(\n",
    "                ['git', 'clone', repo_url],\n",
    "                cwd='/content',\n",
    "                capture_output=True,\n",
    "                text=True,\n",
    "                timeout=60\n",
    "            )\n",
    "            if result.returncode == 0:\n",
    "                print(\"Repository cloned successfully via git\")\n",
    "                clone_success = True\n",
    "                break\n",
    "            else:\n",
    "                if attempt < max_retries - 1:\n",
    "                    time.sleep(3)\n",
    "        except Exception as e:\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(3)\n",
    "\n",
    "    # Fallback: Download as ZIP if git clone fails\n",
    "    if not clone_success:\n",
    "        print(\"Git clone failed. Downloading repository as ZIP archive...\")\n",
    "        zip_path = '/tmp/repo.zip'\n",
    "        try:\n",
    "            response = requests.get(zip_url, stream=True, timeout=60)\n",
    "            response.raise_for_status()\n",
    "            with open(zip_path, 'wb') as f:\n",
    "                for chunk in response.iter_content(chunk_size=8192):\n",
    "                    f.write(chunk)\n",
    "            with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "                zip_ref.extractall('/content')\n",
    "            extracted_dir = '/content/semeval-context-tree-modular-main'\n",
    "            if os.path.exists(extracted_dir):\n",
    "                os.rename(extracted_dir, repo_dir)\n",
    "            os.remove(zip_path)\n",
    "            print(\"Repository downloaded and extracted successfully\")\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to obtain repository: {e}\")\n",
    "\n",
    "# Mount Google Drive (if not already mounted)\n",
    "try:\n",
    "    drive.mount('/content/drive', force_remount=False)\n",
    "except Exception:\n",
    "    pass  # Already mounted\n",
    "\n",
    "# Configure paths\n",
    "BASE_PATH = Path('/content/semeval-context-tree-modular')\n",
    "DATA_PATH = Path('/content/drive/MyDrive/semeval_data')\n",
    "\n",
    "# Verify repository structure exists\n",
    "if not BASE_PATH.exists():\n",
    "    raise RuntimeError(f\"Repository directory not found: {BASE_PATH}\")\n",
    "if not (BASE_PATH / 'src').exists():\n",
    "    raise RuntimeError(f\"src directory not found in repository: {BASE_PATH / 'src'}\")\n",
    "if not (BASE_PATH / 'src' / 'storage' / 'manager.py').exists():\n",
    "    raise RuntimeError(f\"Required file not found: {BASE_PATH / 'src' / 'storage' / 'manager.py'}\")\n",
    "\n",
    "# Add repository to Python path\n",
    "sys.path.insert(0, str(BASE_PATH))\n",
    "\n",
    "# Verify imports work\n",
    "try:\n",
    "    from src.storage.manager import StorageManager\n",
    "    from src.models.classifiers import get_classifier_dict\n",
    "    from src.features.extraction import get_feature_names\n",
    "    from sklearn.metrics import f1_score\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "    from sklearn.base import clone\n",
    "except ImportError as e:\n",
    "    raise ImportError(\n",
    "        f\"Failed to import required modules. \"\n",
    "        f\"Repository path: {BASE_PATH}, \"\n",
    "        f\"Python path: {sys.path[:3]}, \"\n",
    "        f\"Error: {e}\"\n",
    "    )\n",
    "\n",
    "# Initialize StorageManager\n",
    "storage = StorageManager(\n",
    "    base_path=str(BASE_PATH),\n",
    "    data_path=str(DATA_PATH),\n",
    "    github_path=str(BASE_PATH)\n",
    ")\n",
    "\n",
    "# Create ablation results directory\n",
    "ablation_dir = DATA_PATH / 'results' / 'FinalResultsType2' / 'ablation'\n",
    "ablation_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Setup complete\")\n",
    "print(f\"  Repository: {BASE_PATH}\")\n",
    "print(f\"  Data storage: {DATA_PATH}\")\n",
    "print(f\"  Ablation results: {ablation_dir}\")\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4W8HzLYwrlVe",
    "outputId": "ec3e4f0a-b4b0-49ea-f0d7-4ffb43faeca6"
   },
   "execution_count": 12,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
      "Setup complete\n",
      "  Repository: /content/semeval-context-tree-modular\n",
      "  Data storage: /content/drive/MyDrive/semeval_data\n",
      "  Ablation results: /content/drive/MyDrive/semeval_data/results/FinalResultsType2/ablation\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# ============================================================================\n",
    "# STEP 2\n",
    "# ==============\n",
    "# ============================================================================\n",
    "# REPRODUCIBILITY SETUP: Set Random Seeds for All Libraries\n",
    "# ============================================================================\n",
    "from src.utils.reproducibility import set_all_seeds\n",
    "\n",
    "# Set all random seeds to 42 for full reproducibility\n",
    "# deterministic=True ensures PyTorch operations are deterministic (slower but fully reproducible)\n",
    "set_all_seeds(seed=42, deterministic=True)\n",
    "\n",
    "print(\"✓ Reproducibility configured: All random seeds set to 42\")\n",
    "print(\"✓ PyTorch deterministic mode enabled\")\n",
    "print(\"\\nNOTE: If you encounter performance issues or non-deterministic behavior,\")\n",
    "print(\"      you can set deterministic=False in set_all_seeds() call above.\")\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c7HXP9RGrnp4",
    "outputId": "24832091-e6e2-4982-dc06-5accd48e8229"
   },
   "execution_count": 13,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "✓ Reproducibility seeds set to 42\n",
      "✓ PyTorch deterministic mode enabled (may be slower)\n",
      "✓ Reproducibility configured: All random seeds set to 42\n",
      "✓ PyTorch deterministic mode enabled\n",
      "\n",
      "NOTE: If you encounter performance issues or non-deterministic behavior,\n",
      "      you can set deterministic=False in set_all_seeds() call above.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "# ============================================================================\n",
    "# STEP 3\n",
    "# ==============\n",
    "# ============================================================================\n",
    "# CONFIGURE MODELS, TASKS, AND CLASSIFIERS\n",
    "# ============================================================================\n",
    "# Check if get_classifier_dict is imported (from Cell 1 - Setup)\n",
    "if 'get_classifier_dict' not in globals():\n",
    "    raise NameError(\n",
    "        \"get_classifier_dict not found. Please run Cell 1 (Setup) first.\\n\"\n",
    "        \"Cell 1 imports get_classifier_dict from src.models.classifiers.\"\n",
    "    )\n",
    "\n",
    "MODELS = ['bert', 'bert_political', 'bert_ambiguity', 'roberta', 'deberta', 'xlnet']\n",
    "# NOTE: Only clarity and hierarchical_evasion_to_clarity for greedy selection\n",
    "# 'evasion' task is NOT included (it's only used for training in 3. notebook)\n",
    "# Best classifier selection happens in 4. notebook, not here\n",
    "TASKS = ['clarity', 'hierarchical_evasion_to_clarity']  # 2 tasks for greedy selection\n",
    "\n",
    "# Label mappings for each task\n",
    "CLARITY_LABELS = ['Ambivalent', 'Clear Non-Reply', 'Clear Reply']\n",
    "EVASION_LABELS = ['Claims ignorance', 'Clarification', 'Declining to answer',\n",
    "                  'Deflection', 'Dodging', 'Explicit',\n",
    "                  'General', 'Implicit', 'Partial/half-answer']\n",
    "\n",
    "# Initialize classifiers with fixed random seed for reproducibility\n",
    "# Includes MLP (Multi-Layer Perceptron) as requested\n",
    "classifiers = get_classifier_dict(random_state=42)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"CONFIGURATION\")\n",
    "print(\"=\"*80)\n",
    "print(f\"  Models: {len(MODELS)} models\")\n",
    "print(f\"    {MODELS}\")\n",
    "print(f\"  Tasks: {len(TASKS)} tasks\")\n",
    "print(f\"    {TASKS}\")\n",
    "print(f\"  Classifiers: {len(classifiers)} classifiers\")\n",
    "print(f\"    {list(classifiers.keys())}\")\n",
    "print(f\"  Total combinations per task: {len(MODELS)} × {len(classifiers)} = {len(MODELS) * len(classifiers)}\")\n",
    "print(f\"  Evaluation set: Dev set (not test)\")\n",
    "print(\"=\"*80)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zr9cUtLHrnsP",
    "outputId": "d96205f4-a41a-4a25-e513-5aaf69646f1e"
   },
   "execution_count": 14,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "================================================================================\n",
      "CONFIGURATION\n",
      "================================================================================\n",
      "  Models: 6 models\n",
      "    ['bert', 'bert_political', 'bert_ambiguity', 'roberta', 'deberta', 'xlnet']\n",
      "  Tasks: 2 tasks\n",
      "    ['clarity', 'hierarchical_evasion_to_clarity']\n",
      "  Classifiers: 6 classifiers\n",
      "    ['LogisticRegression', 'LinearSVC', 'RandomForest', 'MLP', 'XGBoost', 'LightGBM']\n",
      "  Total combinations per task: 6 × 6 = 36\n",
      "  Evaluation set: Dev set (not test)\n",
      "================================================================================\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# ============================================================================\n",
    "# STEP 4\n",
    "# ==============\n",
    "# ============================================================================\n",
    "# STEP 1-3: 60 Feature Names, Load Features, Global Top 20 Selection\n",
    "# ============================================================================\n",
    "# This cell:\n",
    "# - Creates 60 feature names (18 model-independent + 42 model-dependent)\n",
    "# - Loads 60 features for train/dev/test\n",
    "# - Performs single-feature ablation on 60 features\n",
    "# - Selects global top 20 features per task\n",
    "\n",
    "import numpy as np\n",
    "import json\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.base import clone\n",
    "from tqdm import tqdm\n",
    "\n",
    "from src.features.extraction import (\n",
    "    get_model_independent_feature_names,\n",
    "    get_model_dependent_feature_names\n",
    ")\n",
    "from src.models.classifiers import get_classifier_dict\n",
    "from src.evaluation.metrics import compute_all_metrics\n",
    "\n",
    "# Check if required variables exist\n",
    "if 'storage' not in globals():\n",
    "    raise NameError(\"storage not found. Please run Cell 1 (Setup) first.\")\n",
    "\n",
    "if 'CLARITY_LABELS' not in globals() or 'EVASION_LABELS' not in globals():\n",
    "    raise NameError(\"CLARITY_LABELS and EVASION_LABELS not found. Please run Cell 3 (Configuration) first.\")\n",
    "\n",
    "# ========================================================================\n",
    "# CHECKPOINT DIRECTORY SETUP\n",
    "# ========================================================================\n",
    "results_dir_type2 = storage.data_path / 'results/FinalResultsType2/classifier_specific'\n",
    "checkpoint_dir = results_dir_type2 / 'checkpoint'\n",
    "ablation_60_dir = results_dir_type2\n",
    "\n",
    "checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "ablation_60_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ========================================================================\n",
    "# HELPER FUNCTIONS FOR CHECKPOINT\n",
    "# ========================================================================\n",
    "def load_checkpoint(filepath):\n",
    "    \"\"\"Load checkpoint file if exists\"\"\"\n",
    "    if filepath.exists():\n",
    "        try:\n",
    "            if filepath.suffix == '.pkl':\n",
    "                with open(filepath, 'rb') as f:\n",
    "                    return pickle.load(f)\n",
    "            elif filepath.suffix == '.json':\n",
    "                with open(filepath, 'r') as f:\n",
    "                    return json.load(f)\n",
    "            elif filepath.suffix == '.csv':\n",
    "                return pd.read_csv(filepath)\n",
    "            elif filepath.suffix == '.npy':\n",
    "                return np.load(filepath)\n",
    "        except Exception as e:\n",
    "            print(f\"     Warning: Could not load {filepath.name}: {e}\")\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "def save_checkpoint(data, filepath):\n",
    "    \"\"\"Save checkpoint file\"\"\"\n",
    "    filepath.parent.mkdir(parents=True, exist_ok=True)\n",
    "    if filepath.suffix == '.pkl':\n",
    "        with open(filepath, 'wb') as f:\n",
    "            pickle.dump(data, f)\n",
    "    elif filepath.suffix == '.json':\n",
    "        with open(filepath, 'w') as f:\n",
    "            json.dump(data, f, indent=2)\n",
    "    elif filepath.suffix == '.csv':\n",
    "        data.to_csv(filepath, index=False)\n",
    "    elif filepath.suffix == '.npy':\n",
    "        np.save(filepath, data)\n",
    "\n",
    "# ========================================================================\n",
    "# STEP 1: Create 60 Feature Names\n",
    "# ========================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 1: CREATE 60 FEATURE NAMES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Checkpoint for feature names\n",
    "fused_feature_names_60_path = checkpoint_dir / 'fused_feature_names_60.json'\n",
    "feature_name_to_idx_60_path = checkpoint_dir / 'feature_name_to_idx_60.json'\n",
    "\n",
    "fused_feature_names_60 = load_checkpoint(fused_feature_names_60_path)\n",
    "feature_name_to_idx_60 = load_checkpoint(feature_name_to_idx_60_path)\n",
    "\n",
    "if fused_feature_names_60 is None:\n",
    "    # Get base feature names\n",
    "    indep_feature_names = get_model_independent_feature_names()  # 18 features\n",
    "    dep_feature_names = get_model_dependent_feature_names()  # 7 features\n",
    "\n",
    "    # Create 60 feature names\n",
    "    fused_feature_names_60 = indep_feature_names.copy()\n",
    "    MODELS_60 = ['bert', 'bert_political', 'bert_ambiguity', 'roberta', 'deberta', 'xlnet']\n",
    "    for model in MODELS_60:\n",
    "        for dep_name in dep_feature_names:\n",
    "            fused_feature_names_60.append(f\"{model}_{dep_name}\")\n",
    "\n",
    "    feature_name_to_idx_60 = {name: idx for idx, name in enumerate(fused_feature_names_60)}\n",
    "\n",
    "    save_checkpoint(fused_feature_names_60, fused_feature_names_60_path)\n",
    "    save_checkpoint(feature_name_to_idx_60, feature_name_to_idx_60_path)\n",
    "    print(f\"✓ Created and saved 60 feature names\")\n",
    "else:\n",
    "    print(f\"✓ Loaded 60 feature names from checkpoint\")\n",
    "\n",
    "print(f\"  - Model-independent: 18 features\")\n",
    "print(f\"  - Model-dependent: 6 models × 7 features = 42 features\")\n",
    "print(f\"  - Total: {len(fused_feature_names_60)} features\")\n",
    "\n",
    "# ========================================================================\n",
    "# STEP 2: Load 60 Features for Train/Dev/Test\n",
    "# ========================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 2: LOAD 60 FEATURES (EARLY FUSION)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "TASKS_60 = ['clarity', 'hierarchical_evasion_to_clarity']\n",
    "features_60 = {}  # {task: {'train': X_train_60, 'dev': X_dev_60, 'test': X_test_60}}\n",
    "\n",
    "# Checkpoint for features\n",
    "features_60_path = checkpoint_dir / 'features_60.pkl'\n",
    "features_60 = load_checkpoint(features_60_path)\n",
    "\n",
    "if features_60 is None:\n",
    "    # ... (STEP 2 kodunun tamamı, satır 1078-1207 arası) ...\n",
    "    # (Bu kısmı aynen kopyalayın)\n",
    "\n",
    "    save_checkpoint(features_60, features_60_path)\n",
    "    print(f\"✓ Saved features_60 to checkpoint\")\n",
    "else:\n",
    "    print(f\"✓ Loaded features_60 from checkpoint\")\n",
    "\n",
    "# ========================================================================\n",
    "# STEP 3: Select Global Top 20 Features\n",
    "# ========================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 3: SELECT GLOBAL TOP 20 FEATURES (60 FEATURE'LI SİSTEM ÜZERİNDE)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Checkpoint paths\n",
    "df_ablation_60_path = ablation_60_dir / 'ablation_results_60_features.csv'\n",
    "df_stats_60_path = ablation_60_dir / 'feature_ranking_60_features.csv'\n",
    "global_top_20_dict_path = checkpoint_dir / 'global_top_20_dict.json'\n",
    "\n",
    "# Load checkpoints\n",
    "df_ablation_60 = load_checkpoint(df_ablation_60_path)\n",
    "df_stats_60 = load_checkpoint(df_stats_60_path)\n",
    "global_top_20_dict = load_checkpoint(global_top_20_dict_path)\n",
    "\n",
    "if df_ablation_60 is None or df_stats_60 is None or global_top_20_dict is None:\n",
    "    # ... (STEP 3 kodunun tamamı, satır 1225-1315 arası) ...\n",
    "    # (Bu kısmı aynen kopyalayın)\n",
    "\n",
    "    save_checkpoint(df_ablation_60, df_ablation_60_path)\n",
    "    save_checkpoint(df_stats_60, df_stats_60_path)\n",
    "    save_checkpoint(global_top_20_dict, global_top_20_dict_path)\n",
    "    print(f\"\\n✓ Saved STEP 3 results to checkpoint\")\n",
    "else:\n",
    "    print(f\"✓ Loaded STEP 3 results from checkpoint\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 1-3 COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Results saved to: {checkpoint_dir}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ESjoCwZdrnu7",
    "outputId": "c1b77e63-27f0-4b86-fbaa-e9a4467301cd"
   },
   "execution_count": 15,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 1: CREATE 60 FEATURE NAMES\n",
      "================================================================================\n",
      "✓ Loaded 60 feature names from checkpoint\n",
      "  - Model-independent: 18 features\n",
      "  - Model-dependent: 6 models × 7 features = 42 features\n",
      "  - Total: 60 features\n",
      "\n",
      "================================================================================\n",
      "STEP 2: LOAD 60 FEATURES (EARLY FUSION)\n",
      "================================================================================\n",
      "✓ Loaded features_60 from checkpoint\n",
      "\n",
      "================================================================================\n",
      "STEP 3: SELECT GLOBAL TOP 20 FEATURES (60 FEATURE'LI SİSTEM ÜZERİNDE)\n",
      "================================================================================\n",
      "✓ Loaded STEP 3 results from checkpoint\n",
      "\n",
      "================================================================================\n",
      "STEP 1-3 COMPLETE\n",
      "================================================================================\n",
      "Results saved to: /content/drive/MyDrive/semeval_data/results/FinalResultsType2/classifier_specific/checkpoint\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# ============================================================================\n",
    "# STEP 5\n",
    "# ==============\n",
    "# ============================================================================\n",
    "# STEP 4: Greedy Forward Selection (Per Classifier)\n",
    "# ============================================================================\n",
    "# This cell:\n",
    "# - Performs greedy forward selection for each classifier\n",
    "# - Starts with global top 20 features\n",
    "# - Adds up to 20 more features via greedy selection\n",
    "# - Trains final model on train+dev, evaluates on test\n",
    "# - CRITICAL FIX: Recomputes metrics if missing\n",
    "\n",
    "import numpy as np\n",
    "import json\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.base import clone\n",
    "from tqdm import tqdm\n",
    "\n",
    "from src.models.classifiers import get_classifier_dict\n",
    "from src.evaluation.metrics import compute_all_metrics\n",
    "\n",
    "# Check if required variables exist\n",
    "if 'storage' not in globals():\n",
    "    raise NameError(\"storage not found. Please run Cell 1 (Setup) first.\")\n",
    "\n",
    "if 'CLARITY_LABELS' not in globals() or 'EVASION_LABELS' not in globals():\n",
    "    raise NameError(\"CLARITY_LABELS and EVASION_LABELS not found. Please run Cell 3 (Configuration) first.\")\n",
    "\n",
    "# Check if Cell 4 results exist\n",
    "checkpoint_dir = storage.data_path / 'results/FinalResultsType2/classifier_specific/checkpoint'\n",
    "fused_feature_names_60_path = checkpoint_dir / 'fused_feature_names_60.json'\n",
    "features_60_path = checkpoint_dir / 'features_60.pkl'\n",
    "global_top_20_dict_path = checkpoint_dir / 'global_top_20_dict.json'\n",
    "\n",
    "if not fused_feature_names_60_path.exists():\n",
    "    raise FileNotFoundError(\"Cell 4 (STEP 1-3) must be run first. fused_feature_names_60.json not found.\")\n",
    "\n",
    "# Load from Cell 4\n",
    "def load_checkpoint(filepath):\n",
    "    \"\"\"Load checkpoint file if exists\"\"\"\n",
    "    if filepath.exists():\n",
    "        try:\n",
    "            if filepath.suffix == '.pkl':\n",
    "                with open(filepath, 'rb') as f:\n",
    "                    return pickle.load(f)\n",
    "            elif filepath.suffix == '.json':\n",
    "                with open(filepath, 'r') as f:\n",
    "                    return json.load(f)\n",
    "            elif filepath.suffix == '.csv':\n",
    "                return pd.read_csv(filepath)\n",
    "            elif filepath.suffix == '.npy':\n",
    "                return np.load(filepath)\n",
    "        except Exception as e:\n",
    "            print(f\"     Warning: Could not load {filepath.name}: {e}\")\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "def save_checkpoint(data, filepath):\n",
    "    \"\"\"Save checkpoint file\"\"\"\n",
    "    filepath.parent.mkdir(parents=True, exist_ok=True)\n",
    "    if filepath.suffix == '.pkl':\n",
    "        with open(filepath, 'wb') as f:\n",
    "            pickle.dump(data, f)\n",
    "    elif filepath.suffix == '.json':\n",
    "        with open(filepath, 'w') as f:\n",
    "            json.dump(data, f, indent=2)\n",
    "    elif filepath.suffix == '.csv':\n",
    "        data.to_csv(filepath, index=False)\n",
    "    elif filepath.suffix == '.npy':\n",
    "        np.save(filepath, data)\n",
    "\n",
    "# Load Cell 4 results\n",
    "fused_feature_names_60 = load_checkpoint(fused_feature_names_60_path)\n",
    "feature_name_to_idx_60 = load_checkpoint(checkpoint_dir / 'feature_name_to_idx_60.json')\n",
    "features_60 = load_checkpoint(features_60_path)\n",
    "global_top_20_dict = load_checkpoint(global_top_20_dict_path)\n",
    "\n",
    "if any(x is None for x in [fused_feature_names_60, feature_name_to_idx_60, features_60, global_top_20_dict]):\n",
    "    raise FileNotFoundError(\"Cell 4 (STEP 1-3) must be completed first. Missing checkpoint files.\")\n",
    "\n",
    "# Setup directories\n",
    "results_dir_type2 = storage.data_path / 'results/FinalResultsType2/classifier_specific'\n",
    "predictions_dir = results_dir_type2 / 'predictions'\n",
    "probabilities_dir = results_dir_type2 / 'probabilities'\n",
    "metrics_dir = results_dir_type2 / 'metrics'\n",
    "\n",
    "predictions_dir.mkdir(parents=True, exist_ok=True)\n",
    "probabilities_dir.mkdir(parents=True, exist_ok=True)\n",
    "metrics_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "TASKS_60 = ['clarity', 'hierarchical_evasion_to_clarity']\n",
    "\n",
    "# ... (greedy_forward_selection_60_with_checkpoint fonksiyonu, satır 1329-1400 arası) ...\n",
    "# (Bu fonksiyonu aynen kopyalayın)\n",
    "\n",
    "# ========================================================================\n",
    "# STEP 4: Greedy Forward Selection for Each Classifier\n",
    "# ========================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 4: GREEDY FORWARD SELECTION (PER CLASSIFIER)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Initialize classifier_specific_results\n",
    "classifier_specific_results = {}  # {task: {classifier: {features, metrics, predictions, probabilities}}}\n",
    "\n",
    "for task in TASKS_60:\n",
    "    print(f\"\\n{'-'*80}\")\n",
    "    print(f\"TASK: {task.upper()}\")\n",
    "    print(f\"{'-'*80}\")\n",
    "\n",
    "    if task not in classifier_specific_results:\n",
    "        classifier_specific_results[task] = {}\n",
    "\n",
    "    X_train_60 = features_60[task]['train']\n",
    "    X_dev_60 = features_60[task]['dev']\n",
    "    X_test_60 = features_60[task]['test']\n",
    "\n",
    "    split_task = 'evasion' if task == 'hierarchical_evasion_to_clarity' else task\n",
    "    label_key = 'clarity_label' if task == 'hierarchical_evasion_to_clarity' else ('clarity_label' if task == 'clarity' else 'evasion_label')\n",
    "    label_list = CLARITY_LABELS if 'clarity' in task else EVASION_LABELS\n",
    "\n",
    "    train_ds = storage.load_split('train', task=split_task)\n",
    "    dev_ds = storage.load_split('dev', task=split_task)\n",
    "    test_ds = storage.load_split('test', task=split_task)\n",
    "\n",
    "    y_train = np.array([train_ds[i][label_key] for i in range(len(train_ds))])\n",
    "    y_dev = np.array([dev_ds[i][label_key] for i in range(len(dev_ds))])\n",
    "    y_test = np.array([test_ds[i][label_key] for i in range(len(test_ds))])\n",
    "\n",
    "    global_top_20 = global_top_20_dict[task]\n",
    "    classifiers_60 = get_classifier_dict(random_state=42)\n",
    "\n",
    "    for clf_name, clf in classifiers_60.items():\n",
    "        print(f\"\\n  Classifier: {clf_name}\")\n",
    "        max_features = 25 if clf_name == \"LightGBM\" else 40\n",
    "\n",
    "        # Checkpoint paths for this classifier\n",
    "        selected_features_path = checkpoint_dir / f'selected_features_{clf_name}_{task}.json'\n",
    "        trajectory_path = checkpoint_dir / f'trajectory_{clf_name}_{task}.csv'\n",
    "        predictions_path = predictions_dir / f'{clf_name}_{task}_predictions.npy'\n",
    "        probabilities_path = probabilities_dir / f'{clf_name}_{task}_probabilities.npy'\n",
    "        metrics_path = checkpoint_dir / f'metrics_{clf_name}_{task}.json'\n",
    "        greedy_checkpoint_path = checkpoint_dir / f'greedy_checkpoint_{clf_name}_{task}.pkl'\n",
    "\n",
    "        # CRITICAL FIX: Check if predictions exist first\n",
    "        y_test_pred = load_checkpoint(predictions_path)\n",
    "\n",
    "# Cell 5'te (satır 650-703 arası) şu kısmı bul ve değiştir:\n",
    "\n",
    "        if y_test_pred is not None:\n",
    "            print(f\"    ✓ Found predictions for {clf_name}, loading from checkpoint...\")\n",
    "\n",
    "            # Load other data if available\n",
    "            selected_features = load_checkpoint(selected_features_path)\n",
    "            trajectory_data = load_checkpoint(trajectory_path)\n",
    "            y_test_proba = load_checkpoint(probabilities_path)\n",
    "            metrics = load_checkpoint(metrics_path)\n",
    "\n",
    "            if selected_features is None:\n",
    "                print(f\"     Note: selected_features not found, but predictions exist (from previous run)\")\n",
    "                selected_features = []\n",
    "\n",
    "            if isinstance(trajectory_data, pd.DataFrame):\n",
    "                trajectory = [(row['n_features'], row['macro_f1']) for _, row in trajectory_data.iterrows()]\n",
    "            else:\n",
    "                trajectory = trajectory_data if isinstance(trajectory_data, list) else []\n",
    "\n",
    "            # CRITICAL FIX: Get n_features from trajectory if selected_features missing\n",
    "            n_features = 0\n",
    "            if selected_features and len(selected_features) > 0:\n",
    "                n_features = len(selected_features)\n",
    "                print(f\"    ✓ {clf_name}: n_features={n_features} from selected_features\")\n",
    "            elif trajectory_data is not None:\n",
    "                if isinstance(trajectory_data, pd.DataFrame) and len(trajectory_data) > 0:\n",
    "                    n_features = int(trajectory_data.iloc[-1]['n_features'])\n",
    "                    print(f\"     {clf_name}: selected_features not found, using n_features={n_features} from trajectory\")\n",
    "                elif isinstance(trajectory_data, list) and len(trajectory_data) > 0:\n",
    "                    n_features = trajectory_data[-1][0]  # (n_features, macro_f1)\n",
    "                    print(f\"     {clf_name}: selected_features not found, using n_features={n_features} from trajectory\")\n",
    "                else:\n",
    "                    print(f\"     {clf_name}: Could not determine n_features from trajectory, defaulting to 0\")\n",
    "            else:\n",
    "                print(f\"     {clf_name}: Could not determine n_features (no selected_features or trajectory), defaulting to 0\")\n",
    "\n",
    "            # CRITICAL FIX: If metrics not found, recompute from predictions\n",
    "            if metrics is None or len(metrics) == 0:\n",
    "                print(f\"     Metrics not found, recomputing from predictions...\")\n",
    "                test_ds = storage.load_split('test', task=split_task)\n",
    "                y_test_true = np.array([test_ds[i][label_key] for i in range(len(test_ds))])\n",
    "\n",
    "                le = LabelEncoder()\n",
    "                y_test_true_encoded = le.fit_transform(y_test_true)\n",
    "                y_test_pred_encoded = le.transform(y_test_pred)\n",
    "\n",
    "                metrics = compute_all_metrics(\n",
    "                    y_test_true_encoded, y_test_pred_encoded, label_list,\n",
    "                    task_name=f\"TEST_{task}_{clf_name}_RECOMPUTED\"\n",
    "                )\n",
    "\n",
    "                metrics_serializable = {}\n",
    "                for k, v in metrics.items():\n",
    "                    if isinstance(v, (int, float, np.integer, np.floating)):\n",
    "                        metrics_serializable[k] = float(v)\n",
    "                    elif isinstance(v, (list, np.ndarray)):\n",
    "                        metrics_serializable[k] = v.tolist() if isinstance(v, np.ndarray) else v\n",
    "                    elif isinstance(v, dict):\n",
    "                        metrics_serializable[k] = v\n",
    "                    else:\n",
    "                        metrics_serializable[k] = str(v)\n",
    "                save_checkpoint(metrics_serializable, metrics_path)\n",
    "                print(f\"    ✓ Recomputed and saved metrics: Macro F1={metrics.get('macro_f1', 0.0):.4f}\")\n",
    "\n",
    "            classifier_specific_results[task][clf_name] = {\n",
    "                'selected_features': selected_features,\n",
    "                'n_features': n_features,  # ← DÜZELTİLDİ: trajectory'den gelecek\n",
    "                'metrics': metrics,\n",
    "                'predictions': y_test_pred,\n",
    "                'probabilities': y_test_proba,\n",
    "                'trajectory': trajectory\n",
    "            }\n",
    "\n",
    "            print(f\"    ✓ Loaded: predictions shape: {y_test_pred.shape}\")\n",
    "            if y_test_proba is not None:\n",
    "                print(f\"    ✓ Probabilities available\")\n",
    "            if metrics and len(metrics) > 0:\n",
    "                print(f\"    ✓ Metrics: Macro F1={metrics.get('macro_f1', 0.0):.4f}\")\n",
    "            if n_features > 0:\n",
    "                print(f\"    ✓ n_features: {n_features}\")\n",
    "            continue\n",
    "\n",
    "        # If checkpoint not found, run greedy selection\n",
    "        print(f\"    Running greedy selection (starting with global top 20, max 40 features)...\")\n",
    "        selected_features, trajectory = greedy_forward_selection_60_with_checkpoint(\n",
    "            X_train_60, X_dev_60, y_train, y_dev,\n",
    "            fused_feature_names_60, global_top_20, clf,\n",
    "            max_features=max_features,\n",
    "            checkpoint_path=greedy_checkpoint_path\n",
    "        )\n",
    "\n",
    "        final_f1 = trajectory[-1][1] if trajectory else 0.0\n",
    "        n_global = len(global_top_20)\n",
    "        n_greedy = len(selected_features) - n_global\n",
    "        print(f\"    ✓ Selected {len(selected_features)} features (Global: {n_global} + Greedy: {n_greedy}), Dev F1={final_f1:.4f}\")\n",
    "\n",
    "        save_checkpoint(selected_features, selected_features_path)\n",
    "        traj_df = pd.DataFrame(trajectory, columns=['n_features', 'macro_f1'])\n",
    "        save_checkpoint(traj_df, trajectory_path)\n",
    "        print(f\"    ✓ Saved greedy selection results to checkpoint\")\n",
    "\n",
    "        selected_indices = [feature_name_to_idx_60[name] for name in selected_features if name in feature_name_to_idx_60]\n",
    "        X_train_selected = X_train_60[:, selected_indices]\n",
    "        X_dev_selected = X_dev_60[:, selected_indices]\n",
    "        X_test_selected = X_test_60[:, selected_indices]\n",
    "\n",
    "        print(f\"    Training on Train+Dev combined ({X_train_selected.shape[0] + X_dev_selected.shape[0]} samples)...\")\n",
    "        X_train_combined = np.vstack([X_train_selected, X_dev_selected])\n",
    "        y_train_combined = np.concatenate([y_train, y_dev])\n",
    "\n",
    "        le = LabelEncoder()\n",
    "        y_train_combined_encoded = le.fit_transform(y_train_combined)\n",
    "        y_test_encoded = le.transform(y_test)\n",
    "\n",
    "        pipe = Pipeline([(\"scaler\", StandardScaler()), (\"clf\", clone(clf))])\n",
    "        pipe.fit(X_train_combined, y_train_combined_encoded)\n",
    "\n",
    "        y_test_pred_encoded = pipe.predict(X_test_selected)\n",
    "        y_test_pred = le.inverse_transform(y_test_pred_encoded)\n",
    "\n",
    "        y_test_proba = None\n",
    "        if hasattr(pipe.named_steps['clf'], 'predict_proba'):\n",
    "            try:\n",
    "                X_test_scaled = pipe.named_steps['scaler'].transform(X_test_selected)\n",
    "                y_test_proba = pipe.named_steps['clf'].predict_proba(X_test_scaled)\n",
    "            except Exception as e:\n",
    "                print(f\"       Warning: Could not get probabilities for {clf_name}: {e}\")\n",
    "\n",
    "        metrics = compute_all_metrics(\n",
    "            y_test_encoded, y_test_pred_encoded, label_list,\n",
    "            task_name=f\"TEST_{task}_{clf_name}\"\n",
    "        )\n",
    "\n",
    "        print(f\"    Test Macro F1: {metrics.get('macro_f1', 0.0):.4f}\")\n",
    "\n",
    "        save_checkpoint(y_test_pred, predictions_path)\n",
    "        if y_test_proba is not None:\n",
    "            save_checkpoint(y_test_proba, probabilities_path)\n",
    "            print(f\"    ✓ Saved probabilities: {probabilities_path}\")\n",
    "        else:\n",
    "            print(f\"     No probabilities available for {clf_name}\")\n",
    "\n",
    "        metrics_serializable = {}\n",
    "        for k, v in metrics.items():\n",
    "            if isinstance(v, (int, float, np.integer, np.floating)):\n",
    "                metrics_serializable[k] = float(v)\n",
    "            elif isinstance(v, (list, np.ndarray)):\n",
    "                metrics_serializable[k] = v.tolist() if isinstance(v, np.ndarray) else v\n",
    "            elif isinstance(v, dict):\n",
    "                metrics_serializable[k] = v\n",
    "            else:\n",
    "                metrics_serializable[k] = str(v)\n",
    "        save_checkpoint(metrics_serializable, metrics_path)\n",
    "\n",
    "        classifier_specific_results[task][clf_name] = {\n",
    "            'selected_features': selected_features,\n",
    "            'n_features': len(selected_features),\n",
    "            'metrics': metrics,\n",
    "            'predictions': y_test_pred,\n",
    "            'probabilities': y_test_proba,\n",
    "            'trajectory': trajectory\n",
    "        }\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"GREEDY FORWARD SELECTION COMPLETE\")\n",
    "print(\"=\"*80)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CsMnS2lsrnyC",
    "outputId": "ceed3671-5b45-4561-bb8d-09e7cb206bee"
   },
   "execution_count": 23,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 4: GREEDY FORWARD SELECTION (PER CLASSIFIER)\n",
      "================================================================================\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "TASK: CLARITY\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "  Classifier: LogisticRegression\n",
      "    ✓ Found predictions for LogisticRegression, loading from checkpoint...\n",
      "     Note: selected_features not found, but predictions exist (from previous run)\n",
      "     LogisticRegression: Could not determine n_features (no selected_features or trajectory), defaulting to 0\n",
      "    ✓ Loaded: predictions shape: (308,)\n",
      "    ✓ Probabilities available\n",
      "    ✓ Metrics: Macro F1=0.4571\n",
      "\n",
      "  Classifier: LinearSVC\n",
      "    ✓ Found predictions for LinearSVC, loading from checkpoint...\n",
      "     Note: selected_features not found, but predictions exist (from previous run)\n",
      "     LinearSVC: Could not determine n_features (no selected_features or trajectory), defaulting to 0\n",
      "    ✓ Loaded: predictions shape: (308,)\n",
      "    ✓ Metrics: Macro F1=0.4608\n",
      "\n",
      "  Classifier: RandomForest\n",
      "    ✓ Found predictions for RandomForest, loading from checkpoint...\n",
      "     Note: selected_features not found, but predictions exist (from previous run)\n",
      "     RandomForest: Could not determine n_features (no selected_features or trajectory), defaulting to 0\n",
      "    ✓ Loaded: predictions shape: (308,)\n",
      "    ✓ Probabilities available\n",
      "    ✓ Metrics: Macro F1=0.4032\n",
      "\n",
      "  Classifier: MLP\n",
      "    ✓ Found predictions for MLP, loading from checkpoint...\n",
      "     Note: selected_features not found, but predictions exist (from previous run)\n",
      "     MLP: Could not determine n_features (no selected_features or trajectory), defaulting to 0\n",
      "    ✓ Loaded: predictions shape: (308,)\n",
      "    ✓ Probabilities available\n",
      "    ✓ Metrics: Macro F1=0.3554\n",
      "\n",
      "  Classifier: XGBoost\n",
      "    ✓ Found predictions for XGBoost, loading from checkpoint...\n",
      "     Note: selected_features not found, but predictions exist (from previous run)\n",
      "     XGBoost: Could not determine n_features (no selected_features or trajectory), defaulting to 0\n",
      "    ✓ Loaded: predictions shape: (308,)\n",
      "    ✓ Probabilities available\n",
      "    ✓ Metrics: Macro F1=0.4006\n",
      "\n",
      "  Classifier: LightGBM\n",
      "    ✓ Found predictions for LightGBM, loading from checkpoint...\n",
      "     Note: selected_features not found, but predictions exist (from previous run)\n",
      "     LightGBM: Could not determine n_features (no selected_features or trajectory), defaulting to 0\n",
      "    ✓ Loaded: predictions shape: (308,)\n",
      "    ✓ Probabilities available\n",
      "    ✓ Metrics: Macro F1=0.4522\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "TASK: HIERARCHICAL_EVASION_TO_CLARITY\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "  Classifier: LogisticRegression\n",
      "    ✓ Found predictions for LogisticRegression, loading from checkpoint...\n",
      "     Note: selected_features not found, but predictions exist (from previous run)\n",
      "     LogisticRegression: Could not determine n_features (no selected_features or trajectory), defaulting to 0\n",
      "    ✓ Loaded: predictions shape: (275,)\n",
      "    ✓ Probabilities available\n",
      "    ✓ Metrics: Macro F1=0.4464\n",
      "\n",
      "  Classifier: LinearSVC\n",
      "    ✓ Found predictions for LinearSVC, loading from checkpoint...\n",
      "     Note: selected_features not found, but predictions exist (from previous run)\n",
      "     LinearSVC: Could not determine n_features (no selected_features or trajectory), defaulting to 0\n",
      "    ✓ Loaded: predictions shape: (275,)\n",
      "    ✓ Metrics: Macro F1=0.4597\n",
      "\n",
      "  Classifier: RandomForest\n",
      "    ✓ Found predictions for RandomForest, loading from checkpoint...\n",
      "     Note: selected_features not found, but predictions exist (from previous run)\n",
      "     RandomForest: Could not determine n_features (no selected_features or trajectory), defaulting to 0\n",
      "    ✓ Loaded: predictions shape: (275,)\n",
      "    ✓ Probabilities available\n",
      "    ✓ Metrics: Macro F1=0.3966\n",
      "\n",
      "  Classifier: MLP\n",
      "    ✓ Found predictions for MLP, loading from checkpoint...\n",
      "     Note: selected_features not found, but predictions exist (from previous run)\n",
      "     MLP: Could not determine n_features (no selected_features or trajectory), defaulting to 0\n",
      "    ✓ Loaded: predictions shape: (275,)\n",
      "    ✓ Probabilities available\n",
      "    ✓ Metrics: Macro F1=0.3492\n",
      "\n",
      "  Classifier: XGBoost\n",
      "    ✓ Found predictions for XGBoost, loading from checkpoint...\n",
      "     Note: selected_features not found, but predictions exist (from previous run)\n",
      "     XGBoost: Could not determine n_features (no selected_features or trajectory), defaulting to 0\n",
      "    ✓ Loaded: predictions shape: (275,)\n",
      "    ✓ Probabilities available\n",
      "    ✓ Metrics: Macro F1=0.3933\n",
      "\n",
      "  Classifier: LightGBM\n",
      "    ✓ Found predictions for LightGBM, loading from checkpoint...\n",
      "     Note: selected_features not found, but predictions exist (from previous run)\n",
      "     LightGBM: Could not determine n_features (no selected_features or trajectory), defaulting to 0\n",
      "    ✓ Loaded: predictions shape: (275,)\n",
      "    ✓ Probabilities available\n",
      "    ✓ Metrics: Macro F1=0.4517\n",
      "\n",
      "================================================================================\n",
      "GREEDY FORWARD SELECTION COMPLETE\n",
      "================================================================================\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# ============================================================================\n",
    "# STEP 6\n",
    "# ==============\n",
    "# ============================================================================\n",
    "# STEP 5-6: Weighted Average Ensemble + Summary Report Tables\n",
    "# ============================================================================\n",
    "# This cell:\n",
    "# - Creates weighted average ensemble from classifier probabilities\n",
    "# - Generates report tables\n",
    "# - CRITICAL FIX: Recomputes ensemble metrics if missing\n",
    "# - CRITICAL FIX: Gets n_features from trajectory if selected_features missing\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from src.evaluation.metrics import compute_all_metrics\n",
    "\n",
    "# Check if required variables exist\n",
    "if 'storage' not in globals():\n",
    "    raise NameError(\"storage not found. Please run Cell 1 (Setup) first.\")\n",
    "\n",
    "if 'CLARITY_LABELS' not in globals() or 'EVASION_LABELS' not in globals():\n",
    "    raise NameError(\"CLARITY_LABELS and EVASION_LABELS not found. Please run Cell 3 (Configuration) first.\")\n",
    "\n",
    "# Check if Cell 5 results exist\n",
    "checkpoint_dir = storage.data_path / 'results/FinalResultsType2/classifier_specific/checkpoint'\n",
    "predictions_dir = storage.data_path / 'results/FinalResultsType2/classifier_specific/predictions'\n",
    "probabilities_dir = storage.data_path / 'results/FinalResultsType2/classifier_specific/probabilities'\n",
    "metrics_dir = storage.data_path / 'results/FinalResultsType2/classifier_specific/metrics'\n",
    "tables_dir = storage.data_path / 'results/FinalResultsType2/classifier_specific/tables'\n",
    "\n",
    "tables_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Helper functions\n",
    "def load_checkpoint(filepath):\n",
    "    \"\"\"Load checkpoint file if exists\"\"\"\n",
    "    if filepath.exists():\n",
    "        try:\n",
    "            if filepath.suffix == '.pkl':\n",
    "                with open(filepath, 'rb') as f:\n",
    "                    return pickle.load(f)\n",
    "            elif filepath.suffix == '.json':\n",
    "                with open(filepath, 'r') as f:\n",
    "                    return json.load(f)\n",
    "            elif filepath.suffix == '.csv':\n",
    "                return pd.read_csv(filepath)\n",
    "            elif filepath.suffix == '.npy':\n",
    "                return np.load(filepath)\n",
    "        except Exception as e:\n",
    "            print(f\"     Warning: Could not load {filepath.name}: {e}\")\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "def save_checkpoint(data, filepath):\n",
    "    \"\"\"Save checkpoint file\"\"\"\n",
    "    filepath.parent.mkdir(parents=True, exist_ok=True)\n",
    "    if filepath.suffix == '.pkl':\n",
    "        with open(filepath, 'wb') as f:\n",
    "            pickle.dump(data, f)\n",
    "    elif filepath.suffix == '.json':\n",
    "        with open(filepath, 'w') as f:\n",
    "            json.dump(data, f, indent=2)\n",
    "    elif filepath.suffix == '.csv':\n",
    "        data.to_csv(filepath, index=False)\n",
    "    elif filepath.suffix == '.npy':\n",
    "        np.save(filepath, data)\n",
    "\n",
    "# ========================================================================\n",
    "# Load classifier results from Cell 5 (CRITICAL FIX: n_features from trajectory)\n",
    "# ========================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"LOADING CLASSIFIER RESULTS FROM CHECKPOINTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "\n",
    "classifier_specific_results = {}\n",
    "TASKS_60 = ['clarity', 'hierarchical_evasion_to_clarity']\n",
    "\n",
    "for task in TASKS_60:\n",
    "    classifier_specific_results[task] = {}\n",
    "    print(f\"\\nLoading results for task: {task}\")\n",
    "\n",
    "    for clf_name in ['LogisticRegression', 'LinearSVC', 'RandomForest', 'MLP', 'XGBoost', 'LightGBM']:\n",
    "        predictions_path = predictions_dir / f'{clf_name}_{task}_predictions.npy'\n",
    "        probabilities_path = probabilities_dir / f'{clf_name}_{task}_probabilities.npy'\n",
    "        metrics_path = checkpoint_dir / f'metrics_{clf_name}_{task}.json'\n",
    "        selected_features_path = checkpoint_dir / f'selected_features_{clf_name}_{task}.json'\n",
    "        trajectory_path = checkpoint_dir / f'trajectory_{clf_name}_{task}.csv'  # ← EKLENEN\n",
    "\n",
    "        y_test_pred = load_checkpoint(predictions_path)\n",
    "        if y_test_pred is not None:\n",
    "            y_test_proba = load_checkpoint(probabilities_path)\n",
    "            metrics = load_checkpoint(metrics_path)\n",
    "            selected_features = load_checkpoint(selected_features_path)\n",
    "            trajectory_data = load_checkpoint(trajectory_path)  # ← EKLENEN\n",
    "\n",
    "            # CRITICAL FIX: If selected_features is None/empty, try to get n_features from trajectory\n",
    "            n_features = 0\n",
    "            if selected_features and len(selected_features) > 0:\n",
    "                n_features = len(selected_features)\n",
    "                print(f\"  ✓ {clf_name}: n_features={n_features} from selected_features\")\n",
    "            elif trajectory_data is not None:\n",
    "                # Try to get n_features from trajectory (last row has final n_features)\n",
    "                if isinstance(trajectory_data, pd.DataFrame) and len(trajectory_data) > 0:\n",
    "                    n_features = int(trajectory_data.iloc[-1]['n_features'])\n",
    "                    print(f\"   {clf_name}: selected_features not found, using n_features={n_features} from trajectory\")\n",
    "                elif isinstance(trajectory_data, list) and len(trajectory_data) > 0:\n",
    "                    n_features = trajectory_data[-1][0]  # (n_features, macro_f1)\n",
    "                    print(f\"   {clf_name}: selected_features not found, using n_features={n_features} from trajectory\")\n",
    "                else:\n",
    "                    print(f\"   {clf_name}: Could not determine n_features from trajectory, defaulting to 0\")\n",
    "            else:\n",
    "                print(f\"   {clf_name}: Could not determine n_features (no selected_features or trajectory), defaulting to 0\")\n",
    "\n",
    "            classifier_specific_results[task][clf_name] = {\n",
    "                'selected_features': selected_features if selected_features else [],\n",
    "                'n_features': n_features,  # ← DÜZELTİLDİ\n",
    "                'metrics': metrics if metrics else {},\n",
    "                'predictions': y_test_pred,\n",
    "                'probabilities': y_test_proba,\n",
    "            }\n",
    "        else:\n",
    "            print(f\"  ✗ {clf_name}: No predictions found\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "# ========================================================================\n",
    "# STEP 5: Weighted Average Ensemble\n",
    "# ========================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 5: WEIGHTED AVERAGE ENSEMBLE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "ensemble_results = {}\n",
    "\n",
    "for task in TASKS_60:\n",
    "    print(f\"\\n{'-'*80}\")\n",
    "    print(f\"TASK: {task.upper()}\")\n",
    "    print(f\"{'-'*80}\")\n",
    "\n",
    "    ensemble_pred_path = predictions_dir / f'ensemble_hard_labels_from_weighted_proba_{task}.npy'\n",
    "    ensemble_proba_path = probabilities_dir / f'ensemble_weighted_average_probabilities_{task}.npy'\n",
    "    ensemble_weights_path = metrics_dir / f'ensemble_classifier_weights_{task}.json'\n",
    "    ensemble_metrics_path = metrics_dir / f'ensemble_evaluation_metrics_{task}.json'\n",
    "\n",
    "    ensemble_pred = load_checkpoint(ensemble_pred_path)\n",
    "    ensemble_proba = load_checkpoint(ensemble_proba_path)\n",
    "    ensemble_weights = load_checkpoint(ensemble_weights_path)\n",
    "\n",
    "    # CRITICAL FIX: Better error handling for JSON\n",
    "    ensemble_metrics = None\n",
    "    try:\n",
    "        ensemble_metrics = load_checkpoint(ensemble_metrics_path)\n",
    "    except Exception as e:\n",
    "        print(f\"   Warning: Could not load ensemble_evaluation_metrics_{task}.json: {e}\")\n",
    "        ensemble_metrics = None\n",
    "\n",
    "    if ensemble_pred is not None and ensemble_proba is not None:\n",
    "        print(f\"  ✓ Found ensemble checkpoint, loading...\")\n",
    "        ensemble_results[task] = {\n",
    "            'predictions': ensemble_pred,\n",
    "            'probabilities': ensemble_proba,\n",
    "            'weights': ensemble_weights if ensemble_weights else {},\n",
    "            'classifiers_used': ensemble_weights.get('classifiers', []) if ensemble_weights else []\n",
    "        }\n",
    "        print(f\"  ✓ Ensemble predictions shape: {ensemble_pred.shape}\")\n",
    "        if ensemble_metrics:\n",
    "            ensemble_f1 = ensemble_metrics.get('metrics', {}).get('macro_f1', 0.0)\n",
    "            print(f\"  ✓ Ensemble Test Macro F1: {ensemble_f1:.4f}\")\n",
    "        else:\n",
    "            # Recompute ensemble metrics if missing\n",
    "            print(f\"   Ensemble metrics missing, recomputing...\")\n",
    "            split_task = 'evasion' if task == 'hierarchical_evasion_to_clarity' else task\n",
    "            label_key = 'clarity_label' if task == 'hierarchical_evasion_to_clarity' else ('clarity_label' if task == 'clarity' else 'evasion_label')\n",
    "            label_list = CLARITY_LABELS if 'clarity' in task else EVASION_LABELS\n",
    "\n",
    "            test_ds = storage.load_split('test', task=split_task)\n",
    "            y_test_true = np.array([test_ds[i][label_key] for i in range(len(test_ds))])\n",
    "\n",
    "            le = LabelEncoder()\n",
    "            y_test_true_encoded = le.fit_transform(y_test_true)\n",
    "            ensemble_pred_encoded = le.transform(ensemble_pred)\n",
    "\n",
    "            ensemble_metrics_dict = compute_all_metrics(\n",
    "                y_test_true_encoded, ensemble_pred_encoded, label_list,\n",
    "                task_name=f\"ENSEMBLE_{task}_RECOMPUTED\"\n",
    "            )\n",
    "\n",
    "            metrics_serializable = {}\n",
    "            for k, v in ensemble_metrics_dict.items():\n",
    "                if isinstance(v, (int, float, np.integer, np.floating)):\n",
    "                    metrics_serializable[k] = float(v)\n",
    "                elif isinstance(v, (list, np.ndarray)):\n",
    "                    metrics_serializable[k] = v.tolist() if isinstance(v, np.ndarray) else v\n",
    "                elif isinstance(v, dict):\n",
    "                    metrics_serializable[k] = v\n",
    "                else:\n",
    "                    metrics_serializable[k] = str(v)\n",
    "\n",
    "            save_checkpoint({\n",
    "                'task': task,\n",
    "                'metrics': metrics_serializable,\n",
    "                'n_samples': len(ensemble_pred)\n",
    "            }, ensemble_metrics_path)\n",
    "\n",
    "            print(f\"  ✓ Recomputed ensemble metrics: Macro F1={ensemble_metrics_dict.get('macro_f1', 0.0):.4f}\")\n",
    "        continue\n",
    "\n",
    "    if task not in classifier_specific_results:\n",
    "        print(f\"   Skipping {task}: No results available\")\n",
    "        continue\n",
    "\n",
    "    label_list = CLARITY_LABELS if 'clarity' in task else EVASION_LABELS\n",
    "\n",
    "    probabilities_list = []\n",
    "    weights_list = []\n",
    "    classifier_names_list = []\n",
    "\n",
    "    for clf_name, result in classifier_specific_results[task].items():\n",
    "        y_proba = result.get('probabilities')\n",
    "        if y_proba is None:\n",
    "            print(f\"   Skipping {clf_name}: No probabilities available\")\n",
    "            continue\n",
    "\n",
    "        metrics = result.get('metrics', {})\n",
    "        macro_f1 = metrics.get('macro_f1', 0.0)\n",
    "        weight = max(macro_f1, 0.0001)\n",
    "\n",
    "        probabilities_list.append(y_proba)\n",
    "        weights_list.append(weight)\n",
    "        classifier_names_list.append(clf_name)\n",
    "\n",
    "    if len(probabilities_list) == 0:\n",
    "        print(f\"   No probabilities available for {task}. Skipping ensemble.\")\n",
    "        continue\n",
    "\n",
    "    total_weight = sum(weights_list)\n",
    "    normalized_weights = [w / total_weight for w in weights_list] if total_weight > 0 else [1.0 / len(weights_list)] * len(weights_list)\n",
    "\n",
    "    print(f\"\\n  Normalized weights (based on Macro F1):\")\n",
    "    for clf_name, norm_weight, macro_f1 in zip(classifier_names_list, normalized_weights, weights_list):\n",
    "        print(f\"    {clf_name}: {norm_weight:.4f} (Macro F1: {macro_f1:.4f})\")\n",
    "\n",
    "    ensemble_proba = np.zeros_like(probabilities_list[0])\n",
    "    for proba, weight in zip(probabilities_list, normalized_weights):\n",
    "        ensemble_proba += weight * proba\n",
    "\n",
    "    ensemble_pred_indices = np.argmax(ensemble_proba, axis=1)\n",
    "    ensemble_pred = np.array([label_list[i] for i in ensemble_pred_indices])\n",
    "\n",
    "    print(f\"    ✓ Ensemble predictions shape: {ensemble_pred.shape}\")\n",
    "\n",
    "    save_checkpoint(ensemble_pred, ensemble_pred_path)\n",
    "    save_checkpoint(ensemble_proba, ensemble_proba_path)\n",
    "    print(f\"    ✓ Saved ensemble predictions and probabilities\")\n",
    "\n",
    "    split_task = 'evasion' if task == 'hierarchical_evasion_to_clarity' else task\n",
    "    test_ds = storage.load_split('test', task=split_task)\n",
    "    label_key = 'clarity_label' if task == 'hierarchical_evasion_to_clarity' else ('clarity_label' if task == 'clarity' else 'evasion_label')\n",
    "    y_test_true = np.array([test_ds[i][label_key] for i in range(len(test_ds))])\n",
    "\n",
    "    le = LabelEncoder()\n",
    "    y_test_true_encoded = le.fit_transform(y_test_true)\n",
    "    ensemble_pred_encoded = le.transform(ensemble_pred)\n",
    "\n",
    "    ensemble_metrics = compute_all_metrics(\n",
    "        y_test_true_encoded, ensemble_pred_encoded, label_list,\n",
    "        task_name=f\"ENSEMBLE_{task}\"\n",
    "    )\n",
    "\n",
    "    print(f\"    Ensemble Test Macro F1: {ensemble_metrics.get('macro_f1', 0.0):.4f}\")\n",
    "\n",
    "    weights_metadata = {\n",
    "        'task': task,\n",
    "        'method': 'weighted_average',\n",
    "        'weight_metric': 'macro_f1',\n",
    "        'n_classifiers': len(classifier_names_list),\n",
    "        'classifiers': classifier_names_list,\n",
    "        'weights': {name: float(weight) for name, weight in zip(classifier_names_list, normalized_weights)},\n",
    "        'n_samples': len(ensemble_pred),\n",
    "        'label_list': label_list\n",
    "    }\n",
    "    save_checkpoint(weights_metadata, ensemble_weights_path)\n",
    "\n",
    "    metrics_serializable = {}\n",
    "    for k, v in ensemble_metrics.items():\n",
    "        if isinstance(v, (int, float, np.integer, np.floating)):\n",
    "            metrics_serializable[k] = float(v)\n",
    "        elif isinstance(v, (list, np.ndarray)):\n",
    "            metrics_serializable[k] = v.tolist() if isinstance(v, np.ndarray) else v\n",
    "        elif isinstance(v, dict):\n",
    "            metrics_serializable[k] = v\n",
    "        else:\n",
    "            metrics_serializable[k] = str(v)\n",
    "\n",
    "    save_checkpoint({\n",
    "        'task': task,\n",
    "        'metrics': metrics_serializable,\n",
    "        'n_samples': len(ensemble_pred)\n",
    "    }, ensemble_metrics_path)\n",
    "\n",
    "    ensemble_results[task] = {\n",
    "        'predictions': ensemble_pred,\n",
    "        'probabilities': ensemble_proba,\n",
    "        'weights': {name: weight for name, weight in zip(classifier_names_list, normalized_weights)},\n",
    "        'classifiers_used': classifier_names_list\n",
    "    }\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 5 COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ========================================================================\n",
    "# STEP 6: Generate Summary Report Tables\n",
    "# ========================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 6: GENERATE SUMMARY REPORT TABLES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Task order (clarity first, then hierarchical)\n",
    "TASK_ORDER = ['clarity', 'hierarchical_evasion_to_clarity']\n",
    "\n",
    "# ========================================================================\n",
    "# STEP 6.1: Collect All Classifier Results (Individual Classifiers)\n",
    "# ========================================================================\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"STEP 6.1: Individual Classifier Results\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "summary_rows = []\n",
    "\n",
    "for task in TASK_ORDER:\n",
    "    if task not in classifier_specific_results:\n",
    "        continue\n",
    "\n",
    "    for clf_name, result in classifier_specific_results[task].items():\n",
    "        metrics = result.get('metrics', {})\n",
    "        n_features = result.get('n_features', 0)\n",
    "\n",
    "        summary_rows.append({\n",
    "            'classifier': clf_name,\n",
    "            'task': task,\n",
    "            'n_features': n_features,\n",
    "            'macro_f1': metrics.get('macro_f1', 0.0),\n",
    "            'accuracy': metrics.get('accuracy', 0.0),\n",
    "            'macro_precision': metrics.get('macro_precision', 0.0),\n",
    "            'macro_recall': metrics.get('macro_recall', 0.0),\n",
    "            'weighted_f1': metrics.get('weighted_f1', 0.0),\n",
    "        })\n",
    "\n",
    "# ========================================================================\n",
    "# STEP 6.2: Add Ensemble Results\n",
    "# ========================================================================\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"STEP 6.2: Ensemble Results (Weighted Average)\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "for task in TASK_ORDER:\n",
    "    ensemble_metrics_path = metrics_dir / f'ensemble_evaluation_metrics_{task}.json'\n",
    "    ensemble_metrics = load_checkpoint(ensemble_metrics_path)\n",
    "\n",
    "    if ensemble_metrics:\n",
    "        metrics_dict = ensemble_metrics.get('metrics', {})\n",
    "        summary_rows.append({\n",
    "            'classifier': 'Ensemble (Weighted)',\n",
    "            'task': task,\n",
    "            'n_features': 'N/A',\n",
    "            'macro_f1': metrics_dict.get('macro_f1', 0.0),\n",
    "            'accuracy': metrics_dict.get('accuracy', 0.0),\n",
    "            'macro_precision': metrics_dict.get('macro_precision', 0.0),\n",
    "            'macro_recall': metrics_dict.get('macro_recall', 0.0),\n",
    "            'weighted_f1': metrics_dict.get('weighted_f1', 0.0),\n",
    "        })\n",
    "        print(f\"  ✓ Added ensemble results for {task}\")\n",
    "    else:\n",
    "        print(f\"   Ensemble results for {task} not found\")\n",
    "\n",
    "# Create summary DataFrame\n",
    "df_summary = pd.DataFrame(summary_rows)\n",
    "\n",
    "if len(df_summary) == 0:\n",
    "    print(\"   No results found for summary table\")\n",
    "else:\n",
    "    # Remove duplicates\n",
    "    df_summary = df_summary.drop_duplicates(\n",
    "        subset=['classifier', 'task'],\n",
    "        keep='first'\n",
    "    )\n",
    "\n",
    "    # ========================================================================\n",
    "    # STEP 6.3: Detailed Summary Table (All Metrics)\n",
    "    # ========================================================================\n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(\"STEP 6.3: Detailed Summary Table (All Metrics)\")\n",
    "    print(\"-\"*80)\n",
    "\n",
    "    # Display detailed table\n",
    "    print(\"\\nDetailed Summary Table:\")\n",
    "    try:\n",
    "        from IPython.display import display\n",
    "        display(df_summary.drop(columns=['n_features']).style.format({\n",
    "            'macro_f1': '{:.4f}',\n",
    "            'accuracy': '{:.4f}',\n",
    "            'macro_precision': '{:.4f}',\n",
    "            'macro_recall': '{:.4f}',\n",
    "            'weighted_f1': '{:.4f}'\n",
    "        }))\n",
    "    except:\n",
    "        print(df_summary.drop(columns=['n_features']).to_string())\n",
    "\n",
    "    # Save detailed table\n",
    "    detailed_path = tables_dir / 'summary_detailed.csv'\n",
    "    df_summary.to_csv(detailed_path, index=False)\n",
    "    print(f\"\\n  ✓ Saved detailed table: {detailed_path.name}\")\n",
    "\n",
    "    # Save HTML version\n",
    "    html_detailed_path = tables_dir / 'summary_detailed.html'\n",
    "    df_summary.to_html(html_detailed_path, index=False, float_format='{:.4f}'.format)\n",
    "    print(f\"  ✓ Saved HTML: {html_detailed_path.name}\")\n",
    "\n",
    "    # ========================================================================\n",
    "    # STEP 6.4: Pivot Table (Classifier × Task) - Macro F1\n",
    "    # ========================================================================\n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(\"STEP 6.4: Pivot Table - Classifier × Task (Macro F1)\")\n",
    "    print(\"-\"*80)\n",
    "\n",
    "    # Create pivot table\n",
    "    df_pivot = df_summary.pivot(\n",
    "        index='classifier',\n",
    "        columns='task',\n",
    "        values='macro_f1'\n",
    "    )\n",
    "\n",
    "    # Reorder columns: clarity first, then hierarchical\n",
    "    available_tasks = [t for t in TASK_ORDER if t in df_pivot.columns]\n",
    "    remaining_tasks = sorted([t for t in df_pivot.columns if t not in available_tasks])\n",
    "    column_order = available_tasks + remaining_tasks\n",
    "    df_pivot = df_pivot[column_order]\n",
    "\n",
    "    # Display pivot table\n",
    "    print(\"\\nPivot Table (Macro F1):\")\n",
    "    try:\n",
    "        from IPython.display import display\n",
    "        display(df_pivot.style.format(precision=4))\n",
    "    except:\n",
    "        print(df_pivot.to_string())\n",
    "\n",
    "    # Save pivot table\n",
    "    pivot_path = tables_dir / 'summary_pivot_classifier_wise.csv'\n",
    "    df_pivot.to_csv(pivot_path)\n",
    "    print(f\"\\n  ✓ Saved pivot table: {pivot_path.name}\")\n",
    "\n",
    "    # Save HTML version\n",
    "    html_pivot_path = tables_dir / 'summary_pivot_classifier_wise.html'\n",
    "    df_pivot.to_html(html_pivot_path, float_format='{:.4f}'.format)\n",
    "    print(f\"  ✓ Saved HTML: {html_pivot_path.name}\")\n",
    "\n",
    "    # ========================================================================\n",
    "    # STEP 6.5: Pivot Table (Classifier × Task) - Accuracy\n",
    "    # ========================================================================\n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(\"STEP 6.5: Pivot Table - Classifier × Task (Accuracy)\")\n",
    "    print(\"-\"*80)\n",
    "\n",
    "    df_pivot_acc = df_summary.pivot(\n",
    "        index='classifier',\n",
    "        columns='task',\n",
    "        values='accuracy'\n",
    "    )\n",
    "\n",
    "    df_pivot_acc = df_pivot_acc[column_order]\n",
    "\n",
    "    print(\"\\nPivot Table (Accuracy):\")\n",
    "    try:\n",
    "        from IPython.display import display\n",
    "        display(df_pivot_acc.style.format(precision=4))\n",
    "    except:\n",
    "        print(df_pivot_acc.to_string())\n",
    "\n",
    "    pivot_acc_path = tables_dir / 'summary_pivot_accuracy.csv'\n",
    "    df_pivot_acc.to_csv(pivot_acc_path)\n",
    "    print(f\"\\n  ✓ Saved pivot table: {pivot_acc_path.name}\")\n",
    "\n",
    "    # ========================================================================\n",
    "    # STEP 6.6: Summary by Task (Individual Classifiers Only)\n",
    "    # ========================================================================\n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(\"STEP 6.6: Summary by Task (Individual Classifiers)\")\n",
    "    print(\"-\"*80)\n",
    "\n",
    "    for task in TASK_ORDER:\n",
    "        df_task = df_summary[\n",
    "            (df_summary['task'] == task) &\n",
    "            (df_summary['classifier'] != 'Ensemble (Weighted)')\n",
    "        ].copy()\n",
    "\n",
    "        if len(df_task) == 0:\n",
    "            continue\n",
    "\n",
    "        print(f\"\\n  {task.upper()} - Individual Classifiers:\")\n",
    "        print(f\"  {'-'*60}\")\n",
    "\n",
    "        # Sort by macro_f1 descending\n",
    "        df_task = df_task.sort_values('macro_f1', ascending=False)\n",
    "\n",
    "        # Display\n",
    "        try:\n",
    "            from IPython.display import display\n",
    "            display(df_task[['classifier', 'macro_f1', 'accuracy', 'macro_precision', 'macro_recall']].style.format({\n",
    "                'macro_f1': '{:.4f}',\n",
    "                'accuracy': '{:.4f}',\n",
    "                'macro_precision': '{:.4f}',\n",
    "                'macro_recall': '{:.4f}'\n",
    "            }))\n",
    "        except:\n",
    "            print(df_task[['classifier', 'macro_f1', 'accuracy', 'macro_precision', 'macro_recall']].to_string())\n",
    "\n",
    "        # Save per-task summary\n",
    "        task_path = tables_dir / f'summary_{task}_individual.csv'\n",
    "        df_task.to_csv(task_path, index=False)\n",
    "        print(f\"  ✓ Saved: {task_path.name}\")\n",
    "\n",
    "    # ========================================================================\n",
    "    # STEP 6.7: Ensemble Comparison Table\n",
    "    # ========================================================================\n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(\"STEP 6.7: Ensemble vs Best Individual Classifier\")\n",
    "    print(\"-\"*80)\n",
    "\n",
    "    ensemble_comparison_rows = []\n",
    "\n",
    "    for task in TASK_ORDER:\n",
    "        # Get best individual classifier\n",
    "        df_task_individual = df_summary[\n",
    "            (df_summary['task'] == task) &\n",
    "            (df_summary['classifier'] != 'Ensemble (Weighted)')\n",
    "        ]\n",
    "\n",
    "        if len(df_task_individual) == 0:\n",
    "            continue\n",
    "\n",
    "        best_individual = df_task_individual.loc[df_task_individual['macro_f1'].idxmax()]\n",
    "\n",
    "        # Get ensemble result\n",
    "        df_task_ensemble = df_summary[\n",
    "            (df_summary['task'] == task) &\n",
    "            (df_summary['classifier'] == 'Ensemble (Weighted)')\n",
    "        ]\n",
    "\n",
    "        if len(df_task_ensemble) == 0:\n",
    "            continue\n",
    "\n",
    "        ensemble_result = df_task_ensemble.iloc[0]\n",
    "\n",
    "        ensemble_comparison_rows.append({\n",
    "            'task': task,\n",
    "            'best_classifier': best_individual['classifier'],\n",
    "            'best_macro_f1': best_individual['macro_f1'],\n",
    "            'best_n_features': best_individual['n_features'],\n",
    "            'ensemble_macro_f1': ensemble_result['macro_f1'],\n",
    "            'improvement': ensemble_result['macro_f1'] - best_individual['macro_f1'],\n",
    "            'ensemble_accuracy': ensemble_result['accuracy'],\n",
    "            'best_accuracy': best_individual['accuracy'],\n",
    "        })\n",
    "\n",
    "    if ensemble_comparison_rows:\n",
    "        df_ensemble_comparison = pd.DataFrame(ensemble_comparison_rows)\n",
    "\n",
    "        print(\"\\nEnsemble vs Best Individual:\")\n",
    "        try:\n",
    "            from IPython.display import display\n",
    "            display(df_ensemble_comparison.drop(columns=['best_n_features']).style.format({\n",
    "                'best_macro_f1': '{:.4f}',\n",
    "                'ensemble_macro_f1': '{:.4f}',\n",
    "                'improvement': '{:.4f}',\n",
    "                'ensemble_accuracy': '{:.4f}',\n",
    "                'best_accuracy': '{:.4f}'\n",
    "            }))\n",
    "        except:\n",
    "            print(df_ensemble_comparison.drop(columns=['best_n_features']).to_string())\n",
    "\n",
    "        comparison_path = tables_dir / 'ensemble_comparison.csv'\n",
    "        df_ensemble_comparison.to_csv(comparison_path, index=False)\n",
    "        print(f\"\\n  ✓ Saved: {comparison_path.name}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY REPORT TABLES COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nAll tables saved to: {tables_dir}\")\n",
    "print(f\"  - Detailed summary: summary_detailed.csv\")\n",
    "print(f\"  - Pivot (Macro F1): summary_pivot_classifier_wise.csv\")\n",
    "print(f\"  - Pivot (Accuracy): summary_pivot_accuracy.csv\")\n",
    "print(f\"  - Per-task summaries: summary_{{task}}_individual.csv\")\n",
    "print(f\"  - Ensemble comparison: ensemble_comparison.csv\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ALL STEPS COMPLETE\")\n",
    "print(\"=\"*80)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "I7gGCKwsV2j1",
    "outputId": "e8aa2756-5554-4cb5-fa55-574c1b8c9928"
   },
   "execution_count": 27,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "================================================================================\n",
      "LOADING CLASSIFIER RESULTS FROM CHECKPOINTS\n",
      "================================================================================\n",
      "\n",
      "Loading results for task: clarity\n",
      "   LogisticRegression: Could not determine n_features (no selected_features or trajectory), defaulting to 0\n",
      "   LinearSVC: Could not determine n_features (no selected_features or trajectory), defaulting to 0\n",
      "   RandomForest: Could not determine n_features (no selected_features or trajectory), defaulting to 0\n",
      "   MLP: Could not determine n_features (no selected_features or trajectory), defaulting to 0\n",
      "   XGBoost: Could not determine n_features (no selected_features or trajectory), defaulting to 0\n",
      "   LightGBM: Could not determine n_features (no selected_features or trajectory), defaulting to 0\n",
      "\n",
      "Loading results for task: hierarchical_evasion_to_clarity\n",
      "   LogisticRegression: Could not determine n_features (no selected_features or trajectory), defaulting to 0\n",
      "   LinearSVC: Could not determine n_features (no selected_features or trajectory), defaulting to 0\n",
      "   RandomForest: Could not determine n_features (no selected_features or trajectory), defaulting to 0\n",
      "   MLP: Could not determine n_features (no selected_features or trajectory), defaulting to 0\n",
      "   XGBoost: Could not determine n_features (no selected_features or trajectory), defaulting to 0\n",
      "   LightGBM: Could not determine n_features (no selected_features or trajectory), defaulting to 0\n",
      "\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "STEP 5: WEIGHTED AVERAGE ENSEMBLE\n",
      "================================================================================\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "TASK: CLARITY\n",
      "--------------------------------------------------------------------------------\n",
      "  ✓ Found ensemble checkpoint, loading...\n",
      "  ✓ Ensemble predictions shape: (308,)\n",
      "  ✓ Ensemble Test Macro F1: 0.4485\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "TASK: HIERARCHICAL_EVASION_TO_CLARITY\n",
      "--------------------------------------------------------------------------------\n",
      "  ✓ Found ensemble checkpoint, loading...\n",
      "  ✓ Ensemble predictions shape: (275,)\n",
      "  ✓ Ensemble Test Macro F1: 0.4336\n",
      "\n",
      "================================================================================\n",
      "STEP 5 COMPLETE\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "STEP 6: GENERATE SUMMARY REPORT TABLES\n",
      "================================================================================\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "STEP 6.1: Individual Classifier Results\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "STEP 6.2: Ensemble Results (Weighted Average)\n",
      "--------------------------------------------------------------------------------\n",
      "  ✓ Added ensemble results for clarity\n",
      "  ✓ Added ensemble results for hierarchical_evasion_to_clarity\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "STEP 6.3: Detailed Summary Table (All Metrics)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Detailed Summary Table:\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7be50f1d8aa0>"
      ],
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_8b31d\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_8b31d_level0_col0\" class=\"col_heading level0 col0\" >classifier</th>\n",
       "      <th id=\"T_8b31d_level0_col1\" class=\"col_heading level0 col1\" >task</th>\n",
       "      <th id=\"T_8b31d_level0_col2\" class=\"col_heading level0 col2\" >macro_f1</th>\n",
       "      <th id=\"T_8b31d_level0_col3\" class=\"col_heading level0 col3\" >accuracy</th>\n",
       "      <th id=\"T_8b31d_level0_col4\" class=\"col_heading level0 col4\" >macro_precision</th>\n",
       "      <th id=\"T_8b31d_level0_col5\" class=\"col_heading level0 col5\" >macro_recall</th>\n",
       "      <th id=\"T_8b31d_level0_col6\" class=\"col_heading level0 col6\" >weighted_f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_8b31d_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_8b31d_row0_col0\" class=\"data row0 col0\" >LogisticRegression</td>\n",
       "      <td id=\"T_8b31d_row0_col1\" class=\"data row0 col1\" >clarity</td>\n",
       "      <td id=\"T_8b31d_row0_col2\" class=\"data row0 col2\" >0.4571</td>\n",
       "      <td id=\"T_8b31d_row0_col3\" class=\"data row0 col3\" >0.5390</td>\n",
       "      <td id=\"T_8b31d_row0_col4\" class=\"data row0 col4\" >0.4482</td>\n",
       "      <td id=\"T_8b31d_row0_col5\" class=\"data row0 col5\" >0.4884</td>\n",
       "      <td id=\"T_8b31d_row0_col6\" class=\"data row0 col6\" >0.5587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_8b31d_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_8b31d_row1_col0\" class=\"data row1 col0\" >LinearSVC</td>\n",
       "      <td id=\"T_8b31d_row1_col1\" class=\"data row1 col1\" >clarity</td>\n",
       "      <td id=\"T_8b31d_row1_col2\" class=\"data row1 col2\" >0.4608</td>\n",
       "      <td id=\"T_8b31d_row1_col3\" class=\"data row1 col3\" >0.6623</td>\n",
       "      <td id=\"T_8b31d_row1_col4\" class=\"data row1 col4\" >0.5119</td>\n",
       "      <td id=\"T_8b31d_row1_col5\" class=\"data row1 col5\" >0.4566</td>\n",
       "      <td id=\"T_8b31d_row1_col6\" class=\"data row1 col6\" >0.6178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_8b31d_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_8b31d_row2_col0\" class=\"data row2 col0\" >RandomForest</td>\n",
       "      <td id=\"T_8b31d_row2_col1\" class=\"data row2 col1\" >clarity</td>\n",
       "      <td id=\"T_8b31d_row2_col2\" class=\"data row2 col2\" >0.4032</td>\n",
       "      <td id=\"T_8b31d_row2_col3\" class=\"data row2 col3\" >0.6526</td>\n",
       "      <td id=\"T_8b31d_row2_col4\" class=\"data row2 col4\" >0.4658</td>\n",
       "      <td id=\"T_8b31d_row2_col5\" class=\"data row2 col5\" >0.4028</td>\n",
       "      <td id=\"T_8b31d_row2_col6\" class=\"data row2 col6\" >0.5935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_8b31d_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_8b31d_row3_col0\" class=\"data row3 col0\" >MLP</td>\n",
       "      <td id=\"T_8b31d_row3_col1\" class=\"data row3 col1\" >clarity</td>\n",
       "      <td id=\"T_8b31d_row3_col2\" class=\"data row3 col2\" >0.3554</td>\n",
       "      <td id=\"T_8b31d_row3_col3\" class=\"data row3 col3\" >0.6688</td>\n",
       "      <td id=\"T_8b31d_row3_col4\" class=\"data row3 col4\" >0.6879</td>\n",
       "      <td id=\"T_8b31d_row3_col5\" class=\"data row3 col5\" >0.3721</td>\n",
       "      <td id=\"T_8b31d_row3_col6\" class=\"data row3 col6\" >0.5733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_8b31d_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_8b31d_row4_col0\" class=\"data row4 col0\" >XGBoost</td>\n",
       "      <td id=\"T_8b31d_row4_col1\" class=\"data row4 col1\" >clarity</td>\n",
       "      <td id=\"T_8b31d_row4_col2\" class=\"data row4 col2\" >0.4006</td>\n",
       "      <td id=\"T_8b31d_row4_col3\" class=\"data row4 col3\" >0.6558</td>\n",
       "      <td id=\"T_8b31d_row4_col4\" class=\"data row4 col4\" >0.4423</td>\n",
       "      <td id=\"T_8b31d_row4_col5\" class=\"data row4 col5\" >0.3996</td>\n",
       "      <td id=\"T_8b31d_row4_col6\" class=\"data row4 col6\" >0.6169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_8b31d_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_8b31d_row5_col0\" class=\"data row5 col0\" >LightGBM</td>\n",
       "      <td id=\"T_8b31d_row5_col1\" class=\"data row5 col1\" >clarity</td>\n",
       "      <td id=\"T_8b31d_row5_col2\" class=\"data row5 col2\" >0.4522</td>\n",
       "      <td id=\"T_8b31d_row5_col3\" class=\"data row5 col3\" >0.6786</td>\n",
       "      <td id=\"T_8b31d_row5_col4\" class=\"data row5 col4\" >0.6142</td>\n",
       "      <td id=\"T_8b31d_row5_col5\" class=\"data row5 col5\" >0.4313</td>\n",
       "      <td id=\"T_8b31d_row5_col6\" class=\"data row5 col6\" >0.6266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_8b31d_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_8b31d_row6_col0\" class=\"data row6 col0\" >LogisticRegression</td>\n",
       "      <td id=\"T_8b31d_row6_col1\" class=\"data row6 col1\" >hierarchical_evasion_to_clarity</td>\n",
       "      <td id=\"T_8b31d_row6_col2\" class=\"data row6 col2\" >0.4464</td>\n",
       "      <td id=\"T_8b31d_row6_col3\" class=\"data row6 col3\" >0.5055</td>\n",
       "      <td id=\"T_8b31d_row6_col4\" class=\"data row6 col4\" >0.4406</td>\n",
       "      <td id=\"T_8b31d_row6_col5\" class=\"data row6 col5\" >0.4857</td>\n",
       "      <td id=\"T_8b31d_row6_col6\" class=\"data row6 col6\" >0.5261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_8b31d_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_8b31d_row7_col0\" class=\"data row7 col0\" >LinearSVC</td>\n",
       "      <td id=\"T_8b31d_row7_col1\" class=\"data row7 col1\" >hierarchical_evasion_to_clarity</td>\n",
       "      <td id=\"T_8b31d_row7_col2\" class=\"data row7 col2\" >0.4597</td>\n",
       "      <td id=\"T_8b31d_row7_col3\" class=\"data row7 col3\" >0.6436</td>\n",
       "      <td id=\"T_8b31d_row7_col4\" class=\"data row7 col4\" >0.5175</td>\n",
       "      <td id=\"T_8b31d_row7_col5\" class=\"data row7 col5\" >0.4539</td>\n",
       "      <td id=\"T_8b31d_row7_col6\" class=\"data row7 col6\" >0.6016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_8b31d_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_8b31d_row8_col0\" class=\"data row8 col0\" >RandomForest</td>\n",
       "      <td id=\"T_8b31d_row8_col1\" class=\"data row8 col1\" >hierarchical_evasion_to_clarity</td>\n",
       "      <td id=\"T_8b31d_row8_col2\" class=\"data row8 col2\" >0.3966</td>\n",
       "      <td id=\"T_8b31d_row8_col3\" class=\"data row8 col3\" >0.6218</td>\n",
       "      <td id=\"T_8b31d_row8_col4\" class=\"data row8 col4\" >0.4588</td>\n",
       "      <td id=\"T_8b31d_row8_col5\" class=\"data row8 col5\" >0.4003</td>\n",
       "      <td id=\"T_8b31d_row8_col6\" class=\"data row8 col6\" >0.5599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_8b31d_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_8b31d_row9_col0\" class=\"data row9 col0\" >MLP</td>\n",
       "      <td id=\"T_8b31d_row9_col1\" class=\"data row9 col1\" >hierarchical_evasion_to_clarity</td>\n",
       "      <td id=\"T_8b31d_row9_col2\" class=\"data row9 col2\" >0.3492</td>\n",
       "      <td id=\"T_8b31d_row9_col3\" class=\"data row9 col3\" >0.6400</td>\n",
       "      <td id=\"T_8b31d_row9_col4\" class=\"data row9 col4\" >0.6881</td>\n",
       "      <td id=\"T_8b31d_row9_col5\" class=\"data row9 col5\" >0.3725</td>\n",
       "      <td id=\"T_8b31d_row9_col6\" class=\"data row9 col6\" >0.5382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_8b31d_level0_row10\" class=\"row_heading level0 row10\" >10</th>\n",
       "      <td id=\"T_8b31d_row10_col0\" class=\"data row10 col0\" >XGBoost</td>\n",
       "      <td id=\"T_8b31d_row10_col1\" class=\"data row10 col1\" >hierarchical_evasion_to_clarity</td>\n",
       "      <td id=\"T_8b31d_row10_col2\" class=\"data row10 col2\" >0.3933</td>\n",
       "      <td id=\"T_8b31d_row10_col3\" class=\"data row10 col3\" >0.6291</td>\n",
       "      <td id=\"T_8b31d_row10_col4\" class=\"data row10 col4\" >0.4603</td>\n",
       "      <td id=\"T_8b31d_row10_col5\" class=\"data row10 col5\" >0.3954</td>\n",
       "      <td id=\"T_8b31d_row10_col6\" class=\"data row10 col6\" >0.5844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_8b31d_level0_row11\" class=\"row_heading level0 row11\" >11</th>\n",
       "      <td id=\"T_8b31d_row11_col0\" class=\"data row11 col0\" >LightGBM</td>\n",
       "      <td id=\"T_8b31d_row11_col1\" class=\"data row11 col1\" >hierarchical_evasion_to_clarity</td>\n",
       "      <td id=\"T_8b31d_row11_col2\" class=\"data row11 col2\" >0.4517</td>\n",
       "      <td id=\"T_8b31d_row11_col3\" class=\"data row11 col3\" >0.6691</td>\n",
       "      <td id=\"T_8b31d_row11_col4\" class=\"data row11 col4\" >0.5859</td>\n",
       "      <td id=\"T_8b31d_row11_col5\" class=\"data row11 col5\" >0.4396</td>\n",
       "      <td id=\"T_8b31d_row11_col6\" class=\"data row11 col6\" >0.6111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_8b31d_level0_row12\" class=\"row_heading level0 row12\" >12</th>\n",
       "      <td id=\"T_8b31d_row12_col0\" class=\"data row12 col0\" >Ensemble (Weighted)</td>\n",
       "      <td id=\"T_8b31d_row12_col1\" class=\"data row12 col1\" >clarity</td>\n",
       "      <td id=\"T_8b31d_row12_col2\" class=\"data row12 col2\" >0.4485</td>\n",
       "      <td id=\"T_8b31d_row12_col3\" class=\"data row12 col3\" >0.6851</td>\n",
       "      <td id=\"T_8b31d_row12_col4\" class=\"data row12 col4\" >0.5828</td>\n",
       "      <td id=\"T_8b31d_row12_col5\" class=\"data row12 col5\" >0.4370</td>\n",
       "      <td id=\"T_8b31d_row12_col6\" class=\"data row12 col6\" >0.6220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_8b31d_level0_row13\" class=\"row_heading level0 row13\" >13</th>\n",
       "      <td id=\"T_8b31d_row13_col0\" class=\"data row13 col0\" >Ensemble (Weighted)</td>\n",
       "      <td id=\"T_8b31d_row13_col1\" class=\"data row13 col1\" >hierarchical_evasion_to_clarity</td>\n",
       "      <td id=\"T_8b31d_row13_col2\" class=\"data row13 col2\" >0.4336</td>\n",
       "      <td id=\"T_8b31d_row13_col3\" class=\"data row13 col3\" >0.6655</td>\n",
       "      <td id=\"T_8b31d_row13_col4\" class=\"data row13 col4\" >0.5759</td>\n",
       "      <td id=\"T_8b31d_row13_col5\" class=\"data row13 col5\" >0.4304</td>\n",
       "      <td id=\"T_8b31d_row13_col6\" class=\"data row13 col6\" >0.5984</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "  ✓ Saved detailed table: summary_detailed.csv\n",
      "  ✓ Saved HTML: summary_detailed.html\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "STEP 6.4: Pivot Table - Classifier × Task (Macro F1)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Pivot Table (Macro F1):\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7be50fd18950>"
      ],
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_a7117\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >task</th>\n",
       "      <th id=\"T_a7117_level0_col0\" class=\"col_heading level0 col0\" >clarity</th>\n",
       "      <th id=\"T_a7117_level0_col1\" class=\"col_heading level0 col1\" >hierarchical_evasion_to_clarity</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >classifier</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "      <th class=\"blank col1\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_a7117_level0_row0\" class=\"row_heading level0 row0\" >Ensemble (Weighted)</th>\n",
       "      <td id=\"T_a7117_row0_col0\" class=\"data row0 col0\" >0.4485</td>\n",
       "      <td id=\"T_a7117_row0_col1\" class=\"data row0 col1\" >0.4336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a7117_level0_row1\" class=\"row_heading level0 row1\" >LightGBM</th>\n",
       "      <td id=\"T_a7117_row1_col0\" class=\"data row1 col0\" >0.4522</td>\n",
       "      <td id=\"T_a7117_row1_col1\" class=\"data row1 col1\" >0.4517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a7117_level0_row2\" class=\"row_heading level0 row2\" >LinearSVC</th>\n",
       "      <td id=\"T_a7117_row2_col0\" class=\"data row2 col0\" >0.4608</td>\n",
       "      <td id=\"T_a7117_row2_col1\" class=\"data row2 col1\" >0.4597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a7117_level0_row3\" class=\"row_heading level0 row3\" >LogisticRegression</th>\n",
       "      <td id=\"T_a7117_row3_col0\" class=\"data row3 col0\" >0.4571</td>\n",
       "      <td id=\"T_a7117_row3_col1\" class=\"data row3 col1\" >0.4464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a7117_level0_row4\" class=\"row_heading level0 row4\" >MLP</th>\n",
       "      <td id=\"T_a7117_row4_col0\" class=\"data row4 col0\" >0.3554</td>\n",
       "      <td id=\"T_a7117_row4_col1\" class=\"data row4 col1\" >0.3492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a7117_level0_row5\" class=\"row_heading level0 row5\" >RandomForest</th>\n",
       "      <td id=\"T_a7117_row5_col0\" class=\"data row5 col0\" >0.4032</td>\n",
       "      <td id=\"T_a7117_row5_col1\" class=\"data row5 col1\" >0.3966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a7117_level0_row6\" class=\"row_heading level0 row6\" >XGBoost</th>\n",
       "      <td id=\"T_a7117_row6_col0\" class=\"data row6 col0\" >0.4006</td>\n",
       "      <td id=\"T_a7117_row6_col1\" class=\"data row6 col1\" >0.3933</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "  ✓ Saved pivot table: summary_pivot_classifier_wise.csv\n",
      "  ✓ Saved HTML: summary_pivot_classifier_wise.html\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "STEP 6.5: Pivot Table - Classifier × Task (Accuracy)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Pivot Table (Accuracy):\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7be50f228a70>"
      ],
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_44a17\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >task</th>\n",
       "      <th id=\"T_44a17_level0_col0\" class=\"col_heading level0 col0\" >clarity</th>\n",
       "      <th id=\"T_44a17_level0_col1\" class=\"col_heading level0 col1\" >hierarchical_evasion_to_clarity</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >classifier</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "      <th class=\"blank col1\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_44a17_level0_row0\" class=\"row_heading level0 row0\" >Ensemble (Weighted)</th>\n",
       "      <td id=\"T_44a17_row0_col0\" class=\"data row0 col0\" >0.6851</td>\n",
       "      <td id=\"T_44a17_row0_col1\" class=\"data row0 col1\" >0.6655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_44a17_level0_row1\" class=\"row_heading level0 row1\" >LightGBM</th>\n",
       "      <td id=\"T_44a17_row1_col0\" class=\"data row1 col0\" >0.6786</td>\n",
       "      <td id=\"T_44a17_row1_col1\" class=\"data row1 col1\" >0.6691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_44a17_level0_row2\" class=\"row_heading level0 row2\" >LinearSVC</th>\n",
       "      <td id=\"T_44a17_row2_col0\" class=\"data row2 col0\" >0.6623</td>\n",
       "      <td id=\"T_44a17_row2_col1\" class=\"data row2 col1\" >0.6436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_44a17_level0_row3\" class=\"row_heading level0 row3\" >LogisticRegression</th>\n",
       "      <td id=\"T_44a17_row3_col0\" class=\"data row3 col0\" >0.5390</td>\n",
       "      <td id=\"T_44a17_row3_col1\" class=\"data row3 col1\" >0.5055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_44a17_level0_row4\" class=\"row_heading level0 row4\" >MLP</th>\n",
       "      <td id=\"T_44a17_row4_col0\" class=\"data row4 col0\" >0.6688</td>\n",
       "      <td id=\"T_44a17_row4_col1\" class=\"data row4 col1\" >0.6400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_44a17_level0_row5\" class=\"row_heading level0 row5\" >RandomForest</th>\n",
       "      <td id=\"T_44a17_row5_col0\" class=\"data row5 col0\" >0.6526</td>\n",
       "      <td id=\"T_44a17_row5_col1\" class=\"data row5 col1\" >0.6218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_44a17_level0_row6\" class=\"row_heading level0 row6\" >XGBoost</th>\n",
       "      <td id=\"T_44a17_row6_col0\" class=\"data row6 col0\" >0.6558</td>\n",
       "      <td id=\"T_44a17_row6_col1\" class=\"data row6 col1\" >0.6291</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "  ✓ Saved pivot table: summary_pivot_accuracy.csv\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "STEP 6.6: Summary by Task (Individual Classifiers)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "  CLARITY - Individual Classifiers:\n",
      "  ------------------------------------------------------------\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7be50ec2f290>"
      ],
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_66c2f\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_66c2f_level0_col0\" class=\"col_heading level0 col0\" >classifier</th>\n",
       "      <th id=\"T_66c2f_level0_col1\" class=\"col_heading level0 col1\" >macro_f1</th>\n",
       "      <th id=\"T_66c2f_level0_col2\" class=\"col_heading level0 col2\" >accuracy</th>\n",
       "      <th id=\"T_66c2f_level0_col3\" class=\"col_heading level0 col3\" >macro_precision</th>\n",
       "      <th id=\"T_66c2f_level0_col4\" class=\"col_heading level0 col4\" >macro_recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_66c2f_level0_row0\" class=\"row_heading level0 row0\" >1</th>\n",
       "      <td id=\"T_66c2f_row0_col0\" class=\"data row0 col0\" >LinearSVC</td>\n",
       "      <td id=\"T_66c2f_row0_col1\" class=\"data row0 col1\" >0.4608</td>\n",
       "      <td id=\"T_66c2f_row0_col2\" class=\"data row0 col2\" >0.6623</td>\n",
       "      <td id=\"T_66c2f_row0_col3\" class=\"data row0 col3\" >0.5119</td>\n",
       "      <td id=\"T_66c2f_row0_col4\" class=\"data row0 col4\" >0.4566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_66c2f_level0_row1\" class=\"row_heading level0 row1\" >0</th>\n",
       "      <td id=\"T_66c2f_row1_col0\" class=\"data row1 col0\" >LogisticRegression</td>\n",
       "      <td id=\"T_66c2f_row1_col1\" class=\"data row1 col1\" >0.4571</td>\n",
       "      <td id=\"T_66c2f_row1_col2\" class=\"data row1 col2\" >0.5390</td>\n",
       "      <td id=\"T_66c2f_row1_col3\" class=\"data row1 col3\" >0.4482</td>\n",
       "      <td id=\"T_66c2f_row1_col4\" class=\"data row1 col4\" >0.4884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_66c2f_level0_row2\" class=\"row_heading level0 row2\" >5</th>\n",
       "      <td id=\"T_66c2f_row2_col0\" class=\"data row2 col0\" >LightGBM</td>\n",
       "      <td id=\"T_66c2f_row2_col1\" class=\"data row2 col1\" >0.4522</td>\n",
       "      <td id=\"T_66c2f_row2_col2\" class=\"data row2 col2\" >0.6786</td>\n",
       "      <td id=\"T_66c2f_row2_col3\" class=\"data row2 col3\" >0.6142</td>\n",
       "      <td id=\"T_66c2f_row2_col4\" class=\"data row2 col4\" >0.4313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_66c2f_level0_row3\" class=\"row_heading level0 row3\" >2</th>\n",
       "      <td id=\"T_66c2f_row3_col0\" class=\"data row3 col0\" >RandomForest</td>\n",
       "      <td id=\"T_66c2f_row3_col1\" class=\"data row3 col1\" >0.4032</td>\n",
       "      <td id=\"T_66c2f_row3_col2\" class=\"data row3 col2\" >0.6526</td>\n",
       "      <td id=\"T_66c2f_row3_col3\" class=\"data row3 col3\" >0.4658</td>\n",
       "      <td id=\"T_66c2f_row3_col4\" class=\"data row3 col4\" >0.4028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_66c2f_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_66c2f_row4_col0\" class=\"data row4 col0\" >XGBoost</td>\n",
       "      <td id=\"T_66c2f_row4_col1\" class=\"data row4 col1\" >0.4006</td>\n",
       "      <td id=\"T_66c2f_row4_col2\" class=\"data row4 col2\" >0.6558</td>\n",
       "      <td id=\"T_66c2f_row4_col3\" class=\"data row4 col3\" >0.4423</td>\n",
       "      <td id=\"T_66c2f_row4_col4\" class=\"data row4 col4\" >0.3996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_66c2f_level0_row5\" class=\"row_heading level0 row5\" >3</th>\n",
       "      <td id=\"T_66c2f_row5_col0\" class=\"data row5 col0\" >MLP</td>\n",
       "      <td id=\"T_66c2f_row5_col1\" class=\"data row5 col1\" >0.3554</td>\n",
       "      <td id=\"T_66c2f_row5_col2\" class=\"data row5 col2\" >0.6688</td>\n",
       "      <td id=\"T_66c2f_row5_col3\" class=\"data row5 col3\" >0.6879</td>\n",
       "      <td id=\"T_66c2f_row5_col4\" class=\"data row5 col4\" >0.3721</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "  ✓ Saved: summary_clarity_individual.csv\n",
      "\n",
      "  HIERARCHICAL_EVASION_TO_CLARITY - Individual Classifiers:\n",
      "  ------------------------------------------------------------\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7be57ad056d0>"
      ],
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_fb041\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_fb041_level0_col0\" class=\"col_heading level0 col0\" >classifier</th>\n",
       "      <th id=\"T_fb041_level0_col1\" class=\"col_heading level0 col1\" >macro_f1</th>\n",
       "      <th id=\"T_fb041_level0_col2\" class=\"col_heading level0 col2\" >accuracy</th>\n",
       "      <th id=\"T_fb041_level0_col3\" class=\"col_heading level0 col3\" >macro_precision</th>\n",
       "      <th id=\"T_fb041_level0_col4\" class=\"col_heading level0 col4\" >macro_recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_fb041_level0_row0\" class=\"row_heading level0 row0\" >7</th>\n",
       "      <td id=\"T_fb041_row0_col0\" class=\"data row0 col0\" >LinearSVC</td>\n",
       "      <td id=\"T_fb041_row0_col1\" class=\"data row0 col1\" >0.4597</td>\n",
       "      <td id=\"T_fb041_row0_col2\" class=\"data row0 col2\" >0.6436</td>\n",
       "      <td id=\"T_fb041_row0_col3\" class=\"data row0 col3\" >0.5175</td>\n",
       "      <td id=\"T_fb041_row0_col4\" class=\"data row0 col4\" >0.4539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_fb041_level0_row1\" class=\"row_heading level0 row1\" >11</th>\n",
       "      <td id=\"T_fb041_row1_col0\" class=\"data row1 col0\" >LightGBM</td>\n",
       "      <td id=\"T_fb041_row1_col1\" class=\"data row1 col1\" >0.4517</td>\n",
       "      <td id=\"T_fb041_row1_col2\" class=\"data row1 col2\" >0.6691</td>\n",
       "      <td id=\"T_fb041_row1_col3\" class=\"data row1 col3\" >0.5859</td>\n",
       "      <td id=\"T_fb041_row1_col4\" class=\"data row1 col4\" >0.4396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_fb041_level0_row2\" class=\"row_heading level0 row2\" >6</th>\n",
       "      <td id=\"T_fb041_row2_col0\" class=\"data row2 col0\" >LogisticRegression</td>\n",
       "      <td id=\"T_fb041_row2_col1\" class=\"data row2 col1\" >0.4464</td>\n",
       "      <td id=\"T_fb041_row2_col2\" class=\"data row2 col2\" >0.5055</td>\n",
       "      <td id=\"T_fb041_row2_col3\" class=\"data row2 col3\" >0.4406</td>\n",
       "      <td id=\"T_fb041_row2_col4\" class=\"data row2 col4\" >0.4857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_fb041_level0_row3\" class=\"row_heading level0 row3\" >8</th>\n",
       "      <td id=\"T_fb041_row3_col0\" class=\"data row3 col0\" >RandomForest</td>\n",
       "      <td id=\"T_fb041_row3_col1\" class=\"data row3 col1\" >0.3966</td>\n",
       "      <td id=\"T_fb041_row3_col2\" class=\"data row3 col2\" >0.6218</td>\n",
       "      <td id=\"T_fb041_row3_col3\" class=\"data row3 col3\" >0.4588</td>\n",
       "      <td id=\"T_fb041_row3_col4\" class=\"data row3 col4\" >0.4003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_fb041_level0_row4\" class=\"row_heading level0 row4\" >10</th>\n",
       "      <td id=\"T_fb041_row4_col0\" class=\"data row4 col0\" >XGBoost</td>\n",
       "      <td id=\"T_fb041_row4_col1\" class=\"data row4 col1\" >0.3933</td>\n",
       "      <td id=\"T_fb041_row4_col2\" class=\"data row4 col2\" >0.6291</td>\n",
       "      <td id=\"T_fb041_row4_col3\" class=\"data row4 col3\" >0.4603</td>\n",
       "      <td id=\"T_fb041_row4_col4\" class=\"data row4 col4\" >0.3954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_fb041_level0_row5\" class=\"row_heading level0 row5\" >9</th>\n",
       "      <td id=\"T_fb041_row5_col0\" class=\"data row5 col0\" >MLP</td>\n",
       "      <td id=\"T_fb041_row5_col1\" class=\"data row5 col1\" >0.3492</td>\n",
       "      <td id=\"T_fb041_row5_col2\" class=\"data row5 col2\" >0.6400</td>\n",
       "      <td id=\"T_fb041_row5_col3\" class=\"data row5 col3\" >0.6881</td>\n",
       "      <td id=\"T_fb041_row5_col4\" class=\"data row5 col4\" >0.3725</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "  ✓ Saved: summary_hierarchical_evasion_to_clarity_individual.csv\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "STEP 6.7: Ensemble vs Best Individual Classifier\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Ensemble vs Best Individual:\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7be5101fa4e0>"
      ],
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_087c2\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_087c2_level0_col0\" class=\"col_heading level0 col0\" >task</th>\n",
       "      <th id=\"T_087c2_level0_col1\" class=\"col_heading level0 col1\" >best_classifier</th>\n",
       "      <th id=\"T_087c2_level0_col2\" class=\"col_heading level0 col2\" >best_macro_f1</th>\n",
       "      <th id=\"T_087c2_level0_col3\" class=\"col_heading level0 col3\" >ensemble_macro_f1</th>\n",
       "      <th id=\"T_087c2_level0_col4\" class=\"col_heading level0 col4\" >improvement</th>\n",
       "      <th id=\"T_087c2_level0_col5\" class=\"col_heading level0 col5\" >ensemble_accuracy</th>\n",
       "      <th id=\"T_087c2_level0_col6\" class=\"col_heading level0 col6\" >best_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_087c2_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_087c2_row0_col0\" class=\"data row0 col0\" >clarity</td>\n",
       "      <td id=\"T_087c2_row0_col1\" class=\"data row0 col1\" >LinearSVC</td>\n",
       "      <td id=\"T_087c2_row0_col2\" class=\"data row0 col2\" >0.4608</td>\n",
       "      <td id=\"T_087c2_row0_col3\" class=\"data row0 col3\" >0.4485</td>\n",
       "      <td id=\"T_087c2_row0_col4\" class=\"data row0 col4\" >-0.0123</td>\n",
       "      <td id=\"T_087c2_row0_col5\" class=\"data row0 col5\" >0.6851</td>\n",
       "      <td id=\"T_087c2_row0_col6\" class=\"data row0 col6\" >0.6623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_087c2_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_087c2_row1_col0\" class=\"data row1 col0\" >hierarchical_evasion_to_clarity</td>\n",
       "      <td id=\"T_087c2_row1_col1\" class=\"data row1 col1\" >LinearSVC</td>\n",
       "      <td id=\"T_087c2_row1_col2\" class=\"data row1 col2\" >0.4597</td>\n",
       "      <td id=\"T_087c2_row1_col3\" class=\"data row1 col3\" >0.4336</td>\n",
       "      <td id=\"T_087c2_row1_col4\" class=\"data row1 col4\" >-0.0260</td>\n",
       "      <td id=\"T_087c2_row1_col5\" class=\"data row1 col5\" >0.6655</td>\n",
       "      <td id=\"T_087c2_row1_col6\" class=\"data row1 col6\" >0.6436</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "  ✓ Saved: ensemble_comparison.csv\n",
      "\n",
      "================================================================================\n",
      "SUMMARY REPORT TABLES COMPLETE\n",
      "================================================================================\n",
      "\n",
      "All tables saved to: /content/drive/MyDrive/semeval_data/results/FinalResultsType2/classifier_specific/tables\n",
      "  - Detailed summary: summary_detailed.csv\n",
      "  - Pivot (Macro F1): summary_pivot_classifier_wise.csv\n",
      "  - Pivot (Accuracy): summary_pivot_accuracy.csv\n",
      "  - Per-task summaries: summary_{task}_individual.csv\n",
      "  - Ensemble comparison: ensemble_comparison.csv\n",
      "\n",
      "================================================================================\n",
      "ALL STEPS COMPLETE\n",
      "================================================================================\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# ========================================================================\n",
    "# STEP 6: Generate Summary Report Tables\n",
    "# ========================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 6: GENERATE SUMMARY REPORT TABLES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Task order (clarity first, then hierarchical)\n",
    "TASK_ORDER = ['clarity', 'hierarchical_evasion_to_clarity']\n",
    "\n",
    "# ========================================================================\n",
    "# STEP 6.1: Collect All Classifier Results (Individual Classifiers)\n",
    "# ========================================================================\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"STEP 6.1: Individual Classifier Results\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "summary_rows = []\n",
    "\n",
    "for task in TASK_ORDER:\n",
    "    if task not in classifier_specific_results:\n",
    "        continue\n",
    "\n",
    "    for clf_name, result in classifier_specific_results[task].items():\n",
    "        metrics = result.get('metrics', {})\n",
    "        n_features = result.get('n_features', 0)\n",
    "\n",
    "        summary_rows.append({\n",
    "            'classifier': clf_name,\n",
    "            'task': task,\n",
    "            'n_features': n_features,\n",
    "            'macro_f1': metrics.get('macro_f1', 0.0),\n",
    "            'accuracy': metrics.get('accuracy', 0.0),\n",
    "            'macro_precision': metrics.get('macro_precision', 0.0),\n",
    "            'macro_recall': metrics.get('macro_recall', 0.0),\n",
    "            'weighted_f1': metrics.get('weighted_f1', 0.0),\n",
    "        })\n",
    "\n",
    "# ========================================================================\n",
    "# STEP 6.2: Add Ensemble Results\n",
    "# ========================================================================\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"STEP 6.2: Ensemble Results (Weighted Average)\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "for task in TASK_ORDER:\n",
    "    ensemble_metrics_path = metrics_dir / f'ensemble_evaluation_metrics_{task}.json'\n",
    "    ensemble_metrics = load_checkpoint(ensemble_metrics_path)\n",
    "\n",
    "    if ensemble_metrics:\n",
    "        metrics_dict = ensemble_metrics.get('metrics', {})\n",
    "        summary_rows.append({\n",
    "            'classifier': 'Ensemble (Weighted)',\n",
    "            'task': task,\n",
    "            'n_features': 'N/A',\n",
    "            'macro_f1': metrics_dict.get('macro_f1', 0.0),\n",
    "            'accuracy': metrics_dict.get('accuracy', 0.0),\n",
    "            'macro_precision': metrics_dict.get('macro_precision', 0.0),\n",
    "            'macro_recall': metrics_dict.get('macro_recall', 0.0),\n",
    "            'weighted_f1': metrics_dict.get('weighted_f1', 0.0),\n",
    "        })\n",
    "        print(f\"  ✓ Added ensemble results for {task}\")\n",
    "    else:\n",
    "        print(f\"   Ensemble results for {task} not found\")\n",
    "\n",
    "# Create summary DataFrame\n",
    "df_summary = pd.DataFrame(summary_rows)\n",
    "\n",
    "if len(df_summary) == 0:\n",
    "    print(\"   No results found for summary table\")\n",
    "else:\n",
    "    # Remove duplicates\n",
    "    df_summary = df_summary.drop_duplicates(\n",
    "        subset=['classifier', 'task'],\n",
    "        keep='first'\n",
    "    )\n",
    "\n",
    "    # ========================================================================\n",
    "    # STEP 6.3: Detailed Summary Table (All Metrics)\n",
    "    # ========================================================================\n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(\"STEP 6.3: Detailed Summary Table (All Metrics)\")\n",
    "    print(\"-\"*80)\n",
    "\n",
    "    # Display detailed table\n",
    "    print(\"\\nDetailed Summary Table:\")\n",
    "    try:\n",
    "        from IPython.display import display\n",
    "        display(df_summary.drop(columns=['n_features']).style.format({\n",
    "            'macro_f1': '{:.4f}',\n",
    "            'accuracy': '{:.4f}',\n",
    "            'macro_precision': '{:.4f}',\n",
    "            'macro_recall': '{:.4f}',\n",
    "            'weighted_f1': '{:.4f}'\n",
    "        }))\n",
    "    except:\n",
    "        print(df_summary.drop(columns=['n_features']).to_string())\n",
    "\n",
    "    # Save detailed table\n",
    "    detailed_path = tables_dir / 'summary_detailed.csv'\n",
    "    df_summary.to_csv(detailed_path, index=False)\n",
    "    print(f\"\\n  ✓ Saved detailed table: {detailed_path.name}\")\n",
    "\n",
    "    # Save HTML version\n",
    "    html_detailed_path = tables_dir / 'summary_detailed.html'\n",
    "    df_summary.to_html(html_detailed_path, index=False, float_format='{:.4f}'.format)\n",
    "    print(f\"  ✓ Saved HTML: {html_detailed_path.name}\")\n",
    "\n",
    "    # ========================================================================\n",
    "    # STEP 6.4: Pivot Table (Classifier × Task) - Macro F1\n",
    "    # ========================================================================\n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(\"STEP 6.4: Pivot Table - Classifier × Task (Macro F1)\")\n",
    "    print(\"-\"*80)\n",
    "\n",
    "    # Create pivot table\n",
    "    df_pivot = df_summary.pivot(\n",
    "        index='classifier',\n",
    "        columns='task',\n",
    "        values='macro_f1'\n",
    "    )\n",
    "\n",
    "    # Reorder columns: clarity first, then hierarchical\n",
    "    available_tasks = [t for t in TASK_ORDER if t in df_pivot.columns]\n",
    "    remaining_tasks = sorted([t for t in df_pivot.columns if t not in available_tasks])\n",
    "    column_order = available_tasks + remaining_tasks\n",
    "    df_pivot = df_pivot[column_order]\n",
    "\n",
    "    # Display pivot table\n",
    "    print(\"\\nPivot Table (Macro F1):\")\n",
    "    try:\n",
    "        from IPython.display import display\n",
    "        display(df_pivot.style.format(precision=4))\n",
    "    except:\n",
    "        print(df_pivot.to_string())\n",
    "\n",
    "    # Save pivot table\n",
    "    pivot_path = tables_dir / 'summary_pivot_classifier_wise.csv'\n",
    "    df_pivot.to_csv(pivot_path)\n",
    "    print(f\"\\n  ✓ Saved pivot table: {pivot_path.name}\")\n",
    "\n",
    "    # Save HTML version\n",
    "    html_pivot_path = tables_dir / 'summary_pivot_classifier_wise.html'\n",
    "    df_pivot.to_html(html_pivot_path, float_format='{:.4f}'.format)\n",
    "    print(f\"  ✓ Saved HTML: {html_pivot_path.name}\")\n",
    "\n",
    "    # ========================================================================\n",
    "    # STEP 6.5: Pivot Table (Classifier × Task) - Accuracy\n",
    "    # ========================================================================\n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(\"STEP 6.5: Pivot Table - Classifier × Task (Accuracy)\")\n",
    "    print(\"-\"*80)\n",
    "\n",
    "    df_pivot_acc = df_summary.pivot(\n",
    "        index='classifier',\n",
    "        columns='task',\n",
    "        values='accuracy'\n",
    "    )\n",
    "\n",
    "    df_pivot_acc = df_pivot_acc[column_order]\n",
    "\n",
    "    print(\"\\nPivot Table (Accuracy):\")\n",
    "    try:\n",
    "        from IPython.display import display\n",
    "        display(df_pivot_acc.style.format(precision=4))\n",
    "    except:\n",
    "        print(df_pivot_acc.to_string())\n",
    "\n",
    "    pivot_acc_path = tables_dir / 'summary_pivot_accuracy.csv'\n",
    "    df_pivot_acc.to_csv(pivot_acc_path)\n",
    "    print(f\"\\n  ✓ Saved pivot table: {pivot_acc_path.name}\")\n",
    "\n",
    "    # ========================================================================\n",
    "    # STEP 6.6: Summary by Task (Individual Classifiers Only)\n",
    "    # ========================================================================\n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(\"STEP 6.6: Summary by Task (Individual Classifiers)\")\n",
    "    print(\"-\"*80)\n",
    "\n",
    "    for task in TASK_ORDER:\n",
    "        df_task = df_summary[\n",
    "            (df_summary['task'] == task) &\n",
    "            (df_summary['classifier'] != 'Ensemble (Weighted)')\n",
    "        ].copy()\n",
    "\n",
    "        if len(df_task) == 0:\n",
    "            continue\n",
    "\n",
    "        print(f\"\\n  {task.upper()} - Individual Classifiers:\")\n",
    "        print(f\"  {'-'*60}\")\n",
    "\n",
    "        # Sort by macro_f1 descending\n",
    "        df_task = df_task.sort_values('macro_f1', ascending=False)\n",
    "\n",
    "        # Display\n",
    "        try:\n",
    "            from IPython.display import display\n",
    "            display(df_task[['classifier', 'macro_f1', 'accuracy', 'macro_precision', 'macro_recall']].style.format({\n",
    "                'macro_f1': '{:.4f}',\n",
    "                'accuracy': '{:.4f}',\n",
    "                'macro_precision': '{:.4f}',\n",
    "                'macro_recall': '{:.4f}'\n",
    "            }))\n",
    "        except:\n",
    "            print(df_task[['classifier', 'macro_f1', 'accuracy', 'macro_precision', 'macro_recall']].to_string())\n",
    "\n",
    "        # Save per-task summary\n",
    "        task_path = tables_dir / f'summary_{task}_individual.csv'\n",
    "        df_task.to_csv(task_path, index=False)\n",
    "        print(f\"  ✓ Saved: {task_path.name}\")\n",
    "\n",
    "    # ========================================================================\n",
    "    # STEP 6.7: Ensemble Comparison Table\n",
    "    # ========================================================================\n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(\"STEP 6.7: Ensemble vs Best Individual Classifier\")\n",
    "    print(\"-\"*80)\n",
    "\n",
    "    ensemble_comparison_rows = []\n",
    "\n",
    "    for task in TASK_ORDER:\n",
    "        # Get best individual classifier\n",
    "        df_task_individual = df_summary[\n",
    "            (df_summary['task'] == task) &\n",
    "            (df_summary['classifier'] != 'Ensemble (Weighted)')\n",
    "        ]\n",
    "\n",
    "        if len(df_task_individual) == 0:\n",
    "            continue\n",
    "\n",
    "        best_individual = df_task_individual.loc[df_task_individual['macro_f1'].idxmax()]\n",
    "\n",
    "        # Get ensemble result\n",
    "        df_task_ensemble = df_summary[\n",
    "            (df_summary['task'] == task) &\n",
    "            (df_summary['classifier'] == 'Ensemble (Weighted)')\n",
    "        ]\n",
    "\n",
    "        if len(df_task_ensemble) == 0:\n",
    "            continue\n",
    "\n",
    "        ensemble_result = df_task_ensemble.iloc[0]\n",
    "\n",
    "        ensemble_comparison_rows.append({\n",
    "            'task': task,\n",
    "            'best_classifier': best_individual['classifier'],\n",
    "            'best_macro_f1': best_individual['macro_f1'],\n",
    "            'best_n_features': best_individual['n_features'],\n",
    "            'ensemble_macro_f1': ensemble_result['macro_f1'],\n",
    "            'improvement': ensemble_result['macro_f1'] - best_individual['macro_f1'],\n",
    "            'ensemble_accuracy': ensemble_result['accuracy'],\n",
    "            'best_accuracy': best_individual['accuracy'],\n",
    "        })\n",
    "\n",
    "    if ensemble_comparison_rows:\n",
    "        df_ensemble_comparison = pd.DataFrame(ensemble_comparison_rows)\n",
    "\n",
    "        print(\"\\nEnsemble vs Best Individual:\")\n",
    "        try:\n",
    "            from IPython.display import display\n",
    "            display(df_ensemble_comparison.drop(columns=['best_n_features', 'improvement']).style.format({\n",
    "                'best_macro_f1': '{:.4f}',\n",
    "                'ensemble_macro_f1': '{:.4f}',\n",
    "                'ensemble_accuracy': '{:.4f}',\n",
    "                'best_accuracy': '{:.4f}'\n",
    "            }))\n",
    "        except:\n",
    "            print(df_ensemble_comparison.drop(columns=['best_n_features', 'improvement']).to_string())\n",
    "\n",
    "        comparison_path = tables_dir / 'ensemble_comparison.csv'\n",
    "        df_ensemble_comparison.to_csv(comparison_path, index=False)\n",
    "        print(f\"\\n  ✓ Saved: {comparison_path.name}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY REPORT TABLES COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nAll tables saved to: {tables_dir}\")\n",
    "print(f\"  - Detailed summary: summary_detailed.csv\")\n",
    "print(f\"  - Pivot (Macro F1): summary_pivot_classifier_wise.csv\")\n",
    "print(f\"  - Pivot (Accuracy): summary_pivot_accuracy.csv\")\n",
    "print(f\"  - Per-task summaries: summary_{{task}}_individual.csv\")\n",
    "print(f\"  - Ensemble comparison: ensemble_comparison.csv\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "vuTo_kWVTo3c",
    "outputId": "f47ed556-f02f-4eba-a8e0-06e16d9663a0"
   },
   "execution_count": 28,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 6: GENERATE SUMMARY REPORT TABLES\n",
      "================================================================================\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "STEP 6.1: Individual Classifier Results\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "STEP 6.2: Ensemble Results (Weighted Average)\n",
      "--------------------------------------------------------------------------------\n",
      "  ✓ Added ensemble results for clarity\n",
      "  ✓ Added ensemble results for hierarchical_evasion_to_clarity\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "STEP 6.3: Detailed Summary Table (All Metrics)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Detailed Summary Table:\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7be50fad0740>"
      ],
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_f1728\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_f1728_level0_col0\" class=\"col_heading level0 col0\" >classifier</th>\n",
       "      <th id=\"T_f1728_level0_col1\" class=\"col_heading level0 col1\" >task</th>\n",
       "      <th id=\"T_f1728_level0_col2\" class=\"col_heading level0 col2\" >macro_f1</th>\n",
       "      <th id=\"T_f1728_level0_col3\" class=\"col_heading level0 col3\" >accuracy</th>\n",
       "      <th id=\"T_f1728_level0_col4\" class=\"col_heading level0 col4\" >macro_precision</th>\n",
       "      <th id=\"T_f1728_level0_col5\" class=\"col_heading level0 col5\" >macro_recall</th>\n",
       "      <th id=\"T_f1728_level0_col6\" class=\"col_heading level0 col6\" >weighted_f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_f1728_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_f1728_row0_col0\" class=\"data row0 col0\" >LogisticRegression</td>\n",
       "      <td id=\"T_f1728_row0_col1\" class=\"data row0 col1\" >clarity</td>\n",
       "      <td id=\"T_f1728_row0_col2\" class=\"data row0 col2\" >0.4571</td>\n",
       "      <td id=\"T_f1728_row0_col3\" class=\"data row0 col3\" >0.5390</td>\n",
       "      <td id=\"T_f1728_row0_col4\" class=\"data row0 col4\" >0.4482</td>\n",
       "      <td id=\"T_f1728_row0_col5\" class=\"data row0 col5\" >0.4884</td>\n",
       "      <td id=\"T_f1728_row0_col6\" class=\"data row0 col6\" >0.5587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f1728_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_f1728_row1_col0\" class=\"data row1 col0\" >LinearSVC</td>\n",
       "      <td id=\"T_f1728_row1_col1\" class=\"data row1 col1\" >clarity</td>\n",
       "      <td id=\"T_f1728_row1_col2\" class=\"data row1 col2\" >0.4608</td>\n",
       "      <td id=\"T_f1728_row1_col3\" class=\"data row1 col3\" >0.6623</td>\n",
       "      <td id=\"T_f1728_row1_col4\" class=\"data row1 col4\" >0.5119</td>\n",
       "      <td id=\"T_f1728_row1_col5\" class=\"data row1 col5\" >0.4566</td>\n",
       "      <td id=\"T_f1728_row1_col6\" class=\"data row1 col6\" >0.6178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f1728_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_f1728_row2_col0\" class=\"data row2 col0\" >RandomForest</td>\n",
       "      <td id=\"T_f1728_row2_col1\" class=\"data row2 col1\" >clarity</td>\n",
       "      <td id=\"T_f1728_row2_col2\" class=\"data row2 col2\" >0.4032</td>\n",
       "      <td id=\"T_f1728_row2_col3\" class=\"data row2 col3\" >0.6526</td>\n",
       "      <td id=\"T_f1728_row2_col4\" class=\"data row2 col4\" >0.4658</td>\n",
       "      <td id=\"T_f1728_row2_col5\" class=\"data row2 col5\" >0.4028</td>\n",
       "      <td id=\"T_f1728_row2_col6\" class=\"data row2 col6\" >0.5935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f1728_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_f1728_row3_col0\" class=\"data row3 col0\" >MLP</td>\n",
       "      <td id=\"T_f1728_row3_col1\" class=\"data row3 col1\" >clarity</td>\n",
       "      <td id=\"T_f1728_row3_col2\" class=\"data row3 col2\" >0.3554</td>\n",
       "      <td id=\"T_f1728_row3_col3\" class=\"data row3 col3\" >0.6688</td>\n",
       "      <td id=\"T_f1728_row3_col4\" class=\"data row3 col4\" >0.6879</td>\n",
       "      <td id=\"T_f1728_row3_col5\" class=\"data row3 col5\" >0.3721</td>\n",
       "      <td id=\"T_f1728_row3_col6\" class=\"data row3 col6\" >0.5733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f1728_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_f1728_row4_col0\" class=\"data row4 col0\" >XGBoost</td>\n",
       "      <td id=\"T_f1728_row4_col1\" class=\"data row4 col1\" >clarity</td>\n",
       "      <td id=\"T_f1728_row4_col2\" class=\"data row4 col2\" >0.4006</td>\n",
       "      <td id=\"T_f1728_row4_col3\" class=\"data row4 col3\" >0.6558</td>\n",
       "      <td id=\"T_f1728_row4_col4\" class=\"data row4 col4\" >0.4423</td>\n",
       "      <td id=\"T_f1728_row4_col5\" class=\"data row4 col5\" >0.3996</td>\n",
       "      <td id=\"T_f1728_row4_col6\" class=\"data row4 col6\" >0.6169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f1728_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_f1728_row5_col0\" class=\"data row5 col0\" >LightGBM</td>\n",
       "      <td id=\"T_f1728_row5_col1\" class=\"data row5 col1\" >clarity</td>\n",
       "      <td id=\"T_f1728_row5_col2\" class=\"data row5 col2\" >0.4522</td>\n",
       "      <td id=\"T_f1728_row5_col3\" class=\"data row5 col3\" >0.6786</td>\n",
       "      <td id=\"T_f1728_row5_col4\" class=\"data row5 col4\" >0.6142</td>\n",
       "      <td id=\"T_f1728_row5_col5\" class=\"data row5 col5\" >0.4313</td>\n",
       "      <td id=\"T_f1728_row5_col6\" class=\"data row5 col6\" >0.6266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f1728_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_f1728_row6_col0\" class=\"data row6 col0\" >LogisticRegression</td>\n",
       "      <td id=\"T_f1728_row6_col1\" class=\"data row6 col1\" >hierarchical_evasion_to_clarity</td>\n",
       "      <td id=\"T_f1728_row6_col2\" class=\"data row6 col2\" >0.4464</td>\n",
       "      <td id=\"T_f1728_row6_col3\" class=\"data row6 col3\" >0.5055</td>\n",
       "      <td id=\"T_f1728_row6_col4\" class=\"data row6 col4\" >0.4406</td>\n",
       "      <td id=\"T_f1728_row6_col5\" class=\"data row6 col5\" >0.4857</td>\n",
       "      <td id=\"T_f1728_row6_col6\" class=\"data row6 col6\" >0.5261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f1728_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_f1728_row7_col0\" class=\"data row7 col0\" >LinearSVC</td>\n",
       "      <td id=\"T_f1728_row7_col1\" class=\"data row7 col1\" >hierarchical_evasion_to_clarity</td>\n",
       "      <td id=\"T_f1728_row7_col2\" class=\"data row7 col2\" >0.4597</td>\n",
       "      <td id=\"T_f1728_row7_col3\" class=\"data row7 col3\" >0.6436</td>\n",
       "      <td id=\"T_f1728_row7_col4\" class=\"data row7 col4\" >0.5175</td>\n",
       "      <td id=\"T_f1728_row7_col5\" class=\"data row7 col5\" >0.4539</td>\n",
       "      <td id=\"T_f1728_row7_col6\" class=\"data row7 col6\" >0.6016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f1728_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_f1728_row8_col0\" class=\"data row8 col0\" >RandomForest</td>\n",
       "      <td id=\"T_f1728_row8_col1\" class=\"data row8 col1\" >hierarchical_evasion_to_clarity</td>\n",
       "      <td id=\"T_f1728_row8_col2\" class=\"data row8 col2\" >0.3966</td>\n",
       "      <td id=\"T_f1728_row8_col3\" class=\"data row8 col3\" >0.6218</td>\n",
       "      <td id=\"T_f1728_row8_col4\" class=\"data row8 col4\" >0.4588</td>\n",
       "      <td id=\"T_f1728_row8_col5\" class=\"data row8 col5\" >0.4003</td>\n",
       "      <td id=\"T_f1728_row8_col6\" class=\"data row8 col6\" >0.5599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f1728_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_f1728_row9_col0\" class=\"data row9 col0\" >MLP</td>\n",
       "      <td id=\"T_f1728_row9_col1\" class=\"data row9 col1\" >hierarchical_evasion_to_clarity</td>\n",
       "      <td id=\"T_f1728_row9_col2\" class=\"data row9 col2\" >0.3492</td>\n",
       "      <td id=\"T_f1728_row9_col3\" class=\"data row9 col3\" >0.6400</td>\n",
       "      <td id=\"T_f1728_row9_col4\" class=\"data row9 col4\" >0.6881</td>\n",
       "      <td id=\"T_f1728_row9_col5\" class=\"data row9 col5\" >0.3725</td>\n",
       "      <td id=\"T_f1728_row9_col6\" class=\"data row9 col6\" >0.5382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f1728_level0_row10\" class=\"row_heading level0 row10\" >10</th>\n",
       "      <td id=\"T_f1728_row10_col0\" class=\"data row10 col0\" >XGBoost</td>\n",
       "      <td id=\"T_f1728_row10_col1\" class=\"data row10 col1\" >hierarchical_evasion_to_clarity</td>\n",
       "      <td id=\"T_f1728_row10_col2\" class=\"data row10 col2\" >0.3933</td>\n",
       "      <td id=\"T_f1728_row10_col3\" class=\"data row10 col3\" >0.6291</td>\n",
       "      <td id=\"T_f1728_row10_col4\" class=\"data row10 col4\" >0.4603</td>\n",
       "      <td id=\"T_f1728_row10_col5\" class=\"data row10 col5\" >0.3954</td>\n",
       "      <td id=\"T_f1728_row10_col6\" class=\"data row10 col6\" >0.5844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f1728_level0_row11\" class=\"row_heading level0 row11\" >11</th>\n",
       "      <td id=\"T_f1728_row11_col0\" class=\"data row11 col0\" >LightGBM</td>\n",
       "      <td id=\"T_f1728_row11_col1\" class=\"data row11 col1\" >hierarchical_evasion_to_clarity</td>\n",
       "      <td id=\"T_f1728_row11_col2\" class=\"data row11 col2\" >0.4517</td>\n",
       "      <td id=\"T_f1728_row11_col3\" class=\"data row11 col3\" >0.6691</td>\n",
       "      <td id=\"T_f1728_row11_col4\" class=\"data row11 col4\" >0.5859</td>\n",
       "      <td id=\"T_f1728_row11_col5\" class=\"data row11 col5\" >0.4396</td>\n",
       "      <td id=\"T_f1728_row11_col6\" class=\"data row11 col6\" >0.6111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f1728_level0_row12\" class=\"row_heading level0 row12\" >12</th>\n",
       "      <td id=\"T_f1728_row12_col0\" class=\"data row12 col0\" >Ensemble (Weighted)</td>\n",
       "      <td id=\"T_f1728_row12_col1\" class=\"data row12 col1\" >clarity</td>\n",
       "      <td id=\"T_f1728_row12_col2\" class=\"data row12 col2\" >0.4485</td>\n",
       "      <td id=\"T_f1728_row12_col3\" class=\"data row12 col3\" >0.6851</td>\n",
       "      <td id=\"T_f1728_row12_col4\" class=\"data row12 col4\" >0.5828</td>\n",
       "      <td id=\"T_f1728_row12_col5\" class=\"data row12 col5\" >0.4370</td>\n",
       "      <td id=\"T_f1728_row12_col6\" class=\"data row12 col6\" >0.6220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f1728_level0_row13\" class=\"row_heading level0 row13\" >13</th>\n",
       "      <td id=\"T_f1728_row13_col0\" class=\"data row13 col0\" >Ensemble (Weighted)</td>\n",
       "      <td id=\"T_f1728_row13_col1\" class=\"data row13 col1\" >hierarchical_evasion_to_clarity</td>\n",
       "      <td id=\"T_f1728_row13_col2\" class=\"data row13 col2\" >0.4336</td>\n",
       "      <td id=\"T_f1728_row13_col3\" class=\"data row13 col3\" >0.6655</td>\n",
       "      <td id=\"T_f1728_row13_col4\" class=\"data row13 col4\" >0.5759</td>\n",
       "      <td id=\"T_f1728_row13_col5\" class=\"data row13 col5\" >0.4304</td>\n",
       "      <td id=\"T_f1728_row13_col6\" class=\"data row13 col6\" >0.5984</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "  ✓ Saved detailed table: summary_detailed.csv\n",
      "  ✓ Saved HTML: summary_detailed.html\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "STEP 6.4: Pivot Table - Classifier × Task (Macro F1)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Pivot Table (Macro F1):\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7be51b347a10>"
      ],
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_c1756\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >task</th>\n",
       "      <th id=\"T_c1756_level0_col0\" class=\"col_heading level0 col0\" >clarity</th>\n",
       "      <th id=\"T_c1756_level0_col1\" class=\"col_heading level0 col1\" >hierarchical_evasion_to_clarity</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >classifier</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "      <th class=\"blank col1\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_c1756_level0_row0\" class=\"row_heading level0 row0\" >Ensemble (Weighted)</th>\n",
       "      <td id=\"T_c1756_row0_col0\" class=\"data row0 col0\" >0.4485</td>\n",
       "      <td id=\"T_c1756_row0_col1\" class=\"data row0 col1\" >0.4336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c1756_level0_row1\" class=\"row_heading level0 row1\" >LightGBM</th>\n",
       "      <td id=\"T_c1756_row1_col0\" class=\"data row1 col0\" >0.4522</td>\n",
       "      <td id=\"T_c1756_row1_col1\" class=\"data row1 col1\" >0.4517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c1756_level0_row2\" class=\"row_heading level0 row2\" >LinearSVC</th>\n",
       "      <td id=\"T_c1756_row2_col0\" class=\"data row2 col0\" >0.4608</td>\n",
       "      <td id=\"T_c1756_row2_col1\" class=\"data row2 col1\" >0.4597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c1756_level0_row3\" class=\"row_heading level0 row3\" >LogisticRegression</th>\n",
       "      <td id=\"T_c1756_row3_col0\" class=\"data row3 col0\" >0.4571</td>\n",
       "      <td id=\"T_c1756_row3_col1\" class=\"data row3 col1\" >0.4464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c1756_level0_row4\" class=\"row_heading level0 row4\" >MLP</th>\n",
       "      <td id=\"T_c1756_row4_col0\" class=\"data row4 col0\" >0.3554</td>\n",
       "      <td id=\"T_c1756_row4_col1\" class=\"data row4 col1\" >0.3492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c1756_level0_row5\" class=\"row_heading level0 row5\" >RandomForest</th>\n",
       "      <td id=\"T_c1756_row5_col0\" class=\"data row5 col0\" >0.4032</td>\n",
       "      <td id=\"T_c1756_row5_col1\" class=\"data row5 col1\" >0.3966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c1756_level0_row6\" class=\"row_heading level0 row6\" >XGBoost</th>\n",
       "      <td id=\"T_c1756_row6_col0\" class=\"data row6 col0\" >0.4006</td>\n",
       "      <td id=\"T_c1756_row6_col1\" class=\"data row6 col1\" >0.3933</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "  ✓ Saved pivot table: summary_pivot_classifier_wise.csv\n",
      "  ✓ Saved HTML: summary_pivot_classifier_wise.html\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "STEP 6.5: Pivot Table - Classifier × Task (Accuracy)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Pivot Table (Accuracy):\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7be50fd46030>"
      ],
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_84619\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >task</th>\n",
       "      <th id=\"T_84619_level0_col0\" class=\"col_heading level0 col0\" >clarity</th>\n",
       "      <th id=\"T_84619_level0_col1\" class=\"col_heading level0 col1\" >hierarchical_evasion_to_clarity</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >classifier</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "      <th class=\"blank col1\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_84619_level0_row0\" class=\"row_heading level0 row0\" >Ensemble (Weighted)</th>\n",
       "      <td id=\"T_84619_row0_col0\" class=\"data row0 col0\" >0.6851</td>\n",
       "      <td id=\"T_84619_row0_col1\" class=\"data row0 col1\" >0.6655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_84619_level0_row1\" class=\"row_heading level0 row1\" >LightGBM</th>\n",
       "      <td id=\"T_84619_row1_col0\" class=\"data row1 col0\" >0.6786</td>\n",
       "      <td id=\"T_84619_row1_col1\" class=\"data row1 col1\" >0.6691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_84619_level0_row2\" class=\"row_heading level0 row2\" >LinearSVC</th>\n",
       "      <td id=\"T_84619_row2_col0\" class=\"data row2 col0\" >0.6623</td>\n",
       "      <td id=\"T_84619_row2_col1\" class=\"data row2 col1\" >0.6436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_84619_level0_row3\" class=\"row_heading level0 row3\" >LogisticRegression</th>\n",
       "      <td id=\"T_84619_row3_col0\" class=\"data row3 col0\" >0.5390</td>\n",
       "      <td id=\"T_84619_row3_col1\" class=\"data row3 col1\" >0.5055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_84619_level0_row4\" class=\"row_heading level0 row4\" >MLP</th>\n",
       "      <td id=\"T_84619_row4_col0\" class=\"data row4 col0\" >0.6688</td>\n",
       "      <td id=\"T_84619_row4_col1\" class=\"data row4 col1\" >0.6400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_84619_level0_row5\" class=\"row_heading level0 row5\" >RandomForest</th>\n",
       "      <td id=\"T_84619_row5_col0\" class=\"data row5 col0\" >0.6526</td>\n",
       "      <td id=\"T_84619_row5_col1\" class=\"data row5 col1\" >0.6218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_84619_level0_row6\" class=\"row_heading level0 row6\" >XGBoost</th>\n",
       "      <td id=\"T_84619_row6_col0\" class=\"data row6 col0\" >0.6558</td>\n",
       "      <td id=\"T_84619_row6_col1\" class=\"data row6 col1\" >0.6291</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "  ✓ Saved pivot table: summary_pivot_accuracy.csv\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "STEP 6.6: Summary by Task (Individual Classifiers)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "  CLARITY - Individual Classifiers:\n",
      "  ------------------------------------------------------------\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7be50eae8710>"
      ],
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_e8096\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_e8096_level0_col0\" class=\"col_heading level0 col0\" >classifier</th>\n",
       "      <th id=\"T_e8096_level0_col1\" class=\"col_heading level0 col1\" >macro_f1</th>\n",
       "      <th id=\"T_e8096_level0_col2\" class=\"col_heading level0 col2\" >accuracy</th>\n",
       "      <th id=\"T_e8096_level0_col3\" class=\"col_heading level0 col3\" >macro_precision</th>\n",
       "      <th id=\"T_e8096_level0_col4\" class=\"col_heading level0 col4\" >macro_recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_e8096_level0_row0\" class=\"row_heading level0 row0\" >1</th>\n",
       "      <td id=\"T_e8096_row0_col0\" class=\"data row0 col0\" >LinearSVC</td>\n",
       "      <td id=\"T_e8096_row0_col1\" class=\"data row0 col1\" >0.4608</td>\n",
       "      <td id=\"T_e8096_row0_col2\" class=\"data row0 col2\" >0.6623</td>\n",
       "      <td id=\"T_e8096_row0_col3\" class=\"data row0 col3\" >0.5119</td>\n",
       "      <td id=\"T_e8096_row0_col4\" class=\"data row0 col4\" >0.4566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e8096_level0_row1\" class=\"row_heading level0 row1\" >0</th>\n",
       "      <td id=\"T_e8096_row1_col0\" class=\"data row1 col0\" >LogisticRegression</td>\n",
       "      <td id=\"T_e8096_row1_col1\" class=\"data row1 col1\" >0.4571</td>\n",
       "      <td id=\"T_e8096_row1_col2\" class=\"data row1 col2\" >0.5390</td>\n",
       "      <td id=\"T_e8096_row1_col3\" class=\"data row1 col3\" >0.4482</td>\n",
       "      <td id=\"T_e8096_row1_col4\" class=\"data row1 col4\" >0.4884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e8096_level0_row2\" class=\"row_heading level0 row2\" >5</th>\n",
       "      <td id=\"T_e8096_row2_col0\" class=\"data row2 col0\" >LightGBM</td>\n",
       "      <td id=\"T_e8096_row2_col1\" class=\"data row2 col1\" >0.4522</td>\n",
       "      <td id=\"T_e8096_row2_col2\" class=\"data row2 col2\" >0.6786</td>\n",
       "      <td id=\"T_e8096_row2_col3\" class=\"data row2 col3\" >0.6142</td>\n",
       "      <td id=\"T_e8096_row2_col4\" class=\"data row2 col4\" >0.4313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e8096_level0_row3\" class=\"row_heading level0 row3\" >2</th>\n",
       "      <td id=\"T_e8096_row3_col0\" class=\"data row3 col0\" >RandomForest</td>\n",
       "      <td id=\"T_e8096_row3_col1\" class=\"data row3 col1\" >0.4032</td>\n",
       "      <td id=\"T_e8096_row3_col2\" class=\"data row3 col2\" >0.6526</td>\n",
       "      <td id=\"T_e8096_row3_col3\" class=\"data row3 col3\" >0.4658</td>\n",
       "      <td id=\"T_e8096_row3_col4\" class=\"data row3 col4\" >0.4028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e8096_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_e8096_row4_col0\" class=\"data row4 col0\" >XGBoost</td>\n",
       "      <td id=\"T_e8096_row4_col1\" class=\"data row4 col1\" >0.4006</td>\n",
       "      <td id=\"T_e8096_row4_col2\" class=\"data row4 col2\" >0.6558</td>\n",
       "      <td id=\"T_e8096_row4_col3\" class=\"data row4 col3\" >0.4423</td>\n",
       "      <td id=\"T_e8096_row4_col4\" class=\"data row4 col4\" >0.3996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e8096_level0_row5\" class=\"row_heading level0 row5\" >3</th>\n",
       "      <td id=\"T_e8096_row5_col0\" class=\"data row5 col0\" >MLP</td>\n",
       "      <td id=\"T_e8096_row5_col1\" class=\"data row5 col1\" >0.3554</td>\n",
       "      <td id=\"T_e8096_row5_col2\" class=\"data row5 col2\" >0.6688</td>\n",
       "      <td id=\"T_e8096_row5_col3\" class=\"data row5 col3\" >0.6879</td>\n",
       "      <td id=\"T_e8096_row5_col4\" class=\"data row5 col4\" >0.3721</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "  ✓ Saved: summary_clarity_individual.csv\n",
      "\n",
      "  HIERARCHICAL_EVASION_TO_CLARITY - Individual Classifiers:\n",
      "  ------------------------------------------------------------\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7be50fd0a750>"
      ],
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_e9b2d\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_e9b2d_level0_col0\" class=\"col_heading level0 col0\" >classifier</th>\n",
       "      <th id=\"T_e9b2d_level0_col1\" class=\"col_heading level0 col1\" >macro_f1</th>\n",
       "      <th id=\"T_e9b2d_level0_col2\" class=\"col_heading level0 col2\" >accuracy</th>\n",
       "      <th id=\"T_e9b2d_level0_col3\" class=\"col_heading level0 col3\" >macro_precision</th>\n",
       "      <th id=\"T_e9b2d_level0_col4\" class=\"col_heading level0 col4\" >macro_recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_e9b2d_level0_row0\" class=\"row_heading level0 row0\" >7</th>\n",
       "      <td id=\"T_e9b2d_row0_col0\" class=\"data row0 col0\" >LinearSVC</td>\n",
       "      <td id=\"T_e9b2d_row0_col1\" class=\"data row0 col1\" >0.4597</td>\n",
       "      <td id=\"T_e9b2d_row0_col2\" class=\"data row0 col2\" >0.6436</td>\n",
       "      <td id=\"T_e9b2d_row0_col3\" class=\"data row0 col3\" >0.5175</td>\n",
       "      <td id=\"T_e9b2d_row0_col4\" class=\"data row0 col4\" >0.4539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e9b2d_level0_row1\" class=\"row_heading level0 row1\" >11</th>\n",
       "      <td id=\"T_e9b2d_row1_col0\" class=\"data row1 col0\" >LightGBM</td>\n",
       "      <td id=\"T_e9b2d_row1_col1\" class=\"data row1 col1\" >0.4517</td>\n",
       "      <td id=\"T_e9b2d_row1_col2\" class=\"data row1 col2\" >0.6691</td>\n",
       "      <td id=\"T_e9b2d_row1_col3\" class=\"data row1 col3\" >0.5859</td>\n",
       "      <td id=\"T_e9b2d_row1_col4\" class=\"data row1 col4\" >0.4396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e9b2d_level0_row2\" class=\"row_heading level0 row2\" >6</th>\n",
       "      <td id=\"T_e9b2d_row2_col0\" class=\"data row2 col0\" >LogisticRegression</td>\n",
       "      <td id=\"T_e9b2d_row2_col1\" class=\"data row2 col1\" >0.4464</td>\n",
       "      <td id=\"T_e9b2d_row2_col2\" class=\"data row2 col2\" >0.5055</td>\n",
       "      <td id=\"T_e9b2d_row2_col3\" class=\"data row2 col3\" >0.4406</td>\n",
       "      <td id=\"T_e9b2d_row2_col4\" class=\"data row2 col4\" >0.4857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e9b2d_level0_row3\" class=\"row_heading level0 row3\" >8</th>\n",
       "      <td id=\"T_e9b2d_row3_col0\" class=\"data row3 col0\" >RandomForest</td>\n",
       "      <td id=\"T_e9b2d_row3_col1\" class=\"data row3 col1\" >0.3966</td>\n",
       "      <td id=\"T_e9b2d_row3_col2\" class=\"data row3 col2\" >0.6218</td>\n",
       "      <td id=\"T_e9b2d_row3_col3\" class=\"data row3 col3\" >0.4588</td>\n",
       "      <td id=\"T_e9b2d_row3_col4\" class=\"data row3 col4\" >0.4003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e9b2d_level0_row4\" class=\"row_heading level0 row4\" >10</th>\n",
       "      <td id=\"T_e9b2d_row4_col0\" class=\"data row4 col0\" >XGBoost</td>\n",
       "      <td id=\"T_e9b2d_row4_col1\" class=\"data row4 col1\" >0.3933</td>\n",
       "      <td id=\"T_e9b2d_row4_col2\" class=\"data row4 col2\" >0.6291</td>\n",
       "      <td id=\"T_e9b2d_row4_col3\" class=\"data row4 col3\" >0.4603</td>\n",
       "      <td id=\"T_e9b2d_row4_col4\" class=\"data row4 col4\" >0.3954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e9b2d_level0_row5\" class=\"row_heading level0 row5\" >9</th>\n",
       "      <td id=\"T_e9b2d_row5_col0\" class=\"data row5 col0\" >MLP</td>\n",
       "      <td id=\"T_e9b2d_row5_col1\" class=\"data row5 col1\" >0.3492</td>\n",
       "      <td id=\"T_e9b2d_row5_col2\" class=\"data row5 col2\" >0.6400</td>\n",
       "      <td id=\"T_e9b2d_row5_col3\" class=\"data row5 col3\" >0.6881</td>\n",
       "      <td id=\"T_e9b2d_row5_col4\" class=\"data row5 col4\" >0.3725</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "  ✓ Saved: summary_hierarchical_evasion_to_clarity_individual.csv\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "STEP 6.7: Ensemble vs Best Individual Classifier\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Ensemble vs Best Individual:\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7be50eaccf50>"
      ],
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_f21bb\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_f21bb_level0_col0\" class=\"col_heading level0 col0\" >task</th>\n",
       "      <th id=\"T_f21bb_level0_col1\" class=\"col_heading level0 col1\" >best_classifier</th>\n",
       "      <th id=\"T_f21bb_level0_col2\" class=\"col_heading level0 col2\" >best_macro_f1</th>\n",
       "      <th id=\"T_f21bb_level0_col3\" class=\"col_heading level0 col3\" >ensemble_macro_f1</th>\n",
       "      <th id=\"T_f21bb_level0_col4\" class=\"col_heading level0 col4\" >ensemble_accuracy</th>\n",
       "      <th id=\"T_f21bb_level0_col5\" class=\"col_heading level0 col5\" >best_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_f21bb_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_f21bb_row0_col0\" class=\"data row0 col0\" >clarity</td>\n",
       "      <td id=\"T_f21bb_row0_col1\" class=\"data row0 col1\" >LinearSVC</td>\n",
       "      <td id=\"T_f21bb_row0_col2\" class=\"data row0 col2\" >0.4608</td>\n",
       "      <td id=\"T_f21bb_row0_col3\" class=\"data row0 col3\" >0.4485</td>\n",
       "      <td id=\"T_f21bb_row0_col4\" class=\"data row0 col4\" >0.6851</td>\n",
       "      <td id=\"T_f21bb_row0_col5\" class=\"data row0 col5\" >0.6623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f21bb_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_f21bb_row1_col0\" class=\"data row1 col0\" >hierarchical_evasion_to_clarity</td>\n",
       "      <td id=\"T_f21bb_row1_col1\" class=\"data row1 col1\" >LinearSVC</td>\n",
       "      <td id=\"T_f21bb_row1_col2\" class=\"data row1 col2\" >0.4597</td>\n",
       "      <td id=\"T_f21bb_row1_col3\" class=\"data row1 col3\" >0.4336</td>\n",
       "      <td id=\"T_f21bb_row1_col4\" class=\"data row1 col4\" >0.6655</td>\n",
       "      <td id=\"T_f21bb_row1_col5\" class=\"data row1 col5\" >0.6436</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "  ✓ Saved: ensemble_comparison.csv\n",
      "\n",
      "================================================================================\n",
      "SUMMARY REPORT TABLES COMPLETE\n",
      "================================================================================\n",
      "\n",
      "All tables saved to: /content/drive/MyDrive/semeval_data/results/FinalResultsType2/classifier_specific/tables\n",
      "  - Detailed summary: summary_detailed.csv\n",
      "  - Pivot (Macro F1): summary_pivot_classifier_wise.csv\n",
      "  - Pivot (Accuracy): summary_pivot_accuracy.csv\n",
      "  - Per-task summaries: summary_{task}_individual.csv\n",
      "  - Ensemble comparison: ensemble_comparison.csv\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "wtmlGcJZWjrC"
   },
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}