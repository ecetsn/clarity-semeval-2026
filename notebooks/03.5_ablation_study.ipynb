{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Ablation Study: Feature Selection and Ranking\n",
        "\n",
        "================================================================================\n",
        "PURPOSE: Comprehensive feature ablation study to identify optimal feature subsets\n",
        "================================================================================\n",
        "\n",
        "This notebook performs comprehensive ablation studies on Context Tree features\n",
        "to identify the most important features for each model and task combination.\n",
        "The goal is to maximize Macro F1-score by selecting optimal feature subsets.\n",
        "\n",
        "**Workflow:**\n",
        "1. Load features and labels from persistent storage (saved by 03_train_evaluate.ipynb)\n",
        "2. Single-Feature Ablation: Evaluate each feature individually across all models and classifiers\n",
        "3. Global Feature Ranking: Aggregate results to rank features by importance\n",
        "4. Top-K Seed Features: Identify top-performing features for greedy selection\n",
        "5. Greedy Forward Selection: Iteratively add best features to maximize Macro F1\n",
        "6. Save selected feature sets for use in subsequent notebooks\n",
        "\n",
        "**Methods:**\n",
        "- Single-Feature Ablation: Test each of 19 features individually\n",
        "- Global Feature Ranking: Aggregate across all model×classifier combinations\n",
        "- Greedy Forward Selection: Iteratively add features that maximize Macro F1\n",
        "\n",
        "**Output:**\n",
        "- Feature rankings and ablation results saved to Google Drive\n",
        "- Selected feature sets saved for each model×task combination\n",
        "- Comprehensive tables and visualizations\n",
        "\n",
        "================================================================================\n",
        "INPUTS (What this notebook loads)\n",
        "================================================================================\n",
        "\n",
        "**From Google Drive:**\n",
        "- Feature matrices: `features/raw/X_{split}_{model}_{task}.npy`\n",
        "  - For each model (bert, roberta, deberta, xlnet, bert_political, bert_ambiguity)\n",
        "  - For each task (clarity, evasion)\n",
        "  - For Train and Dev splits\n",
        "- Dataset splits: `splits/dataset_splits_{task}.pkl`\n",
        "  - For label extraction\n",
        "\n",
        "**From GitHub:**\n",
        "- Feature metadata: `metadata/features_{split}_{model}_{task}.json`\n",
        "  - Contains feature names (19 features)\n",
        "\n",
        "================================================================================\n",
        "OUTPUTS (What this notebook saves)\n",
        "================================================================================\n",
        "\n",
        "**To Google Drive:**\n",
        "- Ablation results: `results/ablation/single_feature_{model}_{task}.csv`\n",
        "- Feature rankings: `results/ablation/feature_ranking_{model}_{task}.csv`\n",
        "- Selected features: `results/ablation/selected_features_{model}_{task}.json`\n",
        "- Greedy trajectories: `results/ablation/greedy_trajectory_{model}_{task}.csv`\n",
        "\n",
        "**To GitHub:**\n",
        "- Ablation metadata: `results/ablation_metadata_{model}_{task}.json`\n",
        "\n",
        "**What gets passed to next notebook:**\n",
        "- Selected feature indices for each model×task combination\n",
        "- Feature rankings for analysis\n",
        "- Optimal feature subsets for Early Fusion experiments\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ============================================================================\n",
        "# SETUP: Repository Clone, Drive Mount, and Path Configuration\n",
        "# ============================================================================\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import shutil\n",
        "import os\n",
        "import subprocess\n",
        "import time\n",
        "import requests\n",
        "import zipfile\n",
        "import sys\n",
        "from pathlib import Path\n",
        "from google.colab import drive\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Repository configuration\n",
        "repo_dir = '/content/semeval-context-tree-modular'\n",
        "repo_url = 'https://github.com/EonTechie/semeval-context-tree-modular.git'\n",
        "zip_url = 'https://github.com/EonTechie/semeval-context-tree-modular/archive/refs/heads/main.zip'\n",
        "\n",
        "# Clone repository (if not already present)\n",
        "if not os.path.exists(repo_dir):\n",
        "    print(\"Cloning repository from GitHub...\")\n",
        "    max_retries = 2\n",
        "    clone_success = False\n",
        "    \n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            result = subprocess.run(\n",
        "                ['git', 'clone', repo_url],\n",
        "                cwd='/content',\n",
        "                capture_output=True,\n",
        "                text=True,\n",
        "                timeout=60\n",
        "            )\n",
        "            if result.returncode == 0:\n",
        "                print(\"Repository cloned successfully via git\")\n",
        "                clone_success = True\n",
        "                break\n",
        "            else:\n",
        "                if attempt < max_retries - 1:\n",
        "                    time.sleep(3)\n",
        "        except Exception as e:\n",
        "            if attempt < max_retries - 1:\n",
        "                time.sleep(3)\n",
        "    \n",
        "    # Fallback: Download as ZIP if git clone fails\n",
        "    if not clone_success:\n",
        "        print(\"Git clone failed. Downloading repository as ZIP archive...\")\n",
        "        zip_path = '/tmp/repo.zip'\n",
        "        try:\n",
        "            response = requests.get(zip_url, stream=True, timeout=60)\n",
        "            response.raise_for_status()\n",
        "            with open(zip_path, 'wb') as f:\n",
        "                for chunk in response.iter_content(chunk_size=8192):\n",
        "                    f.write(chunk)\n",
        "            with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "                zip_ref.extractall('/content')\n",
        "            extracted_dir = '/content/semeval-context-tree-modular-main'\n",
        "            if os.path.exists(extracted_dir):\n",
        "                os.rename(extracted_dir, repo_dir)\n",
        "            os.remove(zip_path)\n",
        "            print(\"Repository downloaded and extracted successfully\")\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(f\"Failed to obtain repository: {e}\")\n",
        "\n",
        "# Mount Google Drive (if not already mounted)\n",
        "try:\n",
        "    drive.mount('/content/drive', force_remount=False)\n",
        "except Exception:\n",
        "    pass  # Already mounted\n",
        "\n",
        "# Configure paths\n",
        "BASE_PATH = Path('/content/semeval-context-tree-modular')\n",
        "DATA_PATH = Path('/content/drive/MyDrive/semeval_data')\n",
        "\n",
        "# Verify repository structure exists\n",
        "if not BASE_PATH.exists():\n",
        "    raise RuntimeError(f\"Repository directory not found: {BASE_PATH}\")\n",
        "if not (BASE_PATH / 'src').exists():\n",
        "    raise RuntimeError(f\"src directory not found in repository: {BASE_PATH / 'src'}\")\n",
        "if not (BASE_PATH / 'src' / 'storage' / 'manager.py').exists():\n",
        "    raise RuntimeError(f\"Required file not found: {BASE_PATH / 'src' / 'storage' / 'manager.py'}\")\n",
        "\n",
        "# Add repository to Python path\n",
        "sys.path.insert(0, str(BASE_PATH))\n",
        "\n",
        "# Verify imports work\n",
        "try:\n",
        "    from src.storage.manager import StorageManager\n",
        "    from src.models.classifiers import get_classifier_dict\n",
        "    from sklearn.metrics import f1_score\n",
        "    from sklearn.pipeline import Pipeline\n",
        "    from sklearn.preprocessing import StandardScaler\n",
        "    from sklearn.base import clone\n",
        "except ImportError as e:\n",
        "    raise ImportError(\n",
        "        f\"Failed to import required modules. \"\n",
        "        f\"Repository path: {BASE_PATH}, \"\n",
        "        f\"Python path: {sys.path[:3]}, \"\n",
        "        f\"Error: {e}\"\n",
        "    )\n",
        "\n",
        "# Initialize StorageManager\n",
        "storage = StorageManager(\n",
        "    base_path=str(BASE_PATH),\n",
        "    data_path=str(DATA_PATH),\n",
        "    github_path=str(BASE_PATH)\n",
        ")\n",
        "\n",
        "# Create ablation results directory\n",
        "ablation_dir = DATA_PATH / 'results' / 'ablation'\n",
        "ablation_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(\"Setup complete\")\n",
        "print(f\"  Repository: {BASE_PATH}\")\n",
        "print(f\"  Data storage: {DATA_PATH}\")\n",
        "print(f\"  Ablation results: {ablation_dir}\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
