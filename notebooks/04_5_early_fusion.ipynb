{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size: 10px;\">\n",
    "\n",
    "# Early Fusion: Concatenated Multi-Model Features\n",
    "\n",
    "================================================================================\n",
    "PURPOSE: Perform early fusion by concatenating features from all models\n",
    "================================================================================\n",
    "\n",
    "This notebook implements early fusion at the feature level by concatenating\n",
    "Context Tree features extracted from multiple transformer models. This approach\n",
    "combines complementary information from different models to potentially improve\n",
    "classification performance.\n",
    "\n",
    "**Fusion Strategy:**\n",
    "- **Early Fusion (Feature-level)**: Concatenate 60 distinct features\n",
    "- Fused feature vector = [18 Model-Independent (1×) | 6 Models × 7 Model-Dependent]\n",
    "- Total feature dimension = 18 + (6 × 7) = 60 features\n",
    "- Model-independent features loaded once (shared across all models)\n",
    "- Model-dependent features extracted from each model (7 per model)\n",
    "\n",
    "**Workflow:**\n",
    "1. Load model-independent features once (18 features, shared across all models)\n",
    "2. Load full features from all models (25 features each, saved by 02_feature_extraction_separate.ipynb)\n",
    "3. Extract model-dependent portion from each model (first 7 features from each 25-feature vector)\n",
    "4. Concatenate: [18 Model-Independent | 6 Models × 7 Model-Dependent] = 60 features\n",
    "5. Train all 6 classifiers on fused 60-feature vectors\n",
    "6. Evaluate on Dev set and compare with individual model performance\n",
    "7. Save fused features, predictions, and results\n",
    "\n",
    "**Output:**\n",
    "- Fused feature matrices saved to Google Drive\n",
    "- Predictions and probabilities for fused features\n",
    "- Results tables and evaluation plots\n",
    "- Comparison with individual model results\n",
    "\n",
    "================================================================================\n",
    "INPUTS (What this notebook loads)\n",
    "================================================================================\n",
    "\n",
    "**From GitHub:**\n",
    "- Repository code (cloned automatically if not present)\n",
    "- Source modules from `src/` directory:\n",
    "  - `src.storage.manager` (StorageManager)\n",
    "  - `src.features.fusion` (feature fusion functions)\n",
    "  - `src.models.trainer` (training and evaluation functions)\n",
    "  - `src.models.classifiers` (classifier definitions)\n",
    "\n",
    "**From Google Drive:**\n",
    "- Dataset splits: `splits/dataset_splits.pkl`\n",
    "  - Train split (for label extraction)\n",
    "  - Dev split (for label extraction)\n",
    "- Model-independent features: `features/model_independent/X_{split}_{task}_independent.npy`\n",
    "  - 18 features, loaded once per task (shared across all models)\n",
    "  - Loaded via `storage.load_model_independent_features(split, task)`\n",
    "- Feature matrices: `features/raw/X_{split}_{model}_{task}.npy`\n",
    "  - For all models (bert, bert_political, bert_ambiguity, roberta, deberta, xlnet)\n",
    "  - For each task (clarity, evasion)\n",
    "  - For Train and Dev splits\n",
    "  - 25 features each (7 model-dependent + 18 model-independent)\n",
    "  - Loaded via `storage.load_features(model, task, split)`\n",
    "- Feature metadata: `metadata/features_{split}_{model}_{task}.json`\n",
    "  - For feature name extraction\n",
    "\n",
    "**From HuggingFace Hub:**\n",
    "- Nothing (all features already extracted)\n",
    "\n",
    "================================================================================\n",
    "OUTPUTS (What this notebook saves)\n",
    "================================================================================\n",
    "\n",
    "**To Google Drive:**\n",
    "- Fused features: `features/fused/X_{split}_fused_{models}_{task}.npy`\n",
    "  - Concatenated 60 features: 18 model-independent + 42 model-dependent (6×7)\n",
    "  - For Train and Dev splits\n",
    "  - Shape: (N_samples, 60)\n",
    "- Predictions: `predictions/pred_{split}_fused_{classifier}_{task}.npy`\n",
    "  - Hard label predictions for Dev set\n",
    "  - For each classifier/task combination\n",
    "- Probabilities: `features/probabilities/probs_{split}_fused_{classifier}_{task}.npy`\n",
    "  - Probability distributions for Dev set\n",
    "  - For each classifier/task combination\n",
    "- Evaluation plots: `plots/early_fusion/{task}_{classifier}/`\n",
    "  - Confusion matrices\n",
    "  - Precision-Recall curves\n",
    "  - ROC curves\n",
    "\n",
    "**To GitHub:**\n",
    "- Fused feature metadata: `metadata/fused_{split}_{models}_{task}.json`\n",
    "  - Fused feature names\n",
    "  - Component model information\n",
    "  - Timestamp and data paths\n",
    "- Results metadata: `results/early_fusion_{task}.json`\n",
    "  - Metrics for each classifier\n",
    "  - Fusion method information\n",
    "  - Train/Dev sample counts\n",
    "\n",
    "**Evaluation Metrics Computed and Printed:**\n",
    "- Same comprehensive metrics as 03_train_evaluate.ipynb\n",
    "- Comparison with individual model results from 03_train_evaluate.ipynb\n",
    "\n",
    "**What gets passed to next notebook:**\n",
    "- Fused features saved to persistent storage\n",
    "- Predictions and probabilities for comparison\n",
    "- Results metadata for final evaluation comparison\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SETUP: Repository Clone, Drive Mount, and Path Configuration\n",
    "# ============================================================================\n",
    "# This cell performs minimal setup required for the notebook to run:\n",
    "# 1. Clones repository from GitHub (if not already present)\n",
    "# 2. Mounts Google Drive for persistent data storage\n",
    "# 3. Configures Python paths and initializes StorageManager\n",
    "# 4. Loads data splits and features created in previous notebooks\n",
    "\n",
    "import shutil\n",
    "import os\n",
    "import subprocess\n",
    "import time\n",
    "import requests\n",
    "import zipfile\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from google.colab import drive\n",
    "import numpy as np\n",
    "\n",
    "# Repository configuration\n",
    "repo_dir = '/content/semeval-context-tree-modular'\n",
    "repo_url = 'https://github.com/EonTechie/semeval-context-tree-modular.git'\n",
    "zip_url = 'https://github.com/EonTechie/semeval-context-tree-modular/archive/refs/heads/main.zip'\n",
    "\n",
    "# Clone repository (if not already present)\n",
    "if not os.path.exists(repo_dir):\n",
    "    print(\"Cloning repository from GitHub...\")\n",
    "    max_retries = 2\n",
    "    clone_success = False\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            result = subprocess.run(\n",
    "                ['git', 'clone', repo_url],\n",
    "                cwd='/content',\n",
    "                capture_output=True,\n",
    "                text=True,\n",
    "                timeout=60\n",
    "            )\n",
    "            if result.returncode == 0:\n",
    "                print(\"Repository cloned successfully via git\")\n",
    "                clone_success = True\n",
    "                break\n",
    "            else:\n",
    "                if attempt < max_retries - 1:\n",
    "                    time.sleep(3)\n",
    "        except Exception as e:\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(3)\n",
    "    \n",
    "    # Fallback: Download as ZIP if git clone fails\n",
    "    if not clone_success:\n",
    "        print(\"Git clone failed. Downloading repository as ZIP archive...\")\n",
    "        zip_path = '/tmp/repo.zip'\n",
    "        try:\n",
    "            response = requests.get(zip_url, stream=True, timeout=60)\n",
    "            response.raise_for_status()\n",
    "            with open(zip_path, 'wb') as f:\n",
    "                for chunk in response.iter_content(chunk_size=8192):\n",
    "                    f.write(chunk)\n",
    "            with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "                zip_ref.extractall('/content')\n",
    "            extracted_dir = '/content/semeval-context-tree-modular-main'\n",
    "            if os.path.exists(extracted_dir):\n",
    "                os.rename(extracted_dir, repo_dir)\n",
    "            os.remove(zip_path)\n",
    "            print(\"Repository downloaded and extracted successfully\")\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to obtain repository: {e}\")\n",
    "\n",
    "# Mount Google Drive (if not already mounted)\n",
    "try:\n",
    "    drive.mount('/content/drive', force_remount=False)\n",
    "except Exception:\n",
    "    pass  # Already mounted\n",
    "\n",
    "# Configure paths\n",
    "BASE_PATH = Path('/content/semeval-context-tree-modular')\n",
    "DATA_PATH = Path('/content/drive/MyDrive/semeval_data')\n",
    "\n",
    "# Verify repository structure exists\n",
    "if not BASE_PATH.exists():\n",
    "    raise RuntimeError(f\"Repository directory not found: {BASE_PATH}\")\n",
    "if not (BASE_PATH / 'src').exists():\n",
    "    raise RuntimeError(f\"src directory not found in repository: {BASE_PATH / 'src'}\")\n",
    "if not (BASE_PATH / 'src' / 'storage' / 'manager.py').exists():\n",
    "    raise RuntimeError(f\"Required file not found: {BASE_PATH / 'src' / 'storage' / 'manager.py'}\")\n",
    "\n",
    "# Add repository to Python path\n",
    "sys.path.insert(0, str(BASE_PATH))\n",
    "\n",
    "# Verify imports work\n",
    "try:\n",
    "    from src.storage.manager import StorageManager\n",
    "    from src.features.fusion import fuse_attention_features\n",
    "    from src.models.trainer import train_and_evaluate\n",
    "    from src.models.classifiers import get_classifier_dict\n",
    "except ImportError as e:\n",
    "    raise ImportError(\n",
    "        f\"Failed to import required modules. \"\n",
    "        f\"Repository path: {BASE_PATH}, \"\n",
    "        f\"Python path: {sys.path[:3]}, \"\n",
    "        f\"Error: {e}\"\n",
    "    )\n",
    "\n",
    "# Initialize StorageManager\n",
    "storage = StorageManager(\n",
    "    base_path=str(BASE_PATH),\n",
    "    data_path=str(DATA_PATH),\n",
    "    github_path=str(BASE_PATH)\n",
    ")\n",
    "\n",
    "# Data splits will be loaded per-task in the fusion loop\n",
    "# Clarity and Evasion have different splits (Evasion uses majority voting)\n",
    "\n",
    "print(\"Setup complete\")\n",
    "print(f\"  Repository: {BASE_PATH}\")\n",
    "print(f\"  Data storage: {DATA_PATH}\")\n",
    "print(f\"\\nNOTE: Data splits will be loaded per-task (task-specific splits)\")\n",
    "print(f\"      Clarity and Evasion have different splits due to majority voting\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# REPRODUCIBILITY SETUP: Set Random Seeds for All Libraries\n",
    "# ============================================================================\n",
    "# This cell sets random seeds for Python, NumPy, PyTorch, and HuggingFace\n",
    "# to ensure reproducible results across all runs.\n",
    "# \n",
    "# IMPORTANT: Run this cell FIRST before any other code that uses randomness.\n",
    "# Seed value: 42 (same as used in all other parts of the pipeline)\n",
    "\n",
    "from src.utils.reproducibility import set_all_seeds\n",
    "\n",
    "# Set all random seeds to 42 for full reproducibility\n",
    "# deterministic=True ensures PyTorch operations are deterministic (slower but fully reproducible)\n",
    "set_all_seeds(seed=42, deterministic=True)\n",
    "\n",
    "print(\"✓ Reproducibility configured: All random seeds set to 42\")\n",
    "print(\"✓ PyTorch deterministic mode enabled\")\n",
    "print(\"\\nNOTE: If you encounter performance issues or non-deterministic behavior,\")\n",
    "print(\"      you can set deterministic=False in set_all_seeds() call above.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CONFIGURE MODELS, TASKS, AND CLASSIFIERS\n",
    "# ============================================================================\n",
    "# Defines the models to fuse, tasks to perform, and classifiers to train\n",
    "# Label mappings are defined for clarity (3-class) and evasion (9-class) tasks\n",
    "\n",
    "MODELS = ['bert', 'bert_political', 'bert_ambiguity', 'roberta', 'deberta', 'xlnet']\n",
    "TASKS = ['clarity', 'evasion']\n",
    "\n",
    "# Label mappings for each task\n",
    "CLARITY_LABELS = ['Ambivalent', 'Clear Non-Reply', 'Clear Reply']\n",
    "EVASION_LABELS = ['Claims ignorance', 'Clarification', 'Declining to answer', \n",
    "                  'Deflection', 'Dodging', 'Explicit', \n",
    "                  'General', 'Implicit', 'Partial/half-answer']\n",
    "\n",
    "# Initialize classifiers with fixed random seed for reproducibility\n",
    "classifiers = get_classifier_dict(random_state=42)\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"  Models to fuse: {MODELS}\")\n",
    "print(f\"  Tasks: {TASKS}\")\n",
    "print(f\"  Classifiers: {list(classifiers.keys())}\")\n",
    "print(f\"  Fusion method: Early fusion (feature concatenation)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PERFORM EARLY FUSION - Feature Concatenation Only\n",
    "# ============================================================================\n",
    "# For each task, loads model-independent features once (18 features) and\n",
    "# model-dependent features from each model (7 features per model).\n",
    "# Concatenates: [18 Model-Independent | 6 Models × 7 Model-Dependent] = 60 features\n",
    "# Saves fused features for Train and Dev splits\n",
    "# NOTE: Training and evaluation will be done in the next cell (Cell 6) on Test set\n",
    "\n",
    "from src.features.extraction import get_model_independent_feature_names, get_model_dependent_feature_names\n",
    "\n",
    "for task in TASKS:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"TASK: {task.upper()} - EARLY FUSION (60 FEATURES)\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Select appropriate label list and dataset key for this task\n",
    "    if task == 'clarity':\n",
    "        label_list = CLARITY_LABELS\n",
    "        label_key = 'clarity_label'\n",
    "    else:  # evasion\n",
    "        label_list = EVASION_LABELS\n",
    "        label_key = 'evasion_label'\n",
    "    \n",
    "    # ========================================================================\n",
    "    # STEP 1: Load model-independent features (18 features, shared across all models)\n",
    "    # ========================================================================\n",
    "    print(\"\\nStep 1: Loading model-independent features (18 features, shared)...\")\n",
    "    try:\n",
    "        X_train_indep = storage.load_model_independent_features('train', task=task)\n",
    "        X_dev_indep = storage.load_model_independent_features('dev', task=task)\n",
    "        print(f\"  ✓ Loaded model-independent features: {X_train_indep.shape[1]} features\")\n",
    "        print(f\"    Train: {X_train_indep.shape[0]} samples\")\n",
    "        print(f\"    Dev: {X_dev_indep.shape[0]} samples\")\n",
    "    except FileNotFoundError as e:\n",
    "        raise FileNotFoundError(\n",
    "            f\"Model-independent features not found for task '{task}'. \"\n",
    "            f\"Make sure you have run 02_feature_extraction_separate.ipynb first.\\n\"\n",
    "            f\"Error: {e}\"\n",
    "        )\n",
    "    \n",
    "    # Get model-independent feature names\n",
    "    indep_feature_names = get_model_independent_feature_names()\n",
    "    assert len(indep_feature_names) == 18, f\"Expected 18 model-independent features, got {len(indep_feature_names)}\"\n",
    "    \n",
    "    # ========================================================================\n",
    "    # STEP 2: Load full features from all models and extract model-dependent portion\n",
    "    # ========================================================================\n",
    "    print(\"\\nStep 2: Loading model-dependent features from each model (7 features per model)...\")\n",
    "    model_dep_features_train = {}\n",
    "    model_dep_features_dev = {}\n",
    "    model_dep_feature_names = {}\n",
    "    \n",
    "    # Model-dependent feature names (same for all models)\n",
    "    dep_feature_names = get_model_dependent_feature_names()\n",
    "    assert len(dep_feature_names) == 7, f\"Expected 7 model-dependent features, got {len(dep_feature_names)}\"\n",
    "    \n",
    "    for model in MODELS:\n",
    "        # Load full features (25 features: 7 model-dependent + 18 model-independent)\n",
    "        X_train_full = storage.load_features(model, task, 'train')\n",
    "        X_dev_full = storage.load_features(model, task, 'dev')\n",
    "        \n",
    "        # Extract model-dependent portion (first 7 features)\n",
    "        # Feature order: [7 model-dependent | 18 model-independent]\n",
    "        X_train_dep = X_train_full[:, :7]  # First 7 features\n",
    "        X_dev_dep = X_dev_full[:, :7]\n",
    "        \n",
    "        model_dep_features_train[model] = X_train_dep\n",
    "        model_dep_features_dev[model] = X_dev_dep\n",
    "        model_dep_feature_names[model] = [f\"{model}_{name}\" for name in dep_feature_names]\n",
    "        \n",
    "        print(f\"  {model}: Extracted {X_train_dep.shape[1]} model-dependent features from {X_train_full.shape[1]} total features\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # STEP 3: Concatenate all features: [18 Model-Independent | 6 Models × 7 Model-Dependent]\n",
    "    # ========================================================================\n",
    "    print(\"\\nStep 3: Concatenating features (60 total features)...\")\n",
    "    \n",
    "    # Concatenate model-dependent features from all models\n",
    "    model_dep_list_train = [model_dep_features_train[model] for model in MODELS]\n",
    "    model_dep_list_dev = [model_dep_features_dev[model] for model in MODELS]\n",
    "    \n",
    "    X_train_dep_concat = np.hstack(model_dep_list_train)  # (N, 42)\n",
    "    X_dev_dep_concat = np.hstack(model_dep_list_dev)  # (N, 42)\n",
    "    \n",
    "    # Final concatenation: [18 Model-Independent | 42 Model-Dependent]\n",
    "    X_train_fused = np.hstack([X_train_indep, X_train_dep_concat])  # (N, 60)\n",
    "    X_dev_fused = np.hstack([X_dev_indep, X_dev_dep_concat])  # (N, 60)\n",
    "    \n",
    "    # Build feature names\n",
    "    fused_feature_names = indep_feature_names.copy()  # 18 model-independent\n",
    "    for model in MODELS:\n",
    "        fused_feature_names.extend(model_dep_feature_names[model])  # 7 per model\n",
    "    \n",
    "    print(f\"  ✓ Fused features: {X_train_fused.shape[1]} features total\")\n",
    "    print(f\"    - Model-independent: {X_train_indep.shape[1]} features\")\n",
    "    print(f\"    - Model-dependent: {X_train_dep_concat.shape[1]} features (6 models × 7)\")\n",
    "    print(f\"    Train: {X_train_fused.shape[0]} samples\")\n",
    "    print(f\"    Dev: {X_dev_fused.shape[0]} samples\")\n",
    "    print(f\"\\n  Feature names (first 5): {fused_feature_names[:5]}\")\n",
    "    print(f\"  Feature names (last 5): {fused_feature_names[-5:]}\")\n",
    "    print(f\"\\n  Model-dependent feature examples:\")\n",
    "    for model in MODELS[:2]:  # Show first 2 models\n",
    "        print(f\"    {model}: {model_dep_feature_names[model][:3]}...\")\n",
    "    \n",
    "    # Verify feature count\n",
    "    assert X_train_fused.shape[1] == 60, f\"Expected 60 features, got {X_train_fused.shape[1]}\"\n",
    "    assert len(fused_feature_names) == 60, f\"Expected 60 feature names, got {len(fused_feature_names)}\"\n",
    "    \n",
    "    # ========================================================================\n",
    "    # STEP 4: Save fused features to persistent storage\n",
    "    # ========================================================================\n",
    "    print(\"\\nStep 4: Saving fused features to persistent storage...\")\n",
    "    storage.save_fused_features(\n",
    "        X_train_fused, MODELS, task, 'train',\n",
    "        fused_feature_names, fusion_method='concat_60'\n",
    "    )\n",
    "    storage.save_fused_features(\n",
    "        X_dev_fused, MODELS, task, 'dev',\n",
    "        fused_feature_names, fusion_method='concat_60'\n",
    "    )\n",
    "    print(\"  ✓ Fused features saved (Train and Dev)\")\n",
    "    print(\"  ✓ Ready for Train+Dev training and Test evaluation in next cell\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"Early fusion complete for all tasks (60 features)\")\n",
    "print(f\"{'='*80}\")\n",
    "print(\"\\nSummary:\")\n",
    "print(\"  - 60 features: 18 model-independent + 42 model-dependent (6 models × 7)\")\n",
    "print(\"  - Fused features saved for Train and Dev splits\")\n",
    "print(\"  - Next cell will: Train on Train+Dev, Evaluate on Test set\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ============================================================================\n",
    "# FINAL EVALUATION ON TEST SET (TYPE 3)\n",
    "# ============================================================================\n",
    "# This section performs final evaluation on the TEST set using 60-feature early fusion:\n",
    "# 1. Extract test features (60 features: 18 model-independent + 42 model-dependent)\n",
    "#    - Checkpoint: Load if exists, extract and save if not\n",
    "# 2. Combine Train+Dev for final training\n",
    "# 3. Train all 6 classifiers on Train+Dev (60 features)\n",
    "# 4. Evaluate on Test set (2 tasks: clarity, evasion)\n",
    "# 5. Generate summary tables (like notebook 5)\n",
    "# 6. Save all results to FinalResultsType3 directory structure\n",
    "\n",
    "**Test Set Sizes:**\n",
    "- Clarity: 308 samples\n",
    "- Evasion: 275 samples\n",
    "\n",
    "**Output Structure:**\n",
    "- `results/FinalResultsType3/test/` - Test features (60 features)\n",
    "- `results/FinalResultsType3/predictions/` - Test predictions\n",
    "- `results/FinalResultsType3/tables/` - Summary tables\n",
    "- `results/FinalResultsType3/plots/` - Evaluation plots\n",
    "- `results/FinalResultsType3/` - Other results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# FINAL EVALUATION ON TEST SET (TYPE 3)\n",
    "# ============================================================================\n",
    "# Extract test features (60 features), train on Train+Dev, evaluate on Test\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score\n",
    "from src.features.extraction import (\n",
    "    featurize_model_independent_features,\n",
    "    featurize_model_dependent_features,\n",
    "    get_model_independent_feature_names,\n",
    "    get_model_dependent_feature_names\n",
    ")\n",
    "from src.models.classifiers import train_classifiers\n",
    "from src.evaluation.metrics import compute_all_metrics\n",
    "from src.evaluation.tables import create_final_summary_pivot, style_table_paper\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "import json\n",
    "\n",
    "# Model configurations\n",
    "MODEL_CONFIGS = {\n",
    "    'bert': 'bert-base-uncased',\n",
    "    'bert_political': 'bert-base-uncased',  # Fine-tuned version\n",
    "    'bert_ambiguity': 'bert-base-uncased',  # Fine-tuned version\n",
    "    'roberta': 'roberta-base',\n",
    "    'deberta': 'microsoft/deberta-base',\n",
    "    'xlnet': 'xlnet-base-cased'\n",
    "}\n",
    "\n",
    "MODEL_MAX_LENGTHS = {\n",
    "    'bert': 512,\n",
    "    'bert_political': 512,\n",
    "    'bert_ambiguity': 512,\n",
    "    'roberta': 512,\n",
    "    'deberta': 512,\n",
    "    'xlnet': 1024\n",
    "}\n",
    "\n",
    "# Device setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "# ========================================================================\n",
    "# STEP 1: Create Type3 output directories (CHECKPOINT)\n",
    "# ========================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 1: CREATE TYPE3 OUTPUT DIRECTORIES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Drive directories\n",
    "test_features_dir = storage.data_path / 'results/FinalResultsType3/test'\n",
    "predictions_dir = storage.data_path / 'results/FinalResultsType3/predictions'\n",
    "tables_dir = storage.data_path / 'results/FinalResultsType3/tables'\n",
    "plots_dir = storage.data_path / 'results/FinalResultsType3/plots'\n",
    "results_dir = storage.data_path / 'results/FinalResultsType3'\n",
    "\n",
    "# Create all directories (always, to prevent FileNotFoundError)\n",
    "# CRITICAL: mkdir(parents=True, exist_ok=True) creates ALL parent directories recursively\n",
    "# Example: test_features_dir = 'results/FinalResultsType3/test'\n",
    "#   - Creates 'results/' if not exists\n",
    "#   - Creates 'results/FinalResultsType3/' if not exists\n",
    "#   - Creates 'results/FinalResultsType3/test/' if not exists\n",
    "# This ensures the entire path exists before any save operation\n",
    "\n",
    "test_features_dir.mkdir(parents=True, exist_ok=True)\n",
    "predictions_dir.mkdir(parents=True, exist_ok=True)\n",
    "tables_dir.mkdir(parents=True, exist_ok=True)\n",
    "plots_dir.mkdir(parents=True, exist_ok=True)\n",
    "results_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Verify all directories were created successfully\n",
    "print(\"✓ Created all Type3 output directories:\")\n",
    "print(f\"  - Test features: {test_features_dir} (exists: {test_features_dir.exists()})\")\n",
    "print(f\"  - Predictions: {predictions_dir} (exists: {predictions_dir.exists()})\")\n",
    "print(f\"  - Tables: {tables_dir} (exists: {tables_dir.exists()})\")\n",
    "print(f\"  - Plots: {plots_dir} (exists: {plots_dir.exists()})\")\n",
    "print(f\"  - Results: {results_dir} (exists: {results_dir.exists()})\")\n",
    "\n",
    "# Verify parent directories also exist\n",
    "parent_dir = storage.data_path / 'results'\n",
    "print(f\"\\n✓ Parent directory 'results/' exists: {parent_dir.exists()}\")\n",
    "type3_parent = storage.data_path / 'results/FinalResultsType3'\n",
    "print(f\"✓ Type3 parent directory exists: {type3_parent.exists()}\")\n",
    "\n",
    "# ========================================================================\n",
    "# STEP 2: Extract or load test features (60 features) - CHECKPOINT\n",
    "# ========================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 2: TEST FEATURE EXTRACTION (60 FEATURES)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Load sentiment pipeline for model-independent features\n",
    "print(\"Loading sentiment analysis pipeline...\")\n",
    "try:\n",
    "    from transformers import pipeline\n",
    "    sentiment_pipeline = pipeline(\n",
    "        \"sentiment-analysis\",\n",
    "        model=\"cardiffnlp/twitter-roberta-base-sentiment-latest\",\n",
    "        device=0 if torch.cuda.is_available() else -1,\n",
    "        return_all_scores=True\n",
    "    )\n",
    "    print(\"  ✓ Sentiment pipeline loaded\")\n",
    "except Exception as e:\n",
    "    print(f\"  ⚠ Could not load sentiment pipeline: {e}\")\n",
    "    sentiment_pipeline = None\n",
    "\n",
    "metadata_keys = {\n",
    "    'inaudible': 'inaudible',\n",
    "    'multiple_questions': 'multiple_questions',\n",
    "    'affirmative_questions': 'affirmative_questions'\n",
    "}\n",
    "\n",
    "# Store test features for each task\n",
    "test_features_60 = {}  # {task: {'train': X_train_60, 'dev': X_dev_60, 'test': X_test_60}}\n",
    "\n",
    "for task in TASKS:\n",
    "    print(f\"\\n{'-'*60}\")\n",
    "    print(f\"Task: {task.upper()}\")\n",
    "    print(f\"{'-'*60}\")\n",
    "    \n",
    "    # Load test split\n",
    "    try:\n",
    "        test_ds = storage.load_split('test', task=task)\n",
    "        print(f\"  Test set: {len(test_ds)} samples\")\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"  ⚠ Test split not found for {task}: {e}\")\n",
    "        continue\n",
    "    \n",
    "    # ====================================================================\n",
    "    # 2.1: Extract or load model-independent test features (18 features)\n",
    "    # ====================================================================\n",
    "    print(f\"\\n  2.1: Model-independent test features (18 features)...\")\n",
    "    test_indep_path = test_features_dir / f'X_test_independent_{task}.npy'\n",
    "    \n",
    "    if test_indep_path.exists():\n",
    "        X_test_indep = np.load(test_indep_path)\n",
    "        print(f\"    ✓ Loaded from checkpoint: {X_test_indep.shape}\")\n",
    "    else:\n",
    "        print(f\"    → Extracting model-independent test features...\")\n",
    "        X_test_indep, _ = featurize_model_independent_features(\n",
    "            test_ds,\n",
    "            question_key='interview_question',\n",
    "            answer_key='interview_answer',\n",
    "            batch_size=32,\n",
    "            show_progress=True,\n",
    "            sentiment_pipeline=sentiment_pipeline,\n",
    "            metadata_keys=metadata_keys,\n",
    "        )\n",
    "        # Save to checkpoint (ensure directory exists before saving)\n",
    "        test_features_dir.mkdir(parents=True, exist_ok=True)\n",
    "        np.save(test_indep_path, X_test_indep)\n",
    "        print(f\"    ✓ Extracted and saved: {X_test_indep.shape}\")\n",
    "    \n",
    "    # ====================================================================\n",
    "    # 2.2: Extract or load model-dependent test features (7 features per model)\n",
    "    # ====================================================================\n",
    "    print(f\"\\n  2.2: Model-dependent test features (7 features × 6 models = 42 features)...\")\n",
    "    model_dep_test_features = {}\n",
    "    \n",
    "    for model_key in MODELS:\n",
    "        model_name = MODEL_CONFIGS[model_key]\n",
    "        max_seq_len = MODEL_MAX_LENGTHS[model_key]\n",
    "        \n",
    "        test_dep_path = test_features_dir / f'X_test_{model_key}_dependent_{task}.npy'\n",
    "        \n",
    "        if test_dep_path.exists():\n",
    "            X_test_dep = np.load(test_dep_path)\n",
    "            print(f\"    {model_key}: ✓ Loaded from checkpoint: {X_test_dep.shape}\")\n",
    "        else:\n",
    "            print(f\"    {model_key}: → Extracting model-dependent test features...\")\n",
    "            \n",
    "            # Clear GPU cache\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "                torch.cuda.synchronize()\n",
    "            \n",
    "            # Load tokenizer and model\n",
    "            tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "            model = AutoModel.from_pretrained(model_name)\n",
    "            model.to(device)\n",
    "            model.eval()\n",
    "            \n",
    "            # Extract model-dependent features only\n",
    "            X_test_dep, _ = featurize_model_dependent_features(\n",
    "                test_ds,\n",
    "                tokenizer,\n",
    "                model,\n",
    "                device,\n",
    "                question_key='interview_question',\n",
    "                answer_key='interview_answer',\n",
    "                batch_size=8,\n",
    "                max_sequence_length=max_seq_len,\n",
    "                show_progress=True,\n",
    "            )\n",
    "            \n",
    "            # Save to checkpoint (ensure directory exists before saving)\n",
    "            test_features_dir.mkdir(parents=True, exist_ok=True)\n",
    "            np.save(test_dep_path, X_test_dep)\n",
    "            print(f\"    {model_key}: ✓ Extracted and saved: {X_test_dep.shape}\")\n",
    "            \n",
    "            # Free GPU memory\n",
    "            del model, tokenizer\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "        \n",
    "        model_dep_test_features[model_key] = X_test_dep\n",
    "    \n",
    "    # ====================================================================\n",
    "    # 2.3: Concatenate test features: [18 Model-Independent | 42 Model-Dependent]\n",
    "    # ====================================================================\n",
    "    print(f\"\\n  2.3: Concatenating test features (60 total)...\")\n",
    "    \n",
    "    # Concatenate model-dependent features from all models\n",
    "    model_dep_list = [model_dep_test_features[model] for model in MODELS]\n",
    "    X_test_dep_concat = np.hstack(model_dep_list)  # (N, 42)\n",
    "    \n",
    "    # Final concatenation: [18 Model-Independent | 42 Model-Dependent]\n",
    "    X_test_60 = np.hstack([X_test_indep, X_test_dep_concat])  # (N, 60)\n",
    "    \n",
    "    print(f\"    ✓ Test features: {X_test_60.shape} (60 features)\")\n",
    "    print(f\"      - Model-independent: {X_test_indep.shape[1]} features\")\n",
    "    print(f\"      - Model-dependent: {X_test_dep_concat.shape[1]} features (6 models × 7)\")\n",
    "    \n",
    "    # Verify feature count\n",
    "    assert X_test_60.shape[1] == 60, f\"Expected 60 features, got {X_test_60.shape[1]}\"\n",
    "    \n",
    "    # Save complete test features to checkpoint (ensure directory exists before saving)\n",
    "    test_features_dir.mkdir(parents=True, exist_ok=True)\n",
    "    test_complete_path = test_features_dir / f'X_test_60feat_{task}.npy'\n",
    "    np.save(test_complete_path, X_test_60)\n",
    "    print(f\"    ✓ Saved complete test features to: {test_complete_path.name}\")\n",
    "    \n",
    "    # Store for later use\n",
    "    test_features_60[task] = {\n",
    "        'test': X_test_60\n",
    "    }\n",
    "\n",
    "print(\"\\n✓ Test feature extraction complete for all tasks\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================================\n",
    "# STEP 3: Train on Train+Dev and evaluate on Test (2 tasks, 6 classifiers)\n",
    "# ========================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 3: TRAIN ON TRAIN+DEV AND EVALUATE ON TEST\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Store all results for summary tables\n",
    "all_results_type3 = {}  # {task: {classifier: {metrics, predictions, probabilities}}}\n",
    "\n",
    "for task in TASKS:\n",
    "    print(f\"\\n{'-'*80}\")\n",
    "    print(f\"TASK: {task.upper()}\")\n",
    "    print(f\"{'-'*80}\")\n",
    "    \n",
    "    if task not in test_features_60:\n",
    "        print(f\"  ⚠ Skipping {task}: Test features not available\")\n",
    "        continue\n",
    "    \n",
    "    # Select appropriate label list and dataset key\n",
    "    if task == 'clarity':\n",
    "        label_list = CLARITY_LABELS\n",
    "        label_key = 'clarity_label'\n",
    "    else:  # evasion\n",
    "        label_list = EVASION_LABELS\n",
    "        label_key = 'evasion_label'\n",
    "    \n",
    "    # Load Train+Dev fused features (60 features) - from Cell 4\n",
    "    # These were already created and saved in Cell 4\n",
    "    try:\n",
    "        # Load fused features directly from Cell 4 (no need to reconstruct)\n",
    "        X_train_60 = storage.load_fused_features(MODELS, task, 'train')\n",
    "        X_dev_60 = storage.load_fused_features(MODELS, task, 'dev')\n",
    "        \n",
    "        print(f\"  ✓ Loaded Train fused features: {X_train_60.shape} (60 features)\")\n",
    "        print(f\"  ✓ Loaded Dev fused features: {X_dev_60.shape} (60 features)\")\n",
    "        \n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"  ⚠ Fused features not found for {task}. Make sure Cell 4 completed successfully.\")\n",
    "        print(f\"  Error: {e}\")\n",
    "        continue\n",
    "    except Exception as e:\n",
    "        print(f\"  ⚠ Error loading train+dev features: {e}\")\n",
    "        continue\n",
    "    \n",
    "    # Load labels\n",
    "    train_ds = storage.load_split('train', task=task)\n",
    "    dev_ds = storage.load_split('dev', task=task)\n",
    "    test_ds = storage.load_split('test', task=task)\n",
    "    \n",
    "    y_train = np.array([train_ds[i][label_key] for i in range(len(train_ds))])\n",
    "    y_dev = np.array([dev_ds[i][label_key] for i in range(len(dev_ds))])\n",
    "    y_test = np.array([test_ds[i][label_key] for i in range(len(test_ds))])\n",
    "    \n",
    "    # Combine Train+Dev for final training\n",
    "    X_train_full = np.vstack([X_train_60, X_dev_60])\n",
    "    y_train_full = np.concatenate([y_train, y_dev])\n",
    "    \n",
    "    print(f\"  Combined Train+Dev: {X_train_full.shape[0]} samples\")\n",
    "    print(f\"  Test: {len(y_test)} samples\")\n",
    "    \n",
    "    # Get test features\n",
    "    X_test_60 = test_features_60[task]['test']\n",
    "    \n",
    "    # Train all 6 classifiers and evaluate on test\n",
    "    print(f\"\\n  Training and evaluating {len(classifiers)} classifiers...\")\n",
    "    all_results_type3[task] = {}\n",
    "    \n",
    "    for clf_name, clf in classifiers.items():\n",
    "        print(f\"\\n    Classifier: {clf_name}\")\n",
    "        \n",
    "        # Encode labels\n",
    "        le = LabelEncoder()\n",
    "        y_train_encoded = le.fit_transform(y_train_full)\n",
    "        y_test_encoded = le.transform(y_test)\n",
    "        \n",
    "        # Train classifier\n",
    "        clf.fit(X_train_full, y_train_encoded)\n",
    "        \n",
    "        # Predict on test\n",
    "        y_test_pred_encoded = clf.predict(X_test_60)\n",
    "        y_test_pred = le.inverse_transform(y_test_pred_encoded)\n",
    "        \n",
    "        # Get probabilities (if available)\n",
    "        if hasattr(clf, 'predict_proba'):\n",
    "            y_test_proba = clf.predict_proba(X_test_60)\n",
    "        else:\n",
    "            y_test_proba = None\n",
    "        \n",
    "        # Compute metrics\n",
    "        metrics = compute_all_metrics(\n",
    "            y_test_encoded,\n",
    "            y_test_pred_encoded,\n",
    "            label_list,\n",
    "            task_name=f\"TYPE3_TEST_{task}_{clf_name}\"\n",
    "        )\n",
    "        \n",
    "        # Store results\n",
    "        all_results_type3[task][clf_name] = {\n",
    "            'predictions': y_test_pred,\n",
    "            'probabilities': y_test_proba,\n",
    "            'metrics': metrics\n",
    "        }\n",
    "        \n",
    "        print(f\"      Test Macro F1: {metrics.get('macro_f1', 0.0):.4f}\")\n",
    "        print(f\"      Test Accuracy: {metrics.get('accuracy', 0.0):.4f}\")\n",
    "        \n",
    "        # Save HARD LABELS (predictions) to Type3 folder\n",
    "        predictions_dir.mkdir(parents=True, exist_ok=True)\n",
    "        pred_path = predictions_dir / f'pred_test_{clf_name}_{task}.npy'\n",
    "        np.save(pred_path, y_test_pred)\n",
    "        print(f\"      ✓ Saved HARD LABELS: {pred_path.name}\")\n",
    "        \n",
    "        # Save SOFT LABELS (probabilities) to Type3 folder\n",
    "        if y_test_proba is not None:\n",
    "            predictions_dir.mkdir(parents=True, exist_ok=True)\n",
    "            proba_path = predictions_dir / f'proba_test_{clf_name}_{task}.npy'\n",
    "            np.save(proba_path, y_test_proba)\n",
    "            print(f\"      ✓ Saved SOFT LABELS: {proba_path.name} (shape: {y_test_proba.shape})\")\n",
    "        else:\n",
    "            print(f\"      ⚠ SOFT LABELS not available for {clf_name} (classifier does not support predict_proba)\")\n",
    "\n",
    "print(\"\\n✓ Training and evaluation complete for all tasks and classifiers\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================================\n",
    "# STEP 4: Generate summary tables (like notebook 5)\n",
    "# ========================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 4: GENERATE SUMMARY TABLES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create summary tables for each task\n",
    "for task in TASKS:\n",
    "    if task not in all_results_type3:\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\n{'-'*60}\")\n",
    "    print(f\"Task: {task.upper()}\")\n",
    "    print(f\"{'-'*60}\")\n",
    "    \n",
    "    # Create summary DataFrame\n",
    "    summary_rows = []\n",
    "    for clf_name, result in all_results_type3[task].items():\n",
    "        metrics = result['metrics']\n",
    "        summary_rows.append({\n",
    "            'classifier': clf_name,\n",
    "            'task': task,\n",
    "            'macro_f1': metrics.get('macro_f1', 0.0),\n",
    "            'accuracy': metrics.get('accuracy', 0.0),\n",
    "            'macro_precision': metrics.get('macro_precision', 0.0),\n",
    "            'macro_recall': metrics.get('macro_recall', 0.0),\n",
    "        })\n",
    "    \n",
    "    df_summary = pd.DataFrame(summary_rows)\n",
    "    \n",
    "    # Display table\n",
    "    print(f\"\\nSummary Table for {task.upper()}:\")\n",
    "    display(df_summary.style.format(precision=4))\n",
    "    \n",
    "    # Save table (ensure directory exists before saving)\n",
    "    tables_dir.mkdir(parents=True, exist_ok=True)\n",
    "    table_path = tables_dir / f'summary_{task}.csv'\n",
    "    df_summary.to_csv(table_path, index=False)\n",
    "    print(f\"  ✓ Saved table: {table_path.name}\")\n",
    "    \n",
    "    # Save HTML version\n",
    "    tables_dir.mkdir(parents=True, exist_ok=True)\n",
    "    html_path = tables_dir / f'summary_{task}.html'\n",
    "    df_summary.to_html(html_path, index=False, float_format='{:.4f}'.format)\n",
    "    print(f\"  ✓ Saved HTML: {html_path.name}\")\n",
    "\n",
    "# Create combined summary (all tasks)\n",
    "print(f\"\\n{'-'*60}\")\n",
    "print(\"Combined Summary (All Tasks)\")\n",
    "print(f\"{'-'*60}\")\n",
    "\n",
    "all_summary_rows = []\n",
    "for task in TASKS:\n",
    "    if task not in all_results_type3:\n",
    "        continue\n",
    "    for clf_name, result in all_results_type3[task].items():\n",
    "        metrics = result['metrics']\n",
    "        all_summary_rows.append({\n",
    "            'classifier': clf_name,\n",
    "            'task': task,\n",
    "            'macro_f1': metrics.get('macro_f1', 0.0),\n",
    "            'accuracy': metrics.get('accuracy', 0.0),\n",
    "        })\n",
    "\n",
    "df_all_summary = pd.DataFrame(all_summary_rows)\n",
    "\n",
    "# Pivot table: Classifier × Task\n",
    "if len(df_all_summary) > 0:\n",
    "    df_pivot = df_all_summary.pivot(index='classifier', columns='task', values='macro_f1')\n",
    "    \n",
    "    print(\"\\nPivot Table: Classifier × Task (Macro F1)\")\n",
    "    display(df_pivot.style.format(precision=4))\n",
    "    \n",
    "    # Save pivot table (ensure directory exists before saving)\n",
    "    tables_dir.mkdir(parents=True, exist_ok=True)\n",
    "    pivot_path = tables_dir / 'summary_all_tasks_pivot.csv'\n",
    "    df_pivot.to_csv(pivot_path)\n",
    "    print(f\"  ✓ Saved pivot table: {pivot_path.name}\")\n",
    "    \n",
    "    # Save HTML version\n",
    "    tables_dir.mkdir(parents=True, exist_ok=True)\n",
    "    html_pivot_path = tables_dir / 'summary_all_tasks_pivot.html'\n",
    "    df_pivot.to_html(html_pivot_path, float_format='{:.4f}'.format)\n",
    "    print(f\"  ✓ Saved HTML: {html_pivot_path.name}\")\n",
    "\n",
    "# Save complete results to JSON (ensure directory exists before saving)\n",
    "results_dir.mkdir(parents=True, exist_ok=True)\n",
    "results_json_path = results_dir / 'final_results_type3.json'\n",
    "results_dict = {\n",
    "    'method': 'early_fusion_60feat',\n",
    "    'n_features': 60,\n",
    "    'feature_breakdown': {\n",
    "        'model_independent': 18,\n",
    "        'model_dependent': 42,\n",
    "        'models': len(MODELS),\n",
    "        'features_per_model': 7\n",
    "    },\n",
    "    'tasks': TASKS,\n",
    "    'classifiers': list(classifiers.keys()),\n",
    "    'results': {\n",
    "        task: {\n",
    "            clf_name: {\n",
    "                'metrics': result['metrics'],\n",
    "                'n_test': len(test_features_60[task]['test']) if task in test_features_60 else 0\n",
    "            }\n",
    "            for clf_name, result in task_results.items()\n",
    "        }\n",
    "        for task, task_results in all_results_type3.items()\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(results_json_path, 'w') as f:\n",
    "    json.dump(results_dict, f, indent=2, default=str)\n",
    "\n",
    "print(f\"\\n✓ Saved complete results: {results_json_path.name}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"FINAL EVALUATION TYPE 3 COMPLETE\")\n",
    "print(f\"{'='*80}\")\n",
    "print(\"\\nSummary:\")\n",
    "print(\"  - 60 features: 18 model-independent + 42 model-dependent (6 models × 7)\")\n",
    "print(\"  - Trained on Train+Dev combined data\")\n",
    "print(\"  - Evaluated on Test set (2 tasks: clarity, evasion)\")\n",
    "print(\"  - All 6 classifiers evaluated\")\n",
    "print(\"  - Results saved to FinalResultsType3 directory\")\n",
    "print(\"\\nOutput locations:\")\n",
    "print(f\"  - Test features: {test_features_dir}\")\n",
    "print(f\"  - Predictions: {predictions_dir}\")\n",
    "print(f\"  - Tables: {tables_dir}\")\n",
    "print(f\"  - Results: {results_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ============================================================================\n",
    "# FINAL SUMMARY TABLES: Clarity, EvasionBasedClarity, and Annotator-Based Clarity\n",
    "# ============================================================================\n",
    "# This cell generates comprehensive summary tables similar to 3. and 5. notebooks:\n",
    "#\n",
    "# **Tasks Evaluated:**\n",
    "# 1. **clarity**: Direct clarity predictions (308 samples)\n",
    "#    - Hard labels: Direct clarity predictions from 60-feature early fusion\n",
    "#    - Soft labels: Probabilities from classifiers (if available)\n",
    "#\n",
    "# 2. **evasion_based_clarity**: Evasion predictions mapped to clarity (275 samples)\n",
    "#    - Hard labels: Evasion predictions → clarity mapping (hierarchical)\n",
    "#    - Soft labels: Evasion probabilities → clarity probabilities (if available)\n",
    "#    - Uses evasion predictions from 60-feature early fusion\n",
    "#\n",
    "# 3. **annotator1_based_clarity**: Annotator1's evasion labels mapped to clarity\n",
    "#    - Compares classifier's evasion_based_clarity predictions against annotator1's mapped clarity\n",
    "#\n",
    "# 4. **annotator2_based_clarity**: Annotator2's evasion labels mapped to clarity\n",
    "#    - Compares classifier's evasion_based_clarity predictions against annotator2's mapped clarity\n",
    "#\n",
    "# 5. **annotator3_based_clarity**: Annotator3's evasion labels mapped to clarity\n",
    "#    - Compares classifier's evasion_based_clarity predictions against annotator3's mapped clarity\n",
    "#\n",
    "# **Table Format:**\n",
    "# - Pivot Table: Classifier × Tasks (Macro F1 scores)\n",
    "# - Detailed Table: All metrics (macro F1, accuracy, precision, recall)\n",
    "#\n",
    "# **Saved to:** results/FinalResultsType3/tables/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# FINAL SUMMARY TABLES: Clarity, EvasionBasedClarity, and Annotator-Based Clarity\n",
    "# ============================================================================\n",
    "# Generate comprehensive summary tables similar to 3. and 5. notebooks\n",
    "\n",
    "from src.models.hierarchical import evasion_to_clarity, evaluate_hierarchical_approach\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL SUMMARY TABLES GENERATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ========================================================================\n",
    "# STEP 1: Evaluate Evasion-Based Clarity and Annotator-Based Clarity\n",
    "# ========================================================================\n",
    "print(\"\\nStep 1: Evaluating Evasion-Based Clarity and Annotator-Based Clarity...\")\n",
    "\n",
    "# Store all evaluation results (including clarity, evasion_based_clarity, annotator-based)\n",
    "all_evaluation_results = {}  # {task_name: {classifier: {metrics}}}\n",
    "\n",
    "# First, add direct clarity results\n",
    "if 'clarity' in all_results_type3:\n",
    "    all_evaluation_results['clarity'] = {\n",
    "        clf_name: {'metrics': result['metrics']}\n",
    "        for clf_name, result in all_results_type3['clarity'].items()\n",
    "    }\n",
    "\n",
    "# For evasion task, create evasion_based_clarity and annotator-based clarity\n",
    "if 'evasion' in all_results_type3:\n",
    "    print(\"\\n  Processing evasion task for hierarchical evaluations...\")\n",
    "    \n",
    "    # Load test split for evasion (to get annotator labels)\n",
    "    try:\n",
    "        test_ds_evasion = storage.load_split('test', task='evasion')\n",
    "        test_ds_clarity = storage.load_split('test', task='clarity')\n",
    "        \n",
    "        # Get true clarity labels\n",
    "        y_clarity_true_test = np.array([test_ds_clarity[i]['clarity_label'] for i in range(len(test_ds_clarity))])\n",
    "        \n",
    "        # Encode clarity labels\n",
    "        le_clarity = LabelEncoder()\n",
    "        y_clarity_true_encoded = le_clarity.fit_transform(y_clarity_true_test)\n",
    "        clarity_label_list = CLARITY_LABELS\n",
    "        \n",
    "        # ====================================================================\n",
    "        # 1.1: Evasion-Based Clarity (evasion predictions → clarity)\n",
    "        # ====================================================================\n",
    "        print(\"\\n  1.1: Evaluating evasion_based_clarity...\")\n",
    "        all_evaluation_results['evasion_based_clarity'] = {}\n",
    "        \n",
    "        for clf_name, result in all_results_type3['evasion'].items():\n",
    "            # Get evasion predictions (string labels)\n",
    "            y_evasion_pred = result['predictions']\n",
    "            \n",
    "            # Map evasion predictions to clarity using hierarchical mapping\n",
    "            hierarchical_metrics = evaluate_hierarchical_approach(\n",
    "                np.zeros(len(y_evasion_pred), dtype=int),  # Dummy evasion_true (not used)\n",
    "                y_evasion_pred,  # Evasion predictions (string labels)\n",
    "                y_clarity_true_encoded,  # True clarity labels (encoded)\n",
    "                EVASION_LABELS,\n",
    "                clarity_label_list\n",
    "            )\n",
    "            \n",
    "            all_evaluation_results['evasion_based_clarity'][clf_name] = {\n",
    "                'metrics': hierarchical_metrics['metrics']\n",
    "            }\n",
    "            print(f\"      {clf_name}: Macro F1 = {hierarchical_metrics['metrics'].get('macro_f1', 0.0):.4f}\")\n",
    "        \n",
    "        # ====================================================================\n",
    "        # 1.2: Annotator-Based Clarity (annotator1/2/3 evasion labels → clarity)\n",
    "        # ====================================================================\n",
    "        print(\"\\n  1.2: Evaluating annotator-based clarity...\")\n",
    "        \n",
    "        # Extract annotator labels from test dataset\n",
    "        try:\n",
    "            y_annotator1_evasion = np.array([test_ds_evasion[i]['annotator1'] for i in range(len(test_ds_evasion))])\n",
    "            y_annotator2_evasion = np.array([test_ds_evasion[i]['annotator2'] for i in range(len(test_ds_evasion))])\n",
    "            y_annotator3_evasion = np.array([test_ds_evasion[i]['annotator3'] for i in range(len(test_ds_evasion))])\n",
    "            \n",
    "            # Evaluate each annotator's labels mapped to clarity\n",
    "            for annotator_name, y_annotator_evasion in [\n",
    "                ('annotator1_based_clarity', y_annotator1_evasion),\n",
    "                ('annotator2_based_clarity', y_annotator2_evasion),\n",
    "                ('annotator3_based_clarity', y_annotator3_evasion)\n",
    "            ]:\n",
    "                print(f\"\\n    Evaluating {annotator_name}...\")\n",
    "                \n",
    "                # Map annotator's evasion labels to clarity\n",
    "                y_annotator_clarity_mapped = np.array([\n",
    "                    evasion_to_clarity(str(ev_label)) for ev_label in y_annotator_evasion\n",
    "                ])\n",
    "                y_annotator_clarity_encoded = le_clarity.transform(y_annotator_clarity_mapped)\n",
    "                \n",
    "                # For each classifier, evaluate its evasion_based_clarity predictions against annotator's mapped clarity\n",
    "                annotator_results = {}\n",
    "                for clf_name, result in all_results_type3['evasion'].items():\n",
    "                    # Get evasion predictions and map to clarity\n",
    "                    y_evasion_pred = result['predictions']\n",
    "                    hierarchical_metrics = evaluate_hierarchical_approach(\n",
    "                        np.zeros(len(y_evasion_pred), dtype=int),\n",
    "                        y_evasion_pred,\n",
    "                        y_annotator_clarity_encoded,  # Compare against annotator's mapped clarity\n",
    "                        EVASION_LABELS,\n",
    "                        clarity_label_list\n",
    "                    )\n",
    "                    \n",
    "                    annotator_results[clf_name] = {\n",
    "                        'metrics': hierarchical_metrics['metrics']\n",
    "                    }\n",
    "                    print(f\"      {clf_name}: Macro F1 = {hierarchical_metrics['metrics'].get('macro_f1', 0.0):.4f}\")\n",
    "                \n",
    "                all_evaluation_results[annotator_name] = annotator_results\n",
    "                \n",
    "        except KeyError as e:\n",
    "            print(f\"    ⚠ WARNING: Could not find annotator columns in test dataset: {e}\")\n",
    "            print(f\"    Skipping annotator-based clarity evaluations...\")\n",
    "            print(f\"    Only clarity and evasion_based_clarity will be shown in tables.\")\n",
    "\n",
    "print(\"\\n✓ All evaluations complete\")\n",
    "\n",
    "# ========================================================================\n",
    "# STEP 2: Create Summary Tables\n",
    "# ========================================================================\n",
    "print(\"\\nStep 2: Creating summary tables...\")\n",
    "\n",
    "# Define task order for tables\n",
    "# If annotator columns exist, show all 5 tasks; otherwise show only 2\n",
    "if 'annotator1_based_clarity' in all_evaluation_results:\n",
    "    all_tasks = ['clarity', 'evasion_based_clarity', 'annotator1_based_clarity', \n",
    "                 'annotator2_based_clarity', 'annotator3_based_clarity']\n",
    "    print(f\"  Tasks: {len(all_tasks)} tasks (including annotator-based clarity)\")\n",
    "else:\n",
    "    all_tasks = ['clarity', 'evasion_based_clarity']\n",
    "    print(f\"  Tasks: {len(all_tasks)} tasks (clarity and evasion_based_clarity only)\")\n",
    "\n",
    "# Create summary DataFrame\n",
    "summary_rows = []\n",
    "for task in all_tasks:\n",
    "    if task not in all_evaluation_results:\n",
    "        continue\n",
    "    for clf_name, result in all_evaluation_results[task].items():\n",
    "        metrics = result['metrics']\n",
    "        summary_rows.append({\n",
    "            'classifier': clf_name,\n",
    "            'task': task,\n",
    "            'macro_f1': metrics.get('macro_f1', 0.0),\n",
    "            'accuracy': metrics.get('accuracy', 0.0),\n",
    "            'macro_precision': metrics.get('macro_precision', 0.0),\n",
    "            'macro_recall': metrics.get('macro_recall', 0.0),\n",
    "        })\n",
    "\n",
    "if not summary_rows:\n",
    "    print(\"  ⚠ WARNING: No results available for summary tables\")\n",
    "else:\n",
    "    df_summary = pd.DataFrame(summary_rows)\n",
    "    \n",
    "    # Remove duplicates (safety)\n",
    "    df_summary = df_summary.drop_duplicates(subset=['classifier', 'task'], keep='first')\n",
    "    \n",
    "    # ====================================================================\n",
    "    # Create Pivot Table: Classifier × Tasks\n",
    "    # ====================================================================\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"FINAL SUMMARY TABLE: Classifier × Tasks (Macro F1)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    try:\n",
    "        df_pivot = df_summary.pivot(index='classifier', columns='task', values='macro_f1')\n",
    "        \n",
    "        # Display table\n",
    "        display(df_pivot.style.format(precision=4))\n",
    "        \n",
    "        # Save table (ensure directory exists)\n",
    "        tables_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Save CSV\n",
    "        pivot_csv_path = tables_dir / 'final_summary_classifier_wise.csv'\n",
    "        df_pivot.to_csv(pivot_csv_path)\n",
    "        print(f\"\\n  ✓ Saved CSV: {pivot_csv_path.name}\")\n",
    "        \n",
    "        # Save HTML\n",
    "        pivot_html_path = tables_dir / 'final_summary_classifier_wise.html'\n",
    "        df_pivot.to_html(pivot_html_path, float_format='{:.4f}'.format)\n",
    "        print(f\"  ✓ Saved HTML: {pivot_html_path.name}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ⚠ Error creating pivot table: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    \n",
    "    # ====================================================================\n",
    "    # Create Detailed Summary Table (all metrics)\n",
    "    # ====================================================================\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"DETAILED SUMMARY TABLE: All Metrics\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    display(df_summary.style.format(precision=4))\n",
    "    \n",
    "    # Save detailed table\n",
    "    tables_dir.mkdir(parents=True, exist_ok=True)\n",
    "    detailed_csv_path = tables_dir / 'final_summary_detailed.csv'\n",
    "    df_summary.to_csv(detailed_csv_path, index=False)\n",
    "    print(f\"\\n  ✓ Saved detailed CSV: {detailed_csv_path.name}\")\n",
    "    \n",
    "    detailed_html_path = tables_dir / 'final_summary_detailed.html'\n",
    "    df_summary.to_html(detailed_html_path, index=False, float_format='{:.4f}'.format)\n",
    "    print(f\"  ✓ Saved detailed HTML: {detailed_html_path.name}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL SUMMARY TABLES COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nTables saved to: {tables_dir}\")\n",
    "print(f\"  - final_summary_classifier_wise.csv/html (Pivot: Classifier × Tasks)\")\n",
    "print(f\"  - final_summary_detailed.csv/html (All metrics)\")\n",
    "print(f\"\\nTasks included: {', '.join(all_tasks)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell is intentionally left empty (placeholder for future use)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
