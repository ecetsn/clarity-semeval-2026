{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Feature Extraction: Context Tree Features for Individual Models\n",
        "\n",
        "================================================================================\n",
        "PURPOSE: Extract 19 Context Tree features for each transformer model separately\n",
        "================================================================================\n",
        "\n",
        "This notebook extracts Context Tree features from question-answer pairs using\n",
        "four different transformer models. The features capture attention patterns,\n",
        "lexical properties, and semantic relationships between questions and answers.\n",
        "\n",
        "**Models:**\n",
        "- BERT (bert-base-uncased)\n",
        "- RoBERTa (roberta-base)\n",
        "- DeBERTa (microsoft/deberta-v3-base)\n",
        "- XLNet (xlnet-base-cased)\n",
        "\n",
        "**Tasks:**\n",
        "- Clarity: 3-class classification (Clear Reply, Ambiguous, Clear Non-Reply)\n",
        "- Evasion: 9-class classification (Direct Answer, Partial Answer, etc.)\n",
        "\n",
        "**Feature Categories:**\n",
        "1. Attention-based features (attention mass, focus strength)\n",
        "2. Pattern-based features (TF-IDF similarity, content word ratios)\n",
        "3. Lexicon-based features (answer lexicon ratios, negation ratios)\n",
        "\n",
        "**Output:** Feature matrices saved to Google Drive for each model/task/split\n",
        "combination. Features are extracted for Train and Dev splits only. Test split\n",
        "features will be extracted in the final evaluation notebook.\n",
        "\n",
        "================================================================================\n",
        "INPUTS (What this notebook loads)\n",
        "================================================================================\n",
        "\n",
        "**From GitHub:**\n",
        "- Repository code (cloned automatically if not present)\n",
        "- Source modules from `src/` directory:\n",
        "  - `src.storage.manager` (StorageManager)\n",
        "  - `src.features.extraction` (feature extraction functions)\n",
        "\n",
        "**From HuggingFace Hub:**\n",
        "- Transformer models (loaded on-the-fly):\n",
        "  - BERT, RoBERTa, DeBERTa, XLNet tokenizers and models\n",
        "\n",
        "**From Google Drive:**\n",
        "- Dataset splits: `splits/dataset_splits.pkl`\n",
        "  - Train split (loaded from 01_data_split.ipynb output)\n",
        "  - Dev split (loaded from 01_data_split.ipynb output)\n",
        "  - Test split (loaded but not used for feature extraction)\n",
        "\n",
        "================================================================================\n",
        "OUTPUTS (What this notebook saves)\n",
        "================================================================================\n",
        "\n",
        "**To Google Drive:**\n",
        "- Feature matrices: `features/raw/X_{split}_{model}_{task}.npy`\n",
        "  - For each model (bert, roberta, deberta, xlnet)\n",
        "  - For each task (clarity, evasion)\n",
        "  - For each split (train, dev)\n",
        "  - Shape: (N_samples, 19_features)\n",
        "\n",
        "**To GitHub:**\n",
        "- Feature metadata: `metadata/features_{split}_{model}_{task}.json`\n",
        "  - Feature names (19 features)\n",
        "  - Feature dimensions\n",
        "  - Timestamp and data paths\n",
        "\n",
        "**What gets passed to next notebook:**\n",
        "- Feature matrices for Train and Dev splits\n",
        "- Feature metadata for all model/task/split combinations\n",
        "- These features are loaded by subsequent notebooks via `storage.load_features()`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# SETUP: Repository Clone, Drive Mount, and Path Configuration\n",
        "# ============================================================================\n",
        "# This cell performs minimal setup required for the notebook to run:\n",
        "# 1. Clones repository from GitHub (if not already present)\n",
        "# 2. Mounts Google Drive for persistent data storage\n",
        "# 3. Configures Python paths and initializes StorageManager\n",
        "# 4. Loads data splits created in 01_data_split.ipynb\n",
        "\n",
        "import shutil\n",
        "import os\n",
        "import subprocess\n",
        "import time\n",
        "import requests\n",
        "import zipfile\n",
        "import sys\n",
        "from pathlib import Path\n",
        "from google.colab import drive\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "# Repository configuration\n",
        "repo_dir = '/content/semeval-context-tree-modular'\n",
        "repo_url = 'https://github.com/EonTechie/semeval-context-tree-modular.git'\n",
        "zip_url = 'https://github.com/EonTechie/semeval-context-tree-modular/archive/refs/heads/main.zip'\n",
        "\n",
        "# Clone repository (if not already present)\n",
        "if not os.path.exists(repo_dir):\n",
        "    print(\"Cloning repository from GitHub...\")\n",
        "    max_retries = 2\n",
        "    clone_success = False\n",
        "    \n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            result = subprocess.run(\n",
        "                ['git', 'clone', repo_url],\n",
        "                cwd='/content',\n",
        "                capture_output=True,\n",
        "                text=True,\n",
        "                timeout=60\n",
        "            )\n",
        "            if result.returncode == 0:\n",
        "                print(\"Repository cloned successfully via git\")\n",
        "                clone_success = True\n",
        "                break\n",
        "            else:\n",
        "                if attempt < max_retries - 1:\n",
        "                    time.sleep(3)\n",
        "        except Exception as e:\n",
        "            if attempt < max_retries - 1:\n",
        "                time.sleep(3)\n",
        "    \n",
        "    # Fallback: Download as ZIP if git clone fails\n",
        "    if not clone_success:\n",
        "        print(\"Git clone failed. Downloading repository as ZIP archive...\")\n",
        "        zip_path = '/tmp/repo.zip'\n",
        "        try:\n",
        "            response = requests.get(zip_url, stream=True, timeout=60)\n",
        "            response.raise_for_status()\n",
        "            with open(zip_path, 'wb') as f:\n",
        "                for chunk in response.iter_content(chunk_size=8192):\n",
        "                    f.write(chunk)\n",
        "            with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "                zip_ref.extractall('/content')\n",
        "            extracted_dir = '/content/semeval-context-tree-modular-main'\n",
        "            if os.path.exists(extracted_dir):\n",
        "                os.rename(extracted_dir, repo_dir)\n",
        "            os.remove(zip_path)\n",
        "            print(\"Repository downloaded and extracted successfully\")\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(f\"Failed to obtain repository: {e}\")\n",
        "\n",
        "# Mount Google Drive (if not already mounted)\n",
        "try:\n",
        "    drive.mount('/content/drive', force_remount=False)\n",
        "except Exception:\n",
        "    pass  # Already mounted\n",
        "\n",
        "# Configure paths\n",
        "BASE_PATH = Path('/content/semeval-context-tree-modular')\n",
        "DATA_PATH = Path('/content/drive/MyDrive/semeval_data')\n",
        "\n",
        "# Verify repository structure exists\n",
        "if not BASE_PATH.exists():\n",
        "    raise RuntimeError(f\"Repository directory not found: {BASE_PATH}\")\n",
        "if not (BASE_PATH / 'src').exists():\n",
        "    raise RuntimeError(f\"src directory not found in repository: {BASE_PATH / 'src'}\")\n",
        "if not (BASE_PATH / 'src' / 'storage' / 'manager.py').exists():\n",
        "    raise RuntimeError(f\"Required file not found: {BASE_PATH / 'src' / 'storage' / 'manager.py'}\")\n",
        "\n",
        "# Add repository to Python path\n",
        "sys.path.insert(0, str(BASE_PATH))\n",
        "\n",
        "# Verify imports work\n",
        "try:\n",
        "    from src.storage.manager import StorageManager\n",
        "    from src.features.extraction import featurize_hf_dataset_in_batches_v2\n",
        "except ImportError as e:\n",
        "    raise ImportError(\n",
        "        f\"Failed to import required modules. \"\n",
        "        f\"Repository path: {BASE_PATH}, \"\n",
        "        f\"Python path: {sys.path[:3]}, \"\n",
        "        f\"Error: {e}\"\n",
        "    )\n",
        "\n",
        "# Initialize StorageManager\n",
        "storage = StorageManager(\n",
        "    base_path=str(BASE_PATH),\n",
        "    data_path=str(DATA_PATH),\n",
        "    github_path=str(BASE_PATH)\n",
        ")\n",
        "\n",
        "# Load data splits (created in 01_data_split.ipynb)\n",
        "train_ds = storage.load_split('train')\n",
        "dev_ds = storage.load_split('dev')\n",
        "test_ds = storage.load_split('test')  # Will be used only in final evaluation\n",
        "\n",
        "print(\"Setup complete\")\n",
        "print(f\"  Repository: {BASE_PATH}\")\n",
        "print(f\"  Data storage: {DATA_PATH}\")\n",
        "print(f\"\\nLoaded data splits:\")\n",
        "print(f\"  Train: {len(train_ds)} samples\")\n",
        "print(f\"  Dev: {len(dev_ds)} samples\")\n",
        "print(f\"  Test: {len(test_ds)} samples (reserved for final evaluation)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CONFIGURE MODELS AND TASKS\n",
        "# ============================================================================\n",
        "# Defines the transformer models and tasks for feature extraction\n",
        "# Each model will be loaded from HuggingFace Hub and used to extract features\n",
        "\n",
        "MODELS = {\n",
        "    'bert': {\n",
        "        'name': 'bert-base-uncased',\n",
        "        'display': 'BERT'\n",
        "    },\n",
        "    'bert_political': {\n",
        "        'name': 'bert-base-uncased',  # TODO: Replace with actual political discourse BERT model from HuggingFace\n",
        "        'display': 'BERT-Political'\n",
        "    },\n",
        "    'bert_ambiguity': {\n",
        "        'name': 'bert-base-uncased',  # TODO: Replace with actual ambiguity-focused BERT model from HuggingFace\n",
        "        'display': 'BERT-Ambiguity'\n",
        "    },\n",
        "    'roberta': {\n",
        "        'name': 'roberta-base',\n",
        "        'display': 'RoBERTa'\n",
        "    },\n",
        "    'deberta': {\n",
        "        'name': 'microsoft/deberta-v3-base',\n",
        "        'display': 'DeBERTa'\n",
        "    },\n",
        "    'xlnet': {\n",
        "        'name': 'xlnet-base-cased',\n",
        "        'display': 'XLNet'\n",
        "    }\n",
        "}\n",
        "\n",
        "TASKS = ['clarity', 'evasion']\n",
        "\n",
        "# Configure device (GPU if available, otherwise CPU)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "print(f\"Models to process: {list(MODELS.keys())}\")\n",
        "print(f\"Tasks: {TASKS}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# EXTRACT FEATURES FOR EACH MODEL AND TASK\n",
        "# ============================================================================\n",
        "# Iterates through each transformer model and extracts Context Tree features\n",
        "# Features are extracted for Train and Dev splits only\n",
        "# Test split features will be extracted in the final evaluation notebook\n",
        "\n",
        "for model_key, model_info in MODELS.items():\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"Processing {model_info['display']} ({model_info['name']})\")\n",
        "    print(f\"{'='*80}\")\n",
        "    \n",
        "    # Load tokenizer and model from HuggingFace Hub\n",
        "    print(f\"Loading {model_info['display']} model and tokenizer...\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_info['name'])\n",
        "    model = AutoModel.from_pretrained(model_info['name'])\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    print(f\"Model loaded and moved to {device}\")\n",
        "    \n",
        "    for task in TASKS:\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"Task: {task.upper()}\")\n",
        "        print(f\"{'='*60}\")\n",
        "        \n",
        "        # Extract features for Train and Dev splits\n",
        "        for split_name, split_ds in [('train', train_ds), ('dev', dev_ds)]:\n",
        "            print(f\"\\nExtracting {split_name} features...\")\n",
        "            \n",
        "            # Extract 19 Context Tree features using the model's attention mechanism\n",
        "            # Features include attention patterns, lexical properties, and semantic relationships\n",
        "            X, feature_names, _ = featurize_hf_dataset_in_batches_v2(\n",
        "                split_ds,\n",
        "                tokenizer,\n",
        "                model,\n",
        "                device,\n",
        "                batch_size=8,              # Batch size for feature extraction\n",
        "                max_sequence_length=256,    # Maximum sequence length\n",
        "                question_key='question',    # Key for question text in dataset\n",
        "                answer_key='answer',        # Key for answer text in dataset\n",
        "                show_progress=True          # Show progress bar\n",
        "            )\n",
        "            \n",
        "            # Save features to persistent storage (Google Drive)\n",
        "            storage.save_features(\n",
        "                X, model_key, task, split_name, feature_names\n",
        "            )\n",
        "            \n",
        "            print(f\"  Saved: {X.shape[0]} samples, {X.shape[1]} features\")\n",
        "            print(f\"  Feature names: {len(feature_names)} features\")\n",
        "    \n",
        "    # Free up GPU memory after processing each model\n",
        "    del model, tokenizer\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "    print(f\"\\nMemory cleared after processing {model_info['display']}\")\n",
        "\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(\"Feature extraction complete for all models and tasks\")\n",
        "print(f\"{'='*80}\")\n",
        "print(\"\\nSummary:\")\n",
        "print(\"  - Features extracted for Train and Dev splits\")\n",
        "print(\"  - Features saved to Google Drive for each model/task/split combination\")\n",
        "print(\"  - Test split features will be extracted in final evaluation notebook\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
