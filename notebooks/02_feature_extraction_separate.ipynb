{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Feature Extraction: Context Tree Features for Individual Models\n",
        "\n",
        "================================================================================\n",
        "PURPOSE: Extract 19 Context Tree features for each transformer model separately\n",
        "================================================================================\n",
        "\n",
        "This notebook extracts Context Tree features from question-answer pairs using\n",
        "four different transformer models. The features capture attention patterns,\n",
        "lexical properties, and semantic relationships between questions and answers.\n",
        "\n",
        "**Models:**\n",
        "- BERT (bert-base-uncased)\n",
        "- RoBERTa (roberta-base)\n",
        "- DeBERTa (microsoft/deberta-v3-base)\n",
        "- XLNet (xlnet-base-cased)\n",
        "\n",
        "**Tasks:**\n",
        "- Clarity: 3-class classification (Clear Reply, Ambiguous, Clear Non-Reply)\n",
        "- Evasion: 9-class classification (Direct Answer, Partial Answer, etc.)\n",
        "\n",
        "**Feature Categories:**\n",
        "1. Attention-based features (attention mass, focus strength)\n",
        "2. Pattern-based features (TF-IDF similarity, content word ratios)\n",
        "3. Lexicon-based features (answer lexicon ratios, negation ratios)\n",
        "\n",
        "**Output:** Feature matrices saved to Google Drive for each model/task/split\n",
        "combination. Features are extracted for Train and Dev splits only. Test split\n",
        "features will be extracted in the final evaluation notebook.\n",
        "\n",
        "================================================================================\n",
        "INPUTS (What this notebook loads)\n",
        "================================================================================\n",
        "\n",
        "**From GitHub:**\n",
        "- Repository code (cloned automatically if not present)\n",
        "- Source modules from `src/` directory:\n",
        "  - `src.storage.manager` (StorageManager)\n",
        "  - `src.features.extraction` (feature extraction functions)\n",
        "\n",
        "**From HuggingFace Hub:**\n",
        "- Transformer models (loaded on-the-fly):\n",
        "  - BERT, RoBERTa, DeBERTa, XLNet tokenizers and models\n",
        "\n",
        "**From Google Drive:**\n",
        "- Dataset splits: `splits/dataset_splits.pkl`\n",
        "  - Train split (loaded from 01_data_split.ipynb output)\n",
        "  - Dev split (loaded from 01_data_split.ipynb output)\n",
        "  - Test split (loaded but not used for feature extraction)\n",
        "\n",
        "================================================================================\n",
        "OUTPUTS (What this notebook saves)\n",
        "================================================================================\n",
        "\n",
        "**To Google Drive:**\n",
        "- Feature matrices: `features/raw/X_{split}_{model}_{task}.npy`\n",
        "  - For each model (bert, roberta, deberta, xlnet)\n",
        "  - For each task (clarity, evasion)\n",
        "  - For each split (train, dev)\n",
        "  - Shape: (N_samples, 19_features)\n",
        "\n",
        "**To GitHub:**\n",
        "- Feature metadata: `metadata/features_{split}_{model}_{task}.json`\n",
        "  - Feature names (19 features)\n",
        "  - Feature dimensions\n",
        "  - Timestamp and data paths\n",
        "\n",
        "**What gets passed to next notebook:**\n",
        "- Feature matrices for Train and Dev splits\n",
        "- Feature metadata for all model/task/split combinations\n",
        "- These features are loaded by subsequent notebooks via `storage.load_features()`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# SETUP: Repository Clone, Drive Mount, and Path Configuration\n",
        "# ============================================================================\n",
        "# This cell performs minimal setup required for the notebook to run:\n",
        "# 1. Clones repository from GitHub (if not already present)\n",
        "# 2. Mounts Google Drive for persistent data storage\n",
        "# 3. Configures Python paths and initializes StorageManager\n",
        "# 4. Loads data splits created in 01_data_split.ipynb\n",
        "\n",
        "import shutil\n",
        "import os\n",
        "import subprocess\n",
        "import time\n",
        "import requests\n",
        "import zipfile\n",
        "import sys\n",
        "from pathlib import Path\n",
        "from google.colab import drive\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "# Repository configuration\n",
        "repo_dir = '/content/semeval-context-tree-modular'\n",
        "repo_url = 'https://github.com/EonTechie/semeval-context-tree-modular.git'\n",
        "zip_url = 'https://github.com/EonTechie/semeval-context-tree-modular/archive/refs/heads/main.zip'\n",
        "\n",
        "# Clone repository (if not already present)\n",
        "if not os.path.exists(repo_dir):\n",
        "    print(\"Cloning repository from GitHub...\")\n",
        "    max_retries = 2\n",
        "    clone_success = False\n",
        "    \n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            result = subprocess.run(\n",
        "                ['git', 'clone', repo_url],\n",
        "                cwd='/content',\n",
        "                capture_output=True,\n",
        "                text=True,\n",
        "                timeout=60\n",
        "            )\n",
        "            if result.returncode == 0:\n",
        "                print(\"Repository cloned successfully via git\")\n",
        "                clone_success = True\n",
        "                break\n",
        "            else:\n",
        "                if attempt < max_retries - 1:\n",
        "                    time.sleep(3)\n",
        "        except Exception as e:\n",
        "            if attempt < max_retries - 1:\n",
        "                time.sleep(3)\n",
        "    \n",
        "    # Fallback: Download as ZIP if git clone fails\n",
        "    if not clone_success:\n",
        "        print(\"Git clone failed. Downloading repository as ZIP archive...\")\n",
        "        zip_path = '/tmp/repo.zip'\n",
        "        try:\n",
        "            response = requests.get(zip_url, stream=True, timeout=60)\n",
        "            response.raise_for_status()\n",
        "            with open(zip_path, 'wb') as f:\n",
        "                for chunk in response.iter_content(chunk_size=8192):\n",
        "                    f.write(chunk)\n",
        "            with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "                zip_ref.extractall('/content')\n",
        "            extracted_dir = '/content/semeval-context-tree-modular-main'\n",
        "            if os.path.exists(extracted_dir):\n",
        "                os.rename(extracted_dir, repo_dir)\n",
        "            os.remove(zip_path)\n",
        "            print(\"Repository downloaded and extracted successfully\")\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(f\"Failed to obtain repository: {e}\")\n",
        "\n",
        "# Mount Google Drive (if not already mounted)\n",
        "try:\n",
        "    drive.mount('/content/drive', force_remount=False)\n",
        "except Exception:\n",
        "    pass  # Already mounted\n",
        "\n",
        "# Configure paths\n",
        "BASE_PATH = Path('/content/semeval-context-tree-modular')\n",
        "DATA_PATH = Path('/content/drive/MyDrive/semeval_data')\n",
        "\n",
        "# Verify repository structure exists\n",
        "if not BASE_PATH.exists():\n",
        "    raise RuntimeError(f\"Repository directory not found: {BASE_PATH}\")\n",
        "if not (BASE_PATH / 'src').exists():\n",
        "    raise RuntimeError(f\"src directory not found in repository: {BASE_PATH / 'src'}\")\n",
        "if not (BASE_PATH / 'src' / 'storage' / 'manager.py').exists():\n",
        "    raise RuntimeError(f\"Required file not found: {BASE_PATH / 'src' / 'storage' / 'manager.py'}\")\n",
        "\n",
        "# Add repository to Python path\n",
        "sys.path.insert(0, str(BASE_PATH))\n",
        "\n",
        "# Verify imports work\n",
        "try:\n",
        "    from src.storage.manager import StorageManager\n",
        "    from src.features.extraction import featurize_hf_dataset_in_batches_v2\n",
        "except ImportError as e:\n",
        "    raise ImportError(\n",
        "        f\"Failed to import required modules. \"\n",
        "        f\"Repository path: {BASE_PATH}, \"\n",
        "        f\"Python path: {sys.path[:3]}, \"\n",
        "        f\"Error: {e}\"\n",
        "    )\n",
        "\n",
        "# Initialize StorageManager\n",
        "storage = StorageManager(\n",
        "    base_path=str(BASE_PATH),\n",
        "    data_path=str(DATA_PATH),\n",
        "    github_path=str(BASE_PATH)\n",
        ")\n",
        "\n",
        "# Data splits will be loaded per-task in the feature extraction loop\n",
        "# Clarity and Evasion have different splits (Evasion uses majority voting)\n",
        "\n",
        "print(\"Setup complete\")\n",
        "print(f\"  Repository: {BASE_PATH}\")\n",
        "print(f\"  Data storage: {DATA_PATH}\")\n",
        "print(f\"\\nNOTE: Data splits will be loaded per-task (task-specific splits)\")\n",
        "print(f\"      Clarity and Evasion have different splits due to majority voting\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CONFIGURE MODELS AND TASKS\n",
        "# ============================================================================\n",
        "# Defines the transformer models and tasks for feature extraction\n",
        "# Each model will be loaded from HuggingFace Hub and used to extract features\n",
        "\n",
        "MODELS = {\n",
        "    'bert': {\n",
        "        'name': 'bert-base-uncased',\n",
        "        'display': 'BERT'\n",
        "    },\n",
        "    'bert_political': {\n",
        "        'name': 'bert-base-uncased',  # TODO: Replace with actual political discourse BERT model from HuggingFace\n",
        "        'display': 'BERT-Political'\n",
        "    },\n",
        "    'bert_ambiguity': {\n",
        "        'name': 'bert-base-uncased',  # TODO: Replace with actual ambiguity-focused BERT model from HuggingFace\n",
        "        'display': 'BERT-Ambiguity'\n",
        "    },\n",
        "    'roberta': {\n",
        "        'name': 'roberta-base',\n",
        "        'display': 'RoBERTa'\n",
        "    },\n",
        "    'deberta': {\n",
        "        'name': 'microsoft/deberta-v3-base',\n",
        "        'display': 'DeBERTa'\n",
        "    },\n",
        "    'xlnet': {\n",
        "        'name': 'xlnet-base-cased',\n",
        "        'display': 'XLNet'\n",
        "    }\n",
        "}\n",
        "\n",
        "# Explicit max sequence length for each model (to avoid tokenizer issues)\n",
        "# These values are model-specific and must be set correctly to prevent OverflowError\n",
        "MODEL_MAX_LENGTHS = {\n",
        "    'bert': 512,\n",
        "    'bert_political': 512,\n",
        "    'bert_ambiguity': 512,\n",
        "    'roberta': 512,\n",
        "    'deberta': 512,\n",
        "    'xlnet': 1024  # XLNet supports 1024 tokens\n",
        "}\n",
        "\n",
        "TASKS = ['clarity', 'evasion']\n",
        "\n",
        "# Configure device (GPU if available, otherwise CPU)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "print(f\"Models to process: {list(MODELS.keys())}\")\n",
        "print(f\"Tasks: {TASKS}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# EXTRACT FEATURES FOR EACH MODEL AND TASK\n",
        "# ============================================================================\n",
        "# Iterates through each transformer model and extracts Context Tree features\n",
        "# Features are extracted for Train and Dev splits only\n",
        "# Test split features will be extracted in the final evaluation notebook\n",
        "\n",
        "for model_key, model_info in MODELS.items():\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"Processing {model_info['display']} ({model_info['name']})\")\n",
        "    print(f\"{'='*80}\")\n",
        "    \n",
        "    # Load tokenizer and model from HuggingFace Hub\n",
        "    print(f\"Loading {model_info['display']} model and tokenizer...\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_info['name'])\n",
        "    model = AutoModel.from_pretrained(model_info['name'])\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    \n",
        "    # Get model-specific max sequence length\n",
        "    # Priority: 1) Explicit MODEL_MAX_LENGTHS dict, 2) tokenizer.model_max_length, 3) model.config.max_position_embeddings\n",
        "    # This ensures each model gets the correct max_length and prevents OverflowError from negative values\n",
        "    if model_key in MODEL_MAX_LENGTHS:\n",
        "        max_seq_len = MODEL_MAX_LENGTHS[model_key]\n",
        "    elif hasattr(tokenizer, 'model_max_length') and tokenizer.model_max_length is not None and tokenizer.model_max_length > 0 and tokenizer.model_max_length < 1e10:\n",
        "        max_seq_len = int(tokenizer.model_max_length)\n",
        "    elif hasattr(model.config, 'max_position_embeddings') and model.config.max_position_embeddings is not None and model.config.max_position_embeddings > 0:\n",
        "        max_seq_len = int(model.config.max_position_embeddings)\n",
        "    else:\n",
        "        # Final fallback: use default based on model type\n",
        "        if 'xlnet' in model_info['name'].lower():\n",
        "            max_seq_len = 1024  # XLNet typically supports 1024\n",
        "        else:\n",
        "            max_seq_len = 512   # BERT, RoBERTa, DeBERTa typically 512\n",
        "    \n",
        "    # Ensure max_seq_len is positive (prevent OverflowError)\n",
        "    if max_seq_len <= 0:\n",
        "        raise ValueError(f\"Invalid max_seq_len for {model_key}: {max_seq_len}. Must be positive.\")\n",
        "    \n",
        "    print(f\"Model loaded and moved to {device}\")\n",
        "    print(f\"Max sequence length for {model_info['display']}: {max_seq_len}\")\n",
        "    \n",
        "    for task in TASKS:\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"Task: {task.upper()}\")\n",
        "        print(f\"{'='*60}\")\n",
        "        \n",
        "        # Load task-specific splits (Clarity and Evasion have different splits)\n",
        "        # Evasion splits are filtered by majority voting\n",
        "        train_ds = storage.load_split('train', task=task)\n",
        "        dev_ds = storage.load_split('dev', task=task)\n",
        "        \n",
        "        print(f\"  Loaded splits for {task}:\")\n",
        "        print(f\"    Train: {len(train_ds)} samples\")\n",
        "        print(f\"    Dev: {len(dev_ds)} samples\")\n",
        "        \n",
        "        # Check if features already exist (skip if already extracted)\n",
        "        try:\n",
        "            X_train_existing = storage.load_features(model_key, task, 'train')\n",
        "            X_dev_existing = storage.load_features(model_key, task, 'dev')\n",
        "            print(f\"  Features already exist for {model_key} Ã— {task}\")\n",
        "            print(f\"    Train: {X_train_existing.shape[0]} samples, {X_train_existing.shape[1]} features\")\n",
        "            print(f\"    Dev: {X_dev_existing.shape[0]} samples, {X_dev_existing.shape[1]} features\")\n",
        "            print(f\"  SKIPPING feature extraction (already done)\")\n",
        "            continue\n",
        "        except FileNotFoundError:\n",
        "            # Features don't exist, proceed with extraction\n",
        "            pass\n",
        "        \n",
        "        # Initialize TF-IDF vectorizer for this task (fit on train split only)\n",
        "        # Each task gets its own TF-IDF vectorizer (as in siparismaili01)\n",
        "        tfidf_vectorizer = None\n",
        "        \n",
        "        # Extract features for Train split first (to fit TF-IDF)\n",
        "        print(f\"\\nExtracting train features (fitting TF-IDF for {task})...\")\n",
        "        X_train, feature_names, tfidf_vectorizer = featurize_hf_dataset_in_batches_v2(\n",
        "            train_ds,\n",
        "            tokenizer,\n",
        "            model,\n",
        "            device,\n",
        "            batch_size=8,              # Batch size for feature extraction\n",
        "            max_sequence_length=max_seq_len,  # Model-specific max sequence length\n",
        "            question_key='interview_question',  # Key for question text in dataset (original question, NOT 'question' which is paraphrased)\n",
        "            answer_key='interview_answer',  # Key for answer text in dataset (QEvasion uses 'interview_answer')\n",
        "            show_progress=True,         # Show progress bar\n",
        "            tfidf_vectorizer=None      # Fit new TF-IDF on train for this task\n",
        "        )\n",
        "        \n",
        "        # Save train features\n",
        "        storage.save_features(\n",
        "            X_train, model_key, task, 'train', feature_names\n",
        "        )\n",
        "        print(f\"  Saved train: {X_train.shape[0]} samples, {X_train.shape[1]} features\")\n",
        "        \n",
        "        # Extract features for Dev split (reuse TF-IDF fitted on train)\n",
        "        print(f\"\\nExtracting dev features (using TF-IDF fitted on train for {task})...\")\n",
        "        X_dev, _, _ = featurize_hf_dataset_in_batches_v2(\n",
        "            dev_ds,\n",
        "            tokenizer,\n",
        "            model,\n",
        "            device,\n",
        "            batch_size=8,              # Batch size for feature extraction\n",
        "            max_sequence_length=max_seq_len,  # Model-specific max sequence length\n",
        "            question_key='interview_question',  # Key for question text in dataset (original question, NOT 'question' which is paraphrased)\n",
        "            answer_key='interview_answer',  # Key for answer text in dataset (QEvasion uses 'interview_answer')\n",
        "            show_progress=True,         # Show progress bar\n",
        "            tfidf_vectorizer=tfidf_vectorizer  # Reuse TF-IDF from train (no leakage)\n",
        "        )\n",
        "        \n",
        "        # Save dev features\n",
        "        storage.save_features(\n",
        "            X_dev, model_key, task, 'dev', feature_names\n",
        "        )\n",
        "        print(f\"  Saved dev: {X_dev.shape[0]} samples, {X_dev.shape[1]} features\")\n",
        "    \n",
        "    # Free up GPU memory after processing each model\n",
        "    del model, tokenizer\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "    print(f\"\\nMemory cleared after processing {model_info['display']}\")\n",
        "\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(\"Feature extraction complete for all models and tasks\")\n",
        "print(f\"{'='*80}\")\n",
        "print(\"\\nSummary:\")\n",
        "print(\"  - Features extracted for Train and Dev splits\")\n",
        "print(\"  - Features saved to Google Drive for each model/task/split combination\")\n",
        "print(\"  - Test split features will be extracted in final evaluation notebook\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
