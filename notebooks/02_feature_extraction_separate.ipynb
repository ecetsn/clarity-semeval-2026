{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size: 8px;\">\n",
    "\n",
    "# Feature Extraction: Context Tree Features for Individual Models\n",
    "\n",
    "================================================================================\n",
    "PURPOSE: Extract 19 Context Tree features for each transformer model separately\n",
    "================================================================================\n",
    "\n",
    "This notebook extracts Context Tree features from question-answer pairs using\n",
    "four different transformer models. The features capture attention patterns,\n",
    "lexical properties, and semantic relationships between questions and answers.\n",
    "\n",
    "**Models:**\n",
    "- BERT (bert-base-uncased)\n",
    "- RoBERTa (roberta-base)\n",
    "- DeBERTa (microsoft/deberta-v3-base)\n",
    "- XLNet (xlnet-base-cased)\n",
    "\n",
    "**Tasks:**\n",
    "- Clarity: 3-class classification (Clear Reply, Ambiguous, Clear Non-Reply)\n",
    "- Evasion: 9-class classification (Direct Answer, Partial Answer, etc.)\n",
    "\n",
    "**Feature Categories:**\n",
    "1. Attention-based features (attention mass, focus strength)\n",
    "2. Pattern-based features (TF-IDF similarity, content word ratios)\n",
    "3. Lexicon-based features (answer lexicon ratios, negation ratios)\n",
    "\n",
    "**Output:** Feature matrices saved to Google Drive for each model/task/split\n",
    "combination. Features are extracted for Train and Dev splits only. Test split\n",
    "features will be extracted in the final evaluation notebook.\n",
    "\n",
    "================================================================================\n",
    "INPUTS (What this notebook loads)\n",
    "================================================================================\n",
    "\n",
    "**From GitHub:**\n",
    "- Repository code (cloned automatically if not present)\n",
    "- Source modules from `src/` directory:\n",
    "  - `src.storage.manager` (StorageManager)\n",
    "  - `src.features.extraction` (feature extraction functions)\n",
    "\n",
    "**From HuggingFace Hub:**\n",
    "- Transformer models (loaded on-the-fly):\n",
    "  - BERT, RoBERTa, DeBERTa, XLNet tokenizers and models\n",
    "\n",
    "**From Google Drive:**\n",
    "- Dataset splits: `splits/dataset_splits.pkl`\n",
    "  - Train split (loaded from 01_data_split.ipynb output)\n",
    "  - Dev split (loaded from 01_data_split.ipynb output)\n",
    "  - Test split (loaded but not used for feature extraction)\n",
    "\n",
    "================================================================================\n",
    "OUTPUTS (What this notebook saves)\n",
    "================================================================================\n",
    "\n",
    "**To Google Drive:**\n",
    "- Feature matrices: `features/raw/X_{split}_{model}_{task}.npy`\n",
    "  - For each model (bert, roberta, deberta, xlnet)\n",
    "  - For each task (clarity, evasion)\n",
    "  - For each split (train, dev)\n",
    "  - Shape: (N_samples, 19_features)\n",
    "\n",
    "**To GitHub:**\n",
    "- Feature metadata: `metadata/features_{split}_{model}_{task}.json`\n",
    "  - Feature names (19 features)\n",
    "  - Feature dimensions\n",
    "  - Timestamp and data paths\n",
    "\n",
    "**What gets passed to next notebook:**\n",
    "- Feature matrices for Train and Dev splits\n",
    "- Feature metadata for all model/task/split combinations\n",
    "- These features are loaded by subsequent notebooks via `storage.load_features()`\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SETUP: Repository Clone, Drive Mount, and Path Configuration\n",
    "# ============================================================================\n",
    "# This cell performs minimal setup required for the notebook to run:\n",
    "# 1. Clones repository from GitHub (if not already present)\n",
    "# 2. Mounts Google Drive for persistent data storage\n",
    "# 3. Configures Python paths and initializes StorageManager\n",
    "# 4. Loads data splits created in 01_data_split.ipynb\n",
    "\n",
    "import shutil\n",
    "import os\n",
    "import subprocess\n",
    "import time\n",
    "import requests\n",
    "import zipfile\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from google.colab import drive\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# Repository configuration\n",
    "repo_dir = '/content/semeval-context-tree-modular'\n",
    "repo_url = 'https://github.com/EonTechie/semeval-context-tree-modular.git'\n",
    "zip_url = 'https://github.com/EonTechie/semeval-context-tree-modular/archive/refs/heads/main.zip'\n",
    "\n",
    "# Clone repository (if not already present)\n",
    "if not os.path.exists(repo_dir):\n",
    "    print(\"Cloning repository from GitHub...\")\n",
    "    max_retries = 2\n",
    "    clone_success = False\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            result = subprocess.run(\n",
    "                ['git', 'clone', repo_url],\n",
    "                cwd='/content',\n",
    "                capture_output=True,\n",
    "                text=True,\n",
    "                timeout=60\n",
    "            )\n",
    "            if result.returncode == 0:\n",
    "                print(\"Repository cloned successfully via git\")\n",
    "                clone_success = True\n",
    "                break\n",
    "            else:\n",
    "                if attempt < max_retries - 1:\n",
    "                    time.sleep(3)\n",
    "        except Exception as e:\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(3)\n",
    "    \n",
    "    # Fallback: Download as ZIP if git clone fails\n",
    "    if not clone_success:\n",
    "        print(\"Git clone failed. Downloading repository as ZIP archive...\")\n",
    "        zip_path = '/tmp/repo.zip'\n",
    "        try:\n",
    "            response = requests.get(zip_url, stream=True, timeout=60)\n",
    "            response.raise_for_status()\n",
    "            with open(zip_path, 'wb') as f:\n",
    "                for chunk in response.iter_content(chunk_size=8192):\n",
    "                    f.write(chunk)\n",
    "            with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "                zip_ref.extractall('/content')\n",
    "            extracted_dir = '/content/semeval-context-tree-modular-main'\n",
    "            if os.path.exists(extracted_dir):\n",
    "                os.rename(extracted_dir, repo_dir)\n",
    "            os.remove(zip_path)\n",
    "            print(\"Repository downloaded and extracted successfully\")\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to obtain repository: {e}\")\n",
    "\n",
    "# Mount Google Drive (if not already mounted)\n",
    "try:\n",
    "    drive.mount('/content/drive', force_remount=False)\n",
    "except Exception:\n",
    "    pass  # Already mounted\n",
    "\n",
    "# Configure paths\n",
    "BASE_PATH = Path('/content/semeval-context-tree-modular')\n",
    "DATA_PATH = Path('/content/drive/MyDrive/semeval_data')\n",
    "\n",
    "# Verify repository structure exists\n",
    "if not BASE_PATH.exists():\n",
    "    raise RuntimeError(f\"Repository directory not found: {BASE_PATH}\")\n",
    "if not (BASE_PATH / 'src').exists():\n",
    "    raise RuntimeError(f\"src directory not found in repository: {BASE_PATH / 'src'}\")\n",
    "if not (BASE_PATH / 'src' / 'storage' / 'manager.py').exists():\n",
    "    raise RuntimeError(f\"Required file not found: {BASE_PATH / 'src' / 'storage' / 'manager.py'}\")\n",
    "\n",
    "# Add repository to Python path\n",
    "sys.path.insert(0, str(BASE_PATH))\n",
    "\n",
    "# Verify imports work\n",
    "try:\n",
    "    from src.storage.manager import StorageManager\n",
    "    from src.features.extraction import (\n",
    "        featurize_hf_dataset_in_batches_v2,\n",
    "        featurize_model_independent_features\n",
    "    )\n",
    "    from transformers import pipeline\n",
    "except ImportError as e:\n",
    "    raise ImportError(\n",
    "        f\"Failed to import required modules. \"\n",
    "        f\"Repository path: {BASE_PATH}, \"\n",
    "        f\"Python path: {sys.path[:3]}, \"\n",
    "        f\"Error: {e}\"\n",
    "    )\n",
    "\n",
    "# Initialize StorageManager\n",
    "storage = StorageManager(\n",
    "    base_path=str(BASE_PATH),\n",
    "    data_path=str(DATA_PATH),\n",
    "    github_path=str(BASE_PATH)\n",
    ")\n",
    "\n",
    "# Data splits will be loaded per-task in the feature extraction loop\n",
    "# Clarity and Evasion have different splits (Evasion uses majority voting)\n",
    "\n",
    "print(\"Setup complete\")\n",
    "print(f\"  Repository: {BASE_PATH}\")\n",
    "print(f\"  Data storage: {DATA_PATH}\")\n",
    "print(f\"\\nNOTE: Data splits will be loaded per-task (task-specific splits)\")\n",
    "print(f\"      Clarity and Evasion have different splits due to majority voting\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# REPRODUCIBILITY SETUP: Set Random Seeds for All Libraries\n",
    "# ============================================================================\n",
    "# This cell sets random seeds for Python, NumPy, PyTorch, and HuggingFace\n",
    "# to ensure reproducible results across all runs.\n",
    "# \n",
    "# IMPORTANT: Run this cell FIRST before any other code that uses randomness.\n",
    "# Seed value: 42 (same as used in all other parts of the pipeline)\n",
    "\n",
    "from src.utils.reproducibility import set_all_seeds\n",
    "\n",
    "# Set all random seeds to 42 for full reproducibility\n",
    "# deterministic=True ensures PyTorch operations are deterministic (slower but fully reproducible)\n",
    "set_all_seeds(seed=42, deterministic=True)\n",
    "\n",
    "print(\"✓ Reproducibility configured: All random seeds set to 42\")\n",
    "print(\"✓ PyTorch deterministic mode enabled\")\n",
    "print(\"\\nNOTE: If you encounter performance issues or non-deterministic behavior,\")\n",
    "print(\"      you can set deterministic=False in set_all_seeds() call above.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CONFIGURE MODELS AND TASKS\n",
    "# ============================================================================\n",
    "# Defines the transformer models and tasks for feature extraction\n",
    "# Each model will be loaded from HuggingFace Hub and used to extract features\n",
    "\n",
    "MODELS = {\n",
    "    'bert': {\n",
    "        'name': 'bert-base-uncased',\n",
    "        'display': 'BERT'\n",
    "    },\n",
    "    'bert_political': {\n",
    "        'name': 'bert-base-uncased',  # TODO: Replace with actual political discourse BERT model from HuggingFace\n",
    "        'display': 'BERT-Political'\n",
    "    },\n",
    "    'bert_ambiguity': {\n",
    "        'name': 'bert-base-uncased',  # TODO: Replace with actual ambiguity-focused BERT model from HuggingFace\n",
    "        'display': 'BERT-Ambiguity'\n",
    "    },\n",
    "    'roberta': {\n",
    "        'name': 'roberta-base',\n",
    "        'display': 'RoBERTa'\n",
    "    },\n",
    "    'deberta': {\n",
    "        'name': 'microsoft/deberta-v3-base',\n",
    "        'display': 'DeBERTa'\n",
    "    },\n",
    "    'xlnet': {\n",
    "        'name': 'xlnet-base-cased',\n",
    "        'display': 'XLNet'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Explicit max sequence length for each model (to avoid tokenizer issues)\n",
    "# These values are model-specific and must be set correctly to prevent OverflowError\n",
    "MODEL_MAX_LENGTHS = {\n",
    "    'bert': 512,\n",
    "    'bert_political': 512,\n",
    "    'bert_ambiguity': 512,\n",
    "    'roberta': 512,\n",
    "    'deberta': 512,\n",
    "    'xlnet': 1024  # XLNet supports 1024 tokens\n",
    "}\n",
    "\n",
    "TASKS = ['clarity', 'evasion']\n",
    "\n",
    "# Configure device (GPU if available, otherwise CPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"Models to process: {list(MODELS.keys())}\")\n",
    "print(f\"Tasks: {TASKS}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# EXTRACT MODEL-INDEPENDENT FEATURES (ONCE FOR ALL MODELS)\n",
    "# ============================================================================\n",
    "# Model-independent features (TF-IDF, sentiment, structural, metadata) are\n",
    "# extracted once and reused for all models. This is more efficient than\n",
    "# extracting them separately for each model.\n",
    "#\n",
    "# These features are text-based and don't depend on the transformer model.\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"STEP 1: EXTRACT MODEL-INDEPENDENT FEATURES\")\n",
    "print(\"=\"*80)\n",
    "print(\"Extracting model-independent features (TF-IDF, sentiment, structural, metadata)\")\n",
    "print(\"These will be reused for all models to improve efficiency.\\n\")\n",
    "\n",
    "# Load sentiment pipeline (for sentiment features)\n",
    "print(\"Loading sentiment analysis pipeline...\")\n",
    "try:\n",
    "    sentiment_pipeline = pipeline(\n",
    "        \"sentiment-analysis\",\n",
    "        model=\"cardiffnlp/twitter-roberta-base-sentiment-latest\",\n",
    "        device=0 if torch.cuda.is_available() else -1,\n",
    "        return_all_scores=True\n",
    "    )\n",
    "    print(\"  ✓ Sentiment pipeline loaded\")\n",
    "except Exception as e:\n",
    "    print(f\"  ⚠ Could not load sentiment pipeline: {e}\")\n",
    "    print(\"  Continuing without sentiment features...\")\n",
    "    sentiment_pipeline = None\n",
    "\n",
    "# Metadata keys for QEvasion dataset\n",
    "metadata_keys = {\n",
    "    'inaudible': 'inaudible',\n",
    "    'multiple_questions': 'multiple_questions',\n",
    "    'affirmative_questions': 'affirmative_questions'\n",
    "}\n",
    "\n",
    "# Extract model-independent features for each task\n",
    "# CHECKPOINT: Try to load from Drive first, extract only if not exists\n",
    "model_independent_features = {}\n",
    "\n",
    "for task in TASKS:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Task: {task.upper()} - Model-Independent Features\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Load task-specific splits\n",
    "    train_ds = storage.load_split('train', task=task)\n",
    "    dev_ds = storage.load_split('dev', task=task)\n",
    "    \n",
    "    print(f\"  Train: {len(train_ds)} samples\")\n",
    "    print(f\"  Dev: {len(dev_ds)} samples\")\n",
    "    \n",
    "    # CHECKPOINT: Try to load train model-independent features from Drive\n",
    "    try:\n",
    "        X_train_indep = storage.load_model_independent_features('train', task=task)\n",
    "        print(f\"  ✓ Loaded train model-independent features from Drive (task: {task})\")\n",
    "        # Get feature names from metadata\n",
    "        import json\n",
    "        meta_path = storage.github_path / f'metadata/features_independent_train_{task}.json'\n",
    "        if meta_path.exists():\n",
    "            with open(meta_path, 'r') as f:\n",
    "                metadata = json.load(f)\n",
    "                feature_names_indep = metadata.get('feature_names', [])\n",
    "        else:\n",
    "            # Fallback: extract to get feature names\n",
    "            _, feature_names_indep = featurize_model_independent_features(\n",
    "                train_ds, question_key='interview_question', answer_key='interview_answer',\n",
    "                batch_size=32, show_progress=False, sentiment_pipeline=sentiment_pipeline,\n",
    "                metadata_keys=metadata_keys\n",
    "            )\n",
    "    except FileNotFoundError:\n",
    "        # Extract if not found\n",
    "        print(f\"\\n  Extracting train model-independent features (task: {task})...\")\n",
    "        X_train_indep, feature_names_indep = featurize_model_independent_features(\n",
    "            train_ds,\n",
    "            question_key='interview_question',\n",
    "            answer_key='interview_answer',\n",
    "            batch_size=32,  # Larger batch for model-independent features\n",
    "            show_progress=True,\n",
    "            sentiment_pipeline=sentiment_pipeline,\n",
    "            metadata_keys=metadata_keys\n",
    "        )\n",
    "        # Save to Drive for future use\n",
    "        storage.save_model_independent_features(\n",
    "            X_train_indep, 'train', feature_names_indep, task=task, question_key='interview_question'\n",
    "        )\n",
    "        print(f\"  ✓ Saved train model-independent features to Drive (task: {task})\")\n",
    "    \n",
    "    # CHECKPOINT: Try to load dev model-independent features from Drive\n",
    "    try:\n",
    "        X_dev_indep = storage.load_model_independent_features('dev', task=task)\n",
    "        print(f\"  ✓ Loaded dev model-independent features from Drive (task: {task})\")\n",
    "    except FileNotFoundError:\n",
    "        # Extract if not found\n",
    "        print(f\"\\n  Extracting dev model-independent features (task: {task})...\")\n",
    "        X_dev_indep, _ = featurize_model_independent_features(\n",
    "            dev_ds,\n",
    "            question_key='interview_question',\n",
    "            answer_key='interview_answer',\n",
    "            batch_size=32,\n",
    "            show_progress=True,\n",
    "            sentiment_pipeline=sentiment_pipeline,\n",
    "            metadata_keys=metadata_keys\n",
    "        )\n",
    "        # Save to Drive for future use\n",
    "        storage.save_model_independent_features(\n",
    "            X_dev_indep, 'dev', feature_names_indep, task=task, question_key='interview_question'\n",
    "        )\n",
    "        print(f\"  ✓ Saved dev model-independent features to Drive (task: {task})\")\n",
    "    \n",
    "    # Store for reuse across all models\n",
    "    model_independent_features[task] = {\n",
    "        'train': X_train_indep,\n",
    "        'dev': X_dev_indep,\n",
    "        'feature_names': feature_names_indep\n",
    "    }\n",
    "    \n",
    "    print(f\"  ✓ Train: {X_train_indep.shape[0]} samples, {X_train_indep.shape[1]} features\")\n",
    "    print(f\"  ✓ Dev: {X_dev_indep.shape[0]} samples, {X_dev_indep.shape[1]} features\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"Model-independent features extracted for all tasks\")\n",
    "print(\"These will be reused for all models (efficiency mode)\")\n",
    "print(f\"{'='*80}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# EXTRACT FEATURES FOR EACH MODEL AND TASK\n",
    "# ============================================================================\n",
    "# Iterates through each transformer model and extracts Context Tree features\n",
    "# Features are extracted for Train and Dev splits only\n",
    "# Test split features will be extracted in the final evaluation notebook\n",
    "\n",
    "for model_key, model_info in MODELS.items():\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Processing {model_info['display']} ({model_info['name']})\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Load tokenizer and model from HuggingFace Hub\n",
    "    print(f\"Loading {model_info['display']} model and tokenizer...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_info['name'])\n",
    "    model = AutoModel.from_pretrained(model_info['name'])\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    # Get model-specific max sequence length\n",
    "    # Priority: 1) Explicit MODEL_MAX_LENGTHS dict, 2) tokenizer.model_max_length, 3) model.config.max_position_embeddings\n",
    "    # This ensures each model gets the correct max_length and prevents OverflowError from negative values\n",
    "    if model_key in MODEL_MAX_LENGTHS:\n",
    "        max_seq_len = MODEL_MAX_LENGTHS[model_key]\n",
    "    elif hasattr(tokenizer, 'model_max_length') and tokenizer.model_max_length is not None and tokenizer.model_max_length > 0 and tokenizer.model_max_length < 1e10:\n",
    "        max_seq_len = int(tokenizer.model_max_length)\n",
    "    elif hasattr(model.config, 'max_position_embeddings') and model.config.max_position_embeddings is not None and model.config.max_position_embeddings > 0:\n",
    "        max_seq_len = int(model.config.max_position_embeddings)\n",
    "    else:\n",
    "        # Final fallback: use default based on model type\n",
    "        if 'xlnet' in model_info['name'].lower():\n",
    "            max_seq_len = 1024  # XLNet typically supports 1024\n",
    "        else:\n",
    "            max_seq_len = 512   # BERT, RoBERTa, DeBERTa typically 512\n",
    "    \n",
    "    # Ensure max_seq_len is positive (prevent OverflowError)\n",
    "    if max_seq_len <= 0:\n",
    "        raise ValueError(f\"Invalid max_seq_len for {model_key}: {max_seq_len}. Must be positive.\")\n",
    "    \n",
    "    print(f\"Model loaded and moved to {device}\")\n",
    "    print(f\"Max sequence length for {model_info['display']}: {max_seq_len}\")\n",
    "    \n",
    "    for task in TASKS:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Task: {task.upper()}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # Load task-specific splits (Clarity and Evasion have different splits)\n",
    "        # Evasion splits are filtered by majority voting\n",
    "        train_ds = storage.load_split('train', task=task)\n",
    "        dev_ds = storage.load_split('dev', task=task)\n",
    "        \n",
    "        print(f\"  Loaded splits for {task}:\")\n",
    "        print(f\"    Train: {len(train_ds)} samples\")\n",
    "        print(f\"    Dev: {len(dev_ds)} samples\")\n",
    "        \n",
    "        # Check if features already exist (skip if already extracted)\n",
    "        try:\n",
    "            X_train_existing = storage.load_features(model_key, task, 'train')\n",
    "            X_dev_existing = storage.load_features(model_key, task, 'dev')\n",
    "            print(f\"  Features already exist for {model_key} × {task}\")\n",
    "            print(f\"    Train: {X_train_existing.shape[0]} samples, {X_train_existing.shape[1]} features\")\n",
    "            print(f\"    Dev: {X_dev_existing.shape[0]} samples, {X_dev_existing.shape[1]} features\")\n",
    "            print(f\"  SKIPPING feature extraction (already done)\")\n",
    "            continue\n",
    "        except FileNotFoundError:\n",
    "            # Features don't exist, proceed with extraction\n",
    "            pass\n",
    "        \n",
    "        # EFFICIENCY MODE: Use pre-extracted model-independent features\n",
    "        # Only extract model-dependent features (attention-based, tokenizer-specific)\n",
    "        print(f\"\\nExtracting train features (model-dependent only, using pre-extracted model-independent)...\")\n",
    "        X_train, feature_names, _ = featurize_hf_dataset_in_batches_v2(\n",
    "            train_ds,\n",
    "            tokenizer,\n",
    "            model,\n",
    "            device,\n",
    "            batch_size=8,              # Batch size for feature extraction\n",
    "            max_sequence_length=max_seq_len,  # Model-specific max sequence length\n",
    "            question_key='interview_question',  # Key for question text in dataset (original question, NOT 'question' which is paraphrased)\n",
    "            answer_key='interview_answer',  # Key for answer text in dataset (QEvasion uses 'interview_answer')\n",
    "            show_progress=True,         # Show progress bar\n",
    "            model_independent_features=model_independent_features[task]['train']  # Reuse pre-extracted features\n",
    "        )\n",
    "        \n",
    "        # Save train features\n",
    "        storage.save_features(\n",
    "            X_train, model_key, task, 'train', feature_names\n",
    "        )\n",
    "        print(f\"  Saved train: {X_train.shape[0]} samples, {X_train.shape[1]} features\")\n",
    "        \n",
    "        # Extract features for Dev split (efficiency mode)\n",
    "        print(f\"\\nExtracting dev features (model-dependent only, using pre-extracted model-independent)...\")\n",
    "        X_dev, _, _ = featurize_hf_dataset_in_batches_v2(\n",
    "            dev_ds,\n",
    "            tokenizer,\n",
    "            model,\n",
    "            device,\n",
    "            batch_size=8,              # Batch size for feature extraction\n",
    "            max_sequence_length=max_seq_len,  # Model-specific max sequence length\n",
    "            question_key='interview_question',  # Key for question text in dataset (original question, NOT 'question' which is paraphrased)\n",
    "            answer_key='interview_answer',  # Key for answer text in dataset (QEvasion uses 'interview_answer')\n",
    "            show_progress=True,         # Show progress bar\n",
    "            model_independent_features=model_independent_features[task]['dev']  # Reuse pre-extracted features\n",
    "        )\n",
    "        \n",
    "        # Save dev features\n",
    "        storage.save_features(\n",
    "            X_dev, model_key, task, 'dev', feature_names\n",
    "        )\n",
    "        print(f\"  Saved dev: {X_dev.shape[0]} samples, {X_dev.shape[1]} features\")\n",
    "    \n",
    "    # Free up GPU memory after processing each model\n",
    "    del model, tokenizer\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    print(f\"\\nMemory cleared after processing {model_info['display']}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"Feature extraction complete for all models and tasks\")\n",
    "print(f\"{'='*80}\")\n",
    "print(\"\\nSummary:\")\n",
    "print(\"  - Features extracted for Train and Dev splits\")\n",
    "print(\"  - Features saved to Google Drive for each model/task/split combination\")\n",
    "print(\"  - Test split features will be extracted in final evaluation notebook\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}