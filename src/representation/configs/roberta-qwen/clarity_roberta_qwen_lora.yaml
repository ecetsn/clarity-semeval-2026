# src/representation/configs/roberta-qwen/clarity_roberta_qwen_lora.yaml
experiment:
  name: clarity_roberta_qwen_lora
  task: clarity

logging:
  base_dir: experiments/logs/representation
  run_group: roberta_qwen_lora

outputs:
  base_dir: experiments/representation/roberta_qwen_lora

data:
  dataset_name: ailsntua/QEvasion
  validation_size: 0.2
  max_train_samples: null
  max_val_samples: null
  max_test_samples: null
  num_workers: 0

training:
  seed: 13
  epochs: 5
  batch_size: 2
  lr: 0.0002
  weight_decay: 0.01
  grad_accum_steps: 2
  max_grad_norm: 1.0
  use_amp: true
  log_every_steps: 50

encoder:
  type: roberta
  model_name: roberta-base
  pooling: masked_mean
  freeze: true

decoder:
  type: qwen
  model_name: Qwen/Qwen2-1.5B
  pooling: last_non_pad
  freeze: true
  max_length: 128
  quantization:
    load_in_4bit: false
    load_in_8bit: false
    torch_dtype: bfloat16
  lora:
    enabled: true
    r: 8
    alpha: 16
    dropout: 0.1
    target_modules: ["q_proj","k_proj","v_proj","o_proj"]

projection:
  encoder:
    type: mlp
    params:
      hidden_dim: 256
      output_dim: 256
      n_layers: 3
      dropout: 0.1
      activation: relu
  decoder:
    type: mlp
    params:
      hidden_dim: 256
      output_dim: 256
      n_layers: 3
      dropout: 0.1
      activation: relu

fusion:
  mode: concat

classifier:
  type: mlp
  params:
    hidden_dims: [256, 256, 256]
    activation: relu
    dropout: 0.1

loss:
  type: cross_entropy
  class_weighted: true
