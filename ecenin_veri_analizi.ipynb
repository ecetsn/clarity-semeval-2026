{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3bbba4da",
   "metadata": {},
   "source": [
    "# Advanced Preprocessing & Sentiment-Augmented Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae49962",
   "metadata": {},
   "source": [
    "This notebook expands the preprocessing pipeline for the CLARITY dataset.\n",
    "Goals:\n",
    "- Keep linguistically informative tokens (no stop-word removal) while normalizing casing/spacing.\n",
    "- Engineer structural features (length, overlap, punctuation) useful for ambiguity/evasion cues.\n",
    "- Enrich each sample with sentiment signals from a free Hugging Face transformer.\n",
    "- Prototype a classifier that fuses TF–IDF text with the engineered numeric features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "579815a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in /home/ecetsn/.local/lib/python3.10/site-packages (4.57.1)\n",
      "Requirement already satisfied: filelock in /home/ecetsn/.local/lib/python3.10/site-packages (from transformers) (3.20.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /home/ecetsn/.local/lib/python3.10/site-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ecetsn/.local/lib/python3.10/site-packages (from transformers) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ecetsn/.local/lib/python3.10/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ecetsn/.local/lib/python3.10/site-packages (from transformers) (2025.11.3)\n",
      "Requirement already satisfied: requests in /home/ecetsn/.local/lib/python3.10/site-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /home/ecetsn/.local/lib/python3.10/site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/ecetsn/.local/lib/python3.10/site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ecetsn/.local/lib/python3.10/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/ecetsn/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ecetsn/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/ecetsn/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/ecetsn/.local/lib/python3.10/site-packages (from requests->transformers) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests->transformers) (1.26.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->transformers) (2020.6.20)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: Pillow in /usr/lib/python3/dist-packages (9.0.1)\n",
      "Collecting Pillow\n",
      "  Downloading pillow-12.0.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (8.8 kB)\n",
      "Downloading pillow-12.0.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (7.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m814.2 kB/s\u001b[0m  \u001b[33m0:00:08\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: Pillow\n",
      "Successfully installed Pillow-12.0.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade transformers\n",
    "!pip install --upgrade Pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c7734bce",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'AutoTokenizer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mAutoTokenizer\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'AutoTokenizer'"
     ]
    }
   ],
   "source": [
    "import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77a3ea3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ecetsn/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-11-13 23:35:54.337741: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-11-13 23:35:54.720343: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-11-13 23:35:56.427940: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
    "\n",
    "pd.options.display.max_columns = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92266b43",
   "metadata": {},
   "source": [
    "## Load QEvasion data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5daa5d8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 3,448 rows\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>date</th>\n",
       "      <th>president</th>\n",
       "      <th>url</th>\n",
       "      <th>question_order</th>\n",
       "      <th>interview_question</th>\n",
       "      <th>interview_answer</th>\n",
       "      <th>gpt3.5_summary</th>\n",
       "      <th>gpt3.5_prediction</th>\n",
       "      <th>question</th>\n",
       "      <th>annotator_id</th>\n",
       "      <th>annotator1</th>\n",
       "      <th>annotator2</th>\n",
       "      <th>annotator3</th>\n",
       "      <th>inaudible</th>\n",
       "      <th>multiple_questions</th>\n",
       "      <th>affirmative_questions</th>\n",
       "      <th>index</th>\n",
       "      <th>clarity_label</th>\n",
       "      <th>evasion_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The President's News Conference in Hanoi, Vietnam</td>\n",
       "      <td>September 10, 2023</td>\n",
       "      <td>Joseph R. Biden</td>\n",
       "      <td>https://www.presidency.ucsb.edu/documents/the-...</td>\n",
       "      <td>1</td>\n",
       "      <td>Q. Of the Biden administration. And accused th...</td>\n",
       "      <td>Well, look, first of all, theI am sincere abou...</td>\n",
       "      <td>The question consists of 2 parts: \\n1. How wou...</td>\n",
       "      <td>Question part: 1. How would you respond to the...</td>\n",
       "      <td>How would you respond to the accusation that t...</td>\n",
       "      <td>85</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>Clear Reply</td>\n",
       "      <td>Explicit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The President's News Conference in Hanoi, Vietnam</td>\n",
       "      <td>September 10, 2023</td>\n",
       "      <td>Joseph R. Biden</td>\n",
       "      <td>https://www.presidency.ucsb.edu/documents/the-...</td>\n",
       "      <td>1</td>\n",
       "      <td>Q. Of the Biden administration. And accused th...</td>\n",
       "      <td>Well, look, first of all, theI am sincere abou...</td>\n",
       "      <td>The question consists of 2 parts: \\n1. How wou...</td>\n",
       "      <td>Question part: 1. How would you respond to the...</td>\n",
       "      <td>Do you think President Xi is being sincere abo...</td>\n",
       "      <td>85</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>Ambivalent</td>\n",
       "      <td>General</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title                date  \\\n",
       "0  The President's News Conference in Hanoi, Vietnam  September 10, 2023   \n",
       "1  The President's News Conference in Hanoi, Vietnam  September 10, 2023   \n",
       "\n",
       "         president                                                url  \\\n",
       "0  Joseph R. Biden  https://www.presidency.ucsb.edu/documents/the-...   \n",
       "1  Joseph R. Biden  https://www.presidency.ucsb.edu/documents/the-...   \n",
       "\n",
       "   question_order                                 interview_question  \\\n",
       "0               1  Q. Of the Biden administration. And accused th...   \n",
       "1               1  Q. Of the Biden administration. And accused th...   \n",
       "\n",
       "                                    interview_answer  \\\n",
       "0  Well, look, first of all, theI am sincere abou...   \n",
       "1  Well, look, first of all, theI am sincere abou...   \n",
       "\n",
       "                                      gpt3.5_summary  \\\n",
       "0  The question consists of 2 parts: \\n1. How wou...   \n",
       "1  The question consists of 2 parts: \\n1. How wou...   \n",
       "\n",
       "                                   gpt3.5_prediction  \\\n",
       "0  Question part: 1. How would you respond to the...   \n",
       "1  Question part: 1. How would you respond to the...   \n",
       "\n",
       "                                            question  annotator_id  \\\n",
       "0  How would you respond to the accusation that t...            85   \n",
       "1  Do you think President Xi is being sincere abo...            85   \n",
       "\n",
       "   annotator1  annotator2  annotator3  inaudible  multiple_questions  \\\n",
       "0         NaN         NaN         NaN      False               False   \n",
       "1         NaN         NaN         NaN      False               False   \n",
       "\n",
       "   affirmative_questions  index clarity_label evasion_label  \n",
       "0                  False      0   Clear Reply      Explicit  \n",
       "1                  False      1    Ambivalent       General  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_PATH = Path('../data/raw/QEvasion.csv')\n",
    "if not DATA_PATH.exists():\n",
    "    raise FileNotFoundError('Expected QEvasion CSV under data/raw/. Run preprocessing first.')\n",
    "\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "print(f\"Loaded {len(df):,} rows\")\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2baead5d",
   "metadata": {},
   "source": [
    "## Normalize dates and keep key columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a4dce98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns after drop: 10\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>interview_year</th>\n",
       "      <th>clarity_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The President's News Conference in Hanoi, Vietnam</td>\n",
       "      <td>2023</td>\n",
       "      <td>Clear Reply</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The President's News Conference in Hanoi, Vietnam</td>\n",
       "      <td>2023</td>\n",
       "      <td>Ambivalent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The President's News Conference in Hanoi, Vietnam</td>\n",
       "      <td>2023</td>\n",
       "      <td>Ambivalent</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  interview_year  \\\n",
       "0  The President's News Conference in Hanoi, Vietnam            2023   \n",
       "1  The President's News Conference in Hanoi, Vietnam            2023   \n",
       "2  The President's News Conference in Hanoi, Vietnam            2023   \n",
       "\n",
       "  clarity_label  \n",
       "0   Clear Reply  \n",
       "1    Ambivalent  \n",
       "2    Ambivalent  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DROP_COLS = [\n",
    "    'annotator_id', 'annotator1', 'annotator2', 'annotator3',\n",
    "    'inaudible', 'multiple_questions', 'affirmative_questions',\n",
    "    'index', 'question_order', 'url'\n",
    "]\n",
    "\n",
    "df['interview_year'] = pd.to_datetime(df['date'], errors='coerce').dt.year\n",
    "df = df.drop(columns=DROP_COLS + ['date'], errors='ignore')\n",
    "print('Columns after drop:', len(df.columns))\n",
    "df[['title', 'interview_year', 'clarity_label']].head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3536fc",
   "metadata": {},
   "source": [
    "## Text normalization (stop-words retained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "624062c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>normalized_question</th>\n",
       "      <th>normalized_answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>q. of the biden administration. and accused th...</td>\n",
       "      <td>well, look, first of all, thei am sincere abou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>q. of the biden administration. and accused th...</td>\n",
       "      <td>well, look, first of all, thei am sincere abou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>q. no worries. do you believe the country's sl...</td>\n",
       "      <td>look, i think china has a difficult economic p...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 normalized_question  \\\n",
       "0  q. of the biden administration. and accused th...   \n",
       "1  q. of the biden administration. and accused th...   \n",
       "2  q. no worries. do you believe the country's sl...   \n",
       "\n",
       "                                   normalized_answer  \n",
       "0  well, look, first of all, thei am sincere abou...  \n",
       "1  well, look, first of all, thei am sincere abou...  \n",
       "2  look, i think china has a difficult economic p...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def normalize_text(series: pd.Series) -> pd.Series:\n",
    "    return (series.fillna('')\n",
    "                  .str.replace(r\"\\s+\", ' ', regex=True)\n",
    "                  .str.strip()\n",
    "                  .str.lower())\n",
    "\n",
    "df['normalized_question'] = normalize_text(df['interview_question'])\n",
    "df['normalized_answer'] = normalize_text(df['interview_answer'])\n",
    "df[['normalized_question', 'normalized_answer']].head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36204938",
   "metadata": {},
   "source": [
    "## Structural feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3fa58f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>answer_word_count</th>\n",
       "      <td>3448.0</td>\n",
       "      <td>293.572216</td>\n",
       "      <td>301.541101</td>\n",
       "      <td>1.0</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>207.000000</td>\n",
       "      <td>440.000000</td>\n",
       "      <td>2117.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>question_word_count</th>\n",
       "      <td>3448.0</td>\n",
       "      <td>61.506090</td>\n",
       "      <td>59.859859</td>\n",
       "      <td>3.0</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>82.000000</td>\n",
       "      <td>780.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>answer_char_count</th>\n",
       "      <td>3448.0</td>\n",
       "      <td>1663.320476</td>\n",
       "      <td>1733.344026</td>\n",
       "      <td>3.0</td>\n",
       "      <td>309.000000</td>\n",
       "      <td>1155.500000</td>\n",
       "      <td>2495.000000</td>\n",
       "      <td>12102.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>answer_sentence_count</th>\n",
       "      <td>3448.0</td>\n",
       "      <td>14.233759</td>\n",
       "      <td>13.007882</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>87.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>answer_avg_sentence_len</th>\n",
       "      <td>3448.0</td>\n",
       "      <td>23.943976</td>\n",
       "      <td>65.226070</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.400000</td>\n",
       "      <td>16.630682</td>\n",
       "      <td>24.291667</td>\n",
       "      <td>1302.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qa_overlap_ratio</th>\n",
       "      <td>3448.0</td>\n",
       "      <td>0.306205</td>\n",
       "      <td>0.166873</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.190357</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.426829</td>\n",
       "      <td>0.8125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>question_mark_flag</th>\n",
       "      <td>3448.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>answer_hedge_freq</th>\n",
       "      <td>3448.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          count         mean          std  min         25%  \\\n",
       "answer_word_count        3448.0   293.572216   301.541101  1.0   56.000000   \n",
       "question_word_count      3448.0    61.506090    59.859859  3.0   22.000000   \n",
       "answer_char_count        3448.0  1663.320476  1733.344026  3.0  309.000000   \n",
       "answer_sentence_count    3448.0    14.233759    13.007882  1.0    4.000000   \n",
       "answer_avg_sentence_len  3448.0    23.943976    65.226070  1.0   10.400000   \n",
       "qa_overlap_ratio         3448.0     0.306205     0.166873  0.0    0.190357   \n",
       "question_mark_flag       3448.0     0.000000     0.000000  0.0    0.000000   \n",
       "answer_hedge_freq        3448.0     0.000000     0.000000  0.0    0.000000   \n",
       "\n",
       "                                 50%          75%         max  \n",
       "answer_word_count         207.000000   440.000000   2117.0000  \n",
       "question_word_count        50.000000    82.000000    780.0000  \n",
       "answer_char_count        1155.500000  2495.000000  12102.0000  \n",
       "answer_sentence_count      11.000000    20.000000     87.0000  \n",
       "answer_avg_sentence_len    16.630682    24.291667   1302.0000  \n",
       "qa_overlap_ratio            0.333333     0.426829      0.8125  \n",
       "question_mark_flag          0.000000     0.000000      0.0000  \n",
       "answer_hedge_freq           0.000000     0.000000      0.0000  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sentence_stats(text: str):\n",
    "    sentences = re.split(r\"(?<=[.!?])\\s+\", text.strip()) if text else []\n",
    "    sentences = [s for s in sentences if s]\n",
    "    if not sentences:\n",
    "        return 0, 0\n",
    "    lengths = [len(s.split()) for s in sentences]\n",
    "    return len(sentences), np.mean(lengths)\n",
    "\n",
    "length_features = df[['normalized_question', 'normalized_answer']].copy()\n",
    "length_features['answer_word_count'] = length_features['normalized_answer'].str.split().str.len()\n",
    "length_features['question_word_count'] = length_features['normalized_question'].str.split().str.len()\n",
    "length_features['answer_char_count'] = df['normalized_answer'].str.len()\n",
    "length_features['answer_sentence_count'], length_features['answer_avg_sentence_len'] = zip(*length_features['normalized_answer'].map(sentence_stats))\n",
    "\n",
    "def lexical_overlap(row):\n",
    "    q_tokens = row['normalized_question'].split()\n",
    "    a_tokens = row['normalized_answer'].split()\n",
    "    if not q_tokens or not a_tokens:\n",
    "        return 0.0\n",
    "    intersection = len(set(q_tokens) & set(a_tokens))\n",
    "    return intersection / len(set(q_tokens))\n",
    "\n",
    "length_features['qa_overlap_ratio'] = df.apply(lexical_overlap, axis=1)\n",
    "length_features['question_mark_flag'] = df['normalized_question'].str.contains('\\?', regex=False).astype(int)\n",
    "length_features['answer_hedge_freq'] = df['normalized_answer'].str.count(r\"\b(maybe|perhaps|sort of|kind of|i think)\b\")\n",
    "\n",
    "feature_cols = [\n",
    "    'answer_word_count', 'question_word_count', 'answer_char_count',\n",
    "    'answer_sentence_count', 'answer_avg_sentence_len', 'qa_overlap_ratio',\n",
    "    'question_mark_flag', 'answer_hedge_freq'\n",
    "]\n",
    "\n",
    "for col in feature_cols:\n",
    "    df[col] = length_features[col]\n",
    "\n",
    "df[feature_cols].describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edde62c0",
   "metadata": {},
   "source": [
    "## Sentiment extraction via Hugging Face model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d328ad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch in /home/ecetsn/.local/lib/python3.10/site-packages (2.9.1)\n",
      "Requirement already satisfied: filelock in /home/ecetsn/.local/lib/python3.10/site-packages (from torch) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/ecetsn/.local/lib/python3.10/site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/ecetsn/.local/lib/python3.10/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /home/ecetsn/.local/lib/python3.10/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/ecetsn/.local/lib/python3.10/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /home/ecetsn/.local/lib/python3.10/site-packages (from torch) (2025.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /home/ecetsn/.local/lib/python3.10/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /home/ecetsn/.local/lib/python3.10/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /home/ecetsn/.local/lib/python3.10/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /home/ecetsn/.local/lib/python3.10/site-packages (from torch) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /home/ecetsn/.local/lib/python3.10/site-packages (from torch) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /home/ecetsn/.local/lib/python3.10/site-packages (from torch) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /home/ecetsn/.local/lib/python3.10/site-packages (from torch) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /home/ecetsn/.local/lib/python3.10/site-packages (from torch) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /home/ecetsn/.local/lib/python3.10/site-packages (from torch) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /home/ecetsn/.local/lib/python3.10/site-packages (from torch) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /home/ecetsn/.local/lib/python3.10/site-packages (from torch) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /home/ecetsn/.local/lib/python3.10/site-packages (from torch) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /home/ecetsn/.local/lib/python3.10/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /home/ecetsn/.local/lib/python3.10/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /home/ecetsn/.local/lib/python3.10/site-packages (from torch) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.5.1 in /home/ecetsn/.local/lib/python3.10/site-packages (from torch) (3.5.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/ecetsn/.local/lib/python3.10/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ecetsn/.local/lib/python3.10/site-packages (from jinja2->torch) (3.0.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc7b8d60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9.1+cu128\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bcb96e29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cuda:0\n",
      "/home/ecetsn/.local/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:111: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_sent_negative</th>\n",
       "      <th>question_sent_neutral</th>\n",
       "      <th>question_sent_positive</th>\n",
       "      <th>answer_sent_negative</th>\n",
       "      <th>answer_sent_neutral</th>\n",
       "      <th>answer_sent_positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.284938</td>\n",
       "      <td>0.694826</td>\n",
       "      <td>0.020236</td>\n",
       "      <td>0.064227</td>\n",
       "      <td>0.749542</td>\n",
       "      <td>0.186231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.284938</td>\n",
       "      <td>0.694826</td>\n",
       "      <td>0.020236</td>\n",
       "      <td>0.064227</td>\n",
       "      <td>0.749542</td>\n",
       "      <td>0.186231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.370508</td>\n",
       "      <td>0.612024</td>\n",
       "      <td>0.017468</td>\n",
       "      <td>0.641184</td>\n",
       "      <td>0.339137</td>\n",
       "      <td>0.019679</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   question_sent_negative  question_sent_neutral  question_sent_positive  \\\n",
       "0                0.284938               0.694826                0.020236   \n",
       "1                0.284938               0.694826                0.020236   \n",
       "2                0.370508               0.612024                0.017468   \n",
       "\n",
       "   answer_sent_negative  answer_sent_neutral  answer_sent_positive  \n",
       "0              0.064227             0.749542              0.186231  \n",
       "1              0.064227             0.749542              0.186231  \n",
       "2              0.641184             0.339137              0.019679  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL_NAME = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME)\n",
    "sentiment_pipeline = pipeline(\n",
    "    \"text-classification\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    truncation=True,\n",
    "    max_length=512,\n",
    "    padding=True,\n",
    "    return_all_scores=True\n",
    ")\n",
    "\n",
    "label_order = [model.config.id2label[idx] for idx in range(model.config.num_labels)]\n",
    "\n",
    "def batch_sentiment(texts, batch_size=32):\n",
    "    scores = sentiment_pipeline(texts, batch_size=batch_size)\n",
    "    rows = []\n",
    "    for entry in scores:\n",
    "        rows.append({item['label']: item['score'] for item in entry})\n",
    "    return pd.DataFrame(rows)[label_order]\n",
    "\n",
    "question_sentiment = batch_sentiment(df['normalized_question'].tolist())\n",
    "question_sentiment.columns = [f'question_sent_{col.lower()}' for col in question_sentiment.columns]\n",
    "answer_sentiment = batch_sentiment(df['normalized_answer'].tolist())\n",
    "answer_sentiment.columns = [f'answer_sent_{col.lower()}' for col in answer_sentiment.columns]\n",
    "\n",
    "df = pd.concat([df, question_sentiment, answer_sentiment], axis=1)\n",
    "df.filter(like='sent_').head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a1758d6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pip in /home/ecetsn/.local/lib/python3.10/site-packages (25.2)\n",
      "Collecting pip\n",
      "  Downloading pip-25.3-py3-none-any.whl.metadata (4.7 kB)\n",
      "Downloading pip-25.3-py3-none-any.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 25.2\n",
      "    Uninstalling pip-25.2:\n",
      "      Successfully uninstalled pip-25.2\n",
      "Successfully installed pip-25.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f29b77",
   "metadata": {},
   "source": [
    "## Sentiment-augmented classifier prototype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "18d85d8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.561\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'                 precision    recall  f1-score   support\\n\\n     Ambivalent       0.69      0.61      0.65       408\\nClear Non-Reply       0.39      0.66      0.49        71\\n    Clear Reply       0.44      0.43      0.44       211\\n\\n       accuracy                           0.56       690\\n      macro avg       0.51      0.57      0.52       690\\n   weighted avg       0.58      0.56      0.57       690\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_df = df.dropna(subset=['clarity_label']).copy()\n",
    "model_df['text_concat'] = model_df['normalized_question'] + ' ' + model_df['normalized_answer']\n",
    "\n",
    "numeric_feats = feature_cols + list(question_sentiment.columns) + list(answer_sentiment.columns)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    model_df[['text_concat'] + numeric_feats],\n",
    "    model_df['clarity_label'],\n",
    "    test_size=0.2,\n",
    "    stratify=model_df['clarity_label'],\n",
    "    random_state=7\n",
    ")\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('text', TfidfVectorizer(max_features=30000, ngram_range=(1,2)), 'text_concat'),\n",
    "    ('numeric', StandardScaler(), numeric_feats)\n",
    "])\n",
    "\n",
    "pipeline_model = Pipeline([\n",
    "    ('features', preprocessor),\n",
    "    ('clf', LogisticRegression(max_iter=1000, class_weight='balanced'))\n",
    "])\n",
    "\n",
    "pipeline_model.fit(X_train, y_train)\n",
    "preds = pipeline_model.predict(X_test)\n",
    "print('Validation accuracy:', (preds == y_test).mean().round(3))\n",
    "classification_report(y_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e124caa0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
