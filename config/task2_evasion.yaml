dataset:
  dataset_name: ailsntua/QEvasion
  label_column: evasion_label
  text_column: interview_answer
  question_column: question
  validation_size: 0.25
  random_seed: 7
  calibration_fraction: 0.2
  max_train_samples: null
  max_val_samples: null
  max_test_samples: null
  resampling:
    type: over          # options: over, under, none/null
    random_seed: 7

base_models:
  - name: tfidf_evasion
    type: tfidf_logreg
    replicas: 1         # set >1 to enable replica ensemble training
    replica_sampling: partition  # options: partition, bootstrap
    replica_weights: null        # optional list of floats
    params:
      ngram_range: [1, 2]
      max_features: 30000
      c: 4.0
  - name: openrouter_evasion
    type: openrouter_logreg
    params:
      model_name: text-embedding-3-large
      batch_size: 6
      c: 1.0
      max_iter: 400
  - name: openrouter_qwen8b
    type: openrouter_logreg
    params:
      model_name: Qwen/Qwen3-Embedding-8B
      batch_size: 4
      c: 1.0
      max_iter: 400
  # - name: zero_shot_bart
  #   type: zero_shot
  #   params:
  #     model_name: facebook/bart-large-mnli
  #     batch_size: 4

fusion:
  type: logistic_regression
  params:
    C: 2.0
    max_iter: 1200

output_dir: experiments

paper_reference:
  description: "Llama-2-70b (evasion-based clarity) from Thomas et al., 2024 (clarity baseline)"
  source: semval_2026_task6.pdf
  metrics:
    accuracy: 0.713
    precision: 0.743
    recall: 0.713
    macro_f1: 0.720
    weighted_f1: 0.752
