task_name: evasion
model_name_or_path: meta-llama/Meta-Llama-3-8B-Instruct
train_file: data/processed/train_evasion.json
eval_file: data/processed/dev_evasion.json
output_dir: experiments/results_dev/evasion
max_seq_length: 2048
learning_rate: 2e-5
batch_size: 4
num_train_epochs: 4
gradient_accumulation_steps: 2
weight_decay: 0.01
warmup_ratio: 0.1
save_total_limit: 2
evaluation_strategy: epoch
lora:
  r: 16
  alpha: 32
  dropout: 0.05
soft_label_training: true
dual_pass: true
