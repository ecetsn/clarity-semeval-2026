dataset:
  dataset_name: ailsntua/QEvasion
  label_column: clarity_label  # Task 1 (switch to evasion_label for Task 2)
  text_column: interview_answer
  question_column: question
  validation_size: 0.2
  random_seed: 42
  calibration_fraction: 0.2
  max_train_samples: null
  max_val_samples: null
  max_test_samples: null

base_models:
  - name: tfidf_baseline
    type: tfidf_logreg
    params:
      ngram_range: [1, 2]
      max_features: 25000
      c: 3.0
      max_iter: 1000
  - name: openrouter_embedder
    type: openrouter_logreg
    params:
      model_name: text-embedding-3-large
      batch_size: 6
      c: 1.5
      max_iter: 300
  - name: zero_shot_bart
    type: zero_shot
    params:
      model_name: facebook/bart-large-mnli
      batch_size: 2

fusion:
  type: logistic_regression # weighted_average for majority voting
  params:
    C: 2.0
    max_iter: 1000

output_dir: experiments

paper_reference:
  description: "Llama-2-70b (evasion-based clarity) from Thomas et al., 2024"
  source: semval_2026_task6.pdf
  metrics:
    accuracy: 0.713
    precision: 0.743
    recall: 0.713
    macro_f1: 0.720
    weighted_f1: 0.752
