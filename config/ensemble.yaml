dataset:
  dataset_name: ailsntua/QEvasion
  label_column: evasion_label  # Task 1 clarity_label (switch to evasion_label for Task 2)
  text_column: interview_answer
  question_column: question
  validation_size: 0.2
  random_seed: 42
  calibration_fraction: 0.2
  max_train_samples: null
  max_val_samples: null
  max_test_samples: null
  resampling:
    type: over          # options: over, under, none/null
    random_seed: 42

base_models:
  - name: tfidf_baseline
    type: tfidf_logreg
    replicas: 20         # set >1 to enable replica ensemble training
    replica_sampling: partition  # options: partition, bootstrap
    replica_weights: null        # optional list of floats
    params:
      ngram_range: [1, 2]
      max_features: 25000
      c: 3.0
      max_iter: 1000
  - name: tfidf_xgboost
    type: tfidf_xgboost
    replicas: 1
    params:
      ngram_range: [1, 2]
      max_features: 30000
      max_depth: 6
      learning_rate: 0.1
      n_estimators: 400
      subsample: 0.8
      colsample_bytree: 0.8
  - name: openrouter_embedder
    type: openrouter_logreg
    params:
      model_name: text-embedding-3-large
      batch_size: 6
      c: 1.5
      max_iter: 300
  - name: zero_shot_bart
    type: zero_shot
    params:
      model_name: facebook/bart-large-mnli
      batch_size: 2

fusion:
  type: weighted_average # weighted_average for majority voting, logistic_regression for fusion
  params:
    C: 2.0
    max_iter: 1000

output_dir: experiments

paper_reference:
  description: "Llama-2-70b (evasion-based clarity) from Thomas et al., 2024"
  source: semval_2026_task6.pdf
  metrics:
    accuracy: 0.713
    precision: 0.743
    recall: 0.713
    macro_f1: 0.720
    weighted_f1: 0.752
